
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML is auto-generated from an M-file.
To make changes, update the M-file and republish this document.
      --><title>tutDiscrimClassif</title><meta name="generator" content="MATLAB 7.9"><meta name="date" content="2010-08-31"><meta name="m-file" content="tutDiscrimClassif"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head><body><div class="content"><h2>Contents</h2><div><ul><li><a href="#1">Classification using discriminative models in pmtk3</a></li><li><a href="#2">Controlling the optimizer</a></li></ul></div><h2>Classification using discriminative models in pmtk3<a name="1"></a></h2><p><i>This page was auto-generated by publishing</i> <a href="http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/tutDiscrimClassif.m">tutDiscrimClassif.m</a></p><p>We extend the discriminative regression techniques discussed <a href="http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/html/tutRegr.html">here</a> to the classification setting.</p><pre class="codeinput"><span class="comment">%{
</span><span class="comment">%% Logistic regression: plugin method
</span><span class="comment">% Consider fitting a binary logistic regression
</span><span class="comment">% model to some SAT scores, where the response is whether the student
</span><span class="comment">% passed or failed the class. First we compute the MLE and use a plugin
</span><span class="comment">% approximation for prediction, as is standard practice (from &lt;http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Introduction/logregSATdemo.m logregSATdemo.m&gt; )
</span><span class="comment">%
</span><span class="comment">%%
</span><span class="comment">close all; clear all
</span><span class="comment">stat = loadData('sat');  y = stat(:,1); X = stat(:,4);
</span><span class="comment">model = logregFit(X, y);
</span><span class="comment">[yhat, prob] = logregPredict(model, X); %ok
</span><span class="comment">figure;
</span><span class="comment">plot(X, y, 'ko', 'linewidth', 2, 'MarkerSize', 7, 'markerfacecolor', 'k');
</span><span class="comment">hold on
</span><span class="comment">plot(X, prob, 'ro', 'linewidth', 2,'MarkerSize', 10)
</span><span class="comment">%%
</span><span class="comment">% We see that the probability of passing the class smoothly increases as
</span><span class="comment">% the SAT score goes up, as is to be expected.
</span><span class="comment">
</span><span class="comment">%% Logistic regression: Bayesian method
</span><span class="comment">% Now let us fit the model using Bayesian inference with an noninformative
</span><span class="comment">% prior, which we approximate by $N(w|0,\infty I)$,
</span><span class="comment">% which corresponds to inference with an L2 regularizer of $\lambda=0$.
</span><span class="comment">% By default, the fitting procedure uses a Laplace
</span><span class="comment">% approximation to the posterior, from which
</span><span class="comment">% we can extract credible intervals on the parameters, etc.
</span><span class="comment">%
</span><span class="comment">% To approximate the predictive density, we
</span><span class="comment">% can plug in the posterior mean:
</span><span class="comment">%%
</span><span class="comment">% $$p(y=1|x,D) = \int \sigma(w^T * x) N(w|\mu,\Sigma) dw
</span><span class="comment">% \approx \sigma(w^T \mu)$$
</span><span class="comment">%%
</span><span class="comment">% However, this gives essentially the same result as plugging in the MLE.
</span><span class="comment">% To get a measure of confidence in this prediction, we can sample values
</span><span class="comment">% of the parameters from their posterior (which we have approximated by a Gaussian), use each such sample to make a
</span><span class="comment">% prediction, and then compute empirical quantiles of this distribution to
</span><span class="comment">% get a 95% credible interval.
</span><span class="comment">% This is done using &lt;http://pmtk3.googlecode.com/svn/trunk/toolbox/SupervisedModels/logisticRegression/logregPredictBayes.m logregPredictBayes.m&gt; and gives the results shown below
</span><span class="comment">% (from &lt;http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Introduction/logregSATdemoBayes.m logregSATdemoBayes.m&gt; )
</span><span class="comment">
</span><span class="comment">%%
</span><span class="comment">model = logregFitBayes(X, y);
</span><span class="comment">[yhat, prob, pCI] = logregPredictBayes(model, X);
</span><span class="comment">figure;
</span><span class="comment">plot(X, y, 'ko', 'linewidth', 2, 'MarkerSize', 7, 'markerfacecolor', 'k');
</span><span class="comment">hold on
</span><span class="comment">plot(X, prob, 'ro', 'linewidth', 2,'MarkerSize', 10)
</span><span class="comment">for i=1:size(X,1)
</span><span class="comment">  line([X(i,1) X(i,1)], [pCI(i,1) pCI(i,2)]);
</span><span class="comment">end
</span><span class="comment">%%
</span><span class="comment">
</span><span class="comment">
</span><span class="comment">%% Visualizing the decision boundaries
</span><span class="comment">% When comparing classification methods, it is useful to apply them to 2d
</span><span class="comment">% datasets and to plot the regions of space that get mapped to each class;
</span><span class="comment">% these are called decision regions, and the boundaries between them are called decision
</span><span class="comment">% boundaries. We can do this using the &lt;http://matlabtools.googlecode.com/svn/trunk/graphics/plotDecisionBoundary.m plotDecisionBoundary.m&gt;
</span><span class="comment">% function, which takes a prediction function as an argument.
</span><span class="comment">% As an example of this, consider the famous XOR dataset.
</span><span class="comment">% Let us try fitting a logistic regression model to it in the original
</span><span class="comment">% feature space (from &lt;http://pmtk3.googlecode.com/svn/trunk/demos/otherDemos/supervisedModels/logregXorLinearDemo.m logregXorLinearDemo.m&gt; )
</span><span class="comment">%%
</span><span class="comment">close all; clear all;
</span><span class="comment">[X, y] = createXORdata();
</span><span class="comment">model = logregFit(X, y);
</span><span class="comment">plotDecisionBoundary(X, y, @(X)logregPredict(model, X));
</span><span class="comment">yhat = logregPredict(model, X);
</span><span class="comment">errorRate = mean(yhat ~= y)
</span><span class="comment">%%
</span><span class="comment">% We see that the method performs at chance level, because the data is not
</span><span class="comment">% linearly separable. A simple fix to this problem is to use
</span><span class="comment">% basis function expansion, as we discuss
</span><span class="comment">% &lt;http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/html/tutKernelClassif.html
</span><span class="comment">% here&gt;.
</span><span class="comment">
</span><span class="comment">%}</span>
</pre><h2>Controlling the optimizer<a name="2"></a></h2><p>Computing the ML or MAP parameter estimate, as well as a Laplace approximation to the posterior, involves solving an optimization problem. When we use a Gaussian prior, the resulting objective is convex and smooth. We use Mark Schmidt's excellent <a href="http://www.cs.ubc.ca/~schmidtm/Software/minFunc.html">minFunc</a> package for such problems. (A version is included in <a href="http://code.google.com/p/pmtksupport/">pmtkSupport</a>.)</p><p><a href="http://pmtksupport.googlecode.com/svn/trunk/markSchmidt-7june2010/minFunc/minFunc.m">minFunc.m</a> implements many different algorithms and supports a variety of optional arguments which control the convergence threshold, max number of iterations, etc. In pmtk, you can pass in optional arguments through to minfunc as follows: <tt>[model] = logregFit(X, y,  'fitOptions', options)</tt></p><p>Below we compare different optimizers for finding the MAP estimate for a binary logistic regression model under a weak Gaussian prior, applied to two of the MNIST digit classes. (The code extends easily to the multi-class case, but this is a bit slower). Specifically, we compare the following</p><p>sd: steepest descent cg: conjugate gradient bb: barzilai borwein lbfgs: limited memory BFGS</p><p>sd, cg and bb are first order methods; lbfgs is second order.</p><p>To make the optimization problem a bit harder, we don't preprocess the data in any way. (By default, <a href="http://pmtk3.googlecode.com/svn/trunk/toolbox/SupervisedModels/logisticRegression/logregFit.m">logregFit.m</a> standardizes its inputs, as well as adding a column of 1s; standardization helps convergence a lot, as well as being advisable for statistical reasons.)</p><p>(Modified from <a href="http://pmtk3.googlecode.com/svn/trunk/demos/otherDemos/supervisedModels/logregOptDemo.m">logregOptDemo.m</a> )</p><pre class="codeinput">clear <span class="string">all</span>
setSeed(0);
Ntrain = [];
[Xtrain, ytrain, Xtest, ytest] = mnistLoad([2 3]);
lambda = 1e-3;

<span class="comment">% minfunc options</span>
options = [];
options.derivativeCheck = <span class="string">'off'</span>;
options.display = <span class="string">'none'</span>;
<span class="comment">%options.display = 'iter';</span>
options.maxIter = 50;
options.maxFunEvals = 50;
options.TolFun = 1e-3; <span class="comment">% default 1e-5</span>
options.TolX = 1e-3; <span class="comment">% default 1e-5</span>

<span class="comment">% algorithms</span>
methods = {<span class="string">'sd'</span>, <span class="string">'cg'</span>, <span class="string">'bb'</span>, <span class="string">'lbfgs'</span>};

<span class="keyword">for</span> m=1:length(methods)
  method = methods{m}
  options.Method = method;
  tic
  [model, X, lambdaVec, opt] = logregFit(Xtrain, ytrain, <span class="string">'regtype'</span>, <span class="string">'l2'</span>, <span class="keyword">...</span>
    <span class="string">'lambda'</span>, lambda, <span class="string">'fitOptions'</span>, options, <span class="string">'preproc'</span>, []);
  t = toc;
  fvalTrace = opt.output.trace.fval;
  finalObj = opt.finalObj;
  figure;
  plot(fvalTrace, <span class="string">'o-'</span>, <span class="string">'linewidth'</span>, 2);
  set(gca, <span class="string">'xlim'</span>, [10 50]); <span class="comment">% make scales comparable</span>
  title(sprintf(<span class="string">'%s, %5.3f seconds, final obj = %5.3f'</span>, <span class="keyword">...</span>
    method, t, finalObj));
<span class="keyword">end</span>
</pre><pre class="codeoutput">method =
sd
Warning: Matrix is close to singular or badly scaled.
         Results may be inaccurate. RCOND = 2.328554e-025. 
method =
cg
Warning: Matrix is close to singular or badly scaled.
         Results may be inaccurate. RCOND = 5.380512e-026. 
Warning: Matrix is close to singular or badly scaled.
         Results may be inaccurate. RCOND = 5.880754e-026. 
Warning: Matrix is close to singular or badly scaled.
         Results may be inaccurate. RCOND = 3.138278e-026. 
Warning: Matrix is close to singular or badly scaled.
         Results may be inaccurate. RCOND = 9.105466e-027. 
Warning: Matrix is close to singular or badly scaled.
         Results may be inaccurate. RCOND = 1.798438e-024. 
Warning: Matrix is close to singular or badly scaled.
         Results may be inaccurate. RCOND = 6.013316e-027. 
Warning: Matrix is close to singular or badly scaled.
         Results may be inaccurate. RCOND = 2.735941e-027. 
Warning: Matrix is close to singular or badly scaled.
         Results may be inaccurate. RCOND = 3.530990e-027. 
Warning: Matrix is close to singular or badly scaled.
         Results may be inaccurate. RCOND = 2.312295e-027. 
method =
bb
method =
lbfgs
</pre><img vspace="5" hspace="5" src="tutDiscrimClassif_01.png" alt=""> <img vspace="5" hspace="5" src="tutDiscrimClassif_02.png" alt=""> <img vspace="5" hspace="5" src="tutDiscrimClassif_03.png" alt=""> <img vspace="5" hspace="5" src="tutDiscrimClassif_04.png" alt=""> <p>Since the objective is convex, all methods find the same solution  if given enough time. If you want a very precise estimate, LBFGS is the way to go, but if a somewhat sloppier estimate is sufficient, fast first-order methods such as BB are the way to go.</p><p class="footer"><br>
      Published with MATLAB&reg; 7.9<br></p></div><!--
##### SOURCE BEGIN #####
%% Classification using discriminative models in pmtk3
% _This page was auto-generated by publishing_
% <http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/tutDiscrimClassif.m tutDiscrimClassif.m>
%
% We extend the discriminative regression techniques discussed
% <http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/html/tutRegr.html
% here> to the classification setting.


%{
%% Logistic regression: plugin method
% Consider fitting a binary logistic regression
% model to some SAT scores, where the response is whether the student
% passed or failed the class. First we compute the MLE and use a plugin
% approximation for prediction, as is standard practice (from <http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Introduction/logregSATdemo.m logregSATdemo.m> )
%
%%
close all; clear all
stat = loadData('sat');  y = stat(:,1); X = stat(:,4);
model = logregFit(X, y);
[yhat, prob] = logregPredict(model, X); %ok
figure;
plot(X, y, 'ko', 'linewidth', 2, 'MarkerSize', 7, 'markerfacecolor', 'k');
hold on
plot(X, prob, 'ro', 'linewidth', 2,'MarkerSize', 10)
%%
% We see that the probability of passing the class smoothly increases as
% the SAT score goes up, as is to be expected.

%% Logistic regression: Bayesian method
% Now let us fit the model using Bayesian inference with an noninformative
% prior, which we approximate by $N(w|0,\infty I)$,
% which corresponds to inference with an L2 regularizer of $\lambda=0$.
% By default, the fitting procedure uses a Laplace
% approximation to the posterior, from which
% we can extract credible intervals on the parameters, etc.
% 
% To approximate the predictive density, we
% can plug in the posterior mean:
%%
% $$p(y=1|x,D) = \int \sigma(w^T * x) N(w|\mu,\Sigma) dw 
% \approx \sigma(w^T \mu)$$
%%
% However, this gives essentially the same result as plugging in the MLE.
% To get a measure of confidence in this prediction, we can sample values
% of the parameters from their posterior (which we have approximated by a Gaussian), use each such sample to make a
% prediction, and then compute empirical quantiles of this distribution to
% get a 95% credible interval.
% This is done using <http://pmtk3.googlecode.com/svn/trunk/toolbox/SupervisedModels/logisticRegression/logregPredictBayes.m logregPredictBayes.m> and gives the results shown below
% (from <http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Introduction/logregSATdemoBayes.m logregSATdemoBayes.m> )

%%
model = logregFitBayes(X, y);
[yhat, prob, pCI] = logregPredictBayes(model, X);
figure;
plot(X, y, 'ko', 'linewidth', 2, 'MarkerSize', 7, 'markerfacecolor', 'k');
hold on
plot(X, prob, 'ro', 'linewidth', 2,'MarkerSize', 10)
for i=1:size(X,1)
  line([X(i,1) X(i,1)], [pCI(i,1) pCI(i,2)]);
end
%%


%% Visualizing the decision boundaries
% When comparing classification methods, it is useful to apply them to 2d
% datasets and to plot the regions of space that get mapped to each class;
% these are called decision regions, and the boundaries between them are called decision
% boundaries. We can do this using the <http://matlabtools.googlecode.com/svn/trunk/graphics/plotDecisionBoundary.m plotDecisionBoundary.m>
% function, which takes a prediction function as an argument.
% As an example of this, consider the famous XOR dataset.
% Let us try fitting a logistic regression model to it in the original
% feature space (from <http://pmtk3.googlecode.com/svn/trunk/demos/otherDemos/supervisedModels/logregXorLinearDemo.m logregXorLinearDemo.m> )
%%
close all; clear all;
[X, y] = createXORdata();
model = logregFit(X, y);
plotDecisionBoundary(X, y, @(X)logregPredict(model, X));
yhat = logregPredict(model, X);
errorRate = mean(yhat ~= y)
%%
% We see that the method performs at chance level, because the data is not
% linearly separable. A simple fix to this problem is to use
% basis function expansion, as we discuss
% <http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/html/tutKernelClassif.html
% here>.

%}

%% Controlling the optimizer
% Computing the ML or MAP parameter estimate, as well as a Laplace
% approximation to the posterior, involves solving an optimization
% problem. When we use a Gaussian prior, the resulting
% objective is convex and smooth. We use 
% Mark Schmidt's excellent
% <http://www.cs.ubc.ca/~schmidtm/Software/minFunc.html minFunc> 
% package for such problems. (A version is included in
% <http://code.google.com/p/pmtksupport/ pmtkSupport>.)
%
% <http://pmtksupport.googlecode.com/svn/trunk/markSchmidt-7june2010/minFunc/minFunc.m minFunc.m> implements many different algorithms and supports
% a variety of optional arguments which control the convergence
% threshold, max number of iterations, etc.
% In pmtk, you can pass in optional arguments through to 
% minfunc as follows:
% |[model] = logregFit(X, y,  'fitOptions', options)|
%
% Below we compare different optimizers for finding the MAP
% estimate for a binary logistic
% regression model under a weak Gaussian prior, applied to two of the MNIST digit classes.
% (The code extends easily to the multi-class case,
% but this is a bit slower).
% Specifically, we compare the following
%%
% sd: steepest descent
% cg: conjugate gradient
% bb: barzilai borwein
% lbfgs: limited memory BFGS 
%%
% sd, cg and bb are first order methods; lbfgs is second order.
%
% To make the optimization problem a bit harder, we don't preprocess
% the data in any way. (By default, <http://pmtk3.googlecode.com/svn/trunk/toolbox/SupervisedModels/logisticRegression/logregFit.m logregFit.m> standardizes its inputs,
% as well as adding a column of 1s; standardization helps convergence
% a lot, as well as being advisable for statistical reasons.)
%
% (Modified from <http://pmtk3.googlecode.com/svn/trunk/demos/otherDemos/supervisedModels/logregOptDemo.m logregOptDemo.m> )
%% 
clear all
setSeed(0);
Ntrain = [];
[Xtrain, ytrain, Xtest, ytest] = mnistLoad([2 3]);
lambda = 1e-3; 

% minfunc options
options = [];
options.derivativeCheck = 'off';
options.display = 'none';
%options.display = 'iter';
options.maxIter = 50;
options.maxFunEvals = 50;
options.TolFun = 1e-3; % default 1e-5
options.TolX = 1e-3; % default 1e-5

% algorithms
methods = {'sd', 'cg', 'bb', 'lbfgs'};

for m=1:length(methods)
  method = methods{m}
  options.Method = method;
  tic
  [model, X, lambdaVec, opt] = logregFit(Xtrain, ytrain, 'regtype', 'l2', ...
    'lambda', lambda, 'fitOptions', options, 'preproc', []);
  t = toc;
  fvalTrace = opt.output.trace.fval;
  finalObj = opt.finalObj;
  figure;
  plot(fvalTrace, 'o-', 'linewidth', 2);
  set(gca, 'xlim', [10 50]); % make scales comparable
  title(sprintf('%s, %5.3f seconds, final obj = %5.3f', ...
    method, t, finalObj));
end
%%
% Since the objective is convex, all methods find the same
% solution  if given enough time.
% If you want a very precise estimate, 
% LBFGS is the way to go, but if a somewhat sloppier estimate
% is sufficient, fast first-order methods such as BB
% are the way to go.



##### SOURCE END #####
--></body></html>