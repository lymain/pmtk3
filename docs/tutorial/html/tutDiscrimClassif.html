
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML is auto-generated from an M-file.
To make changes, update the M-file and republish this document.
      --><title>Supervised learning using discriminative models in pmtk3</title><meta name="generator" content="MATLAB 7.9"><meta name="date" content="2010-08-30"><meta name="m-file" content="tutDiscrimClassif"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head><body><div class="content"><h1>Supervised learning using discriminative models in pmtk3</h1><!--introduction--><p><i>This page was auto-generated by publishing</i> <a href="http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/tutDiscrimClassif.m">http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/tutDiscrimClassif.m</a>.</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Models</a></li><li><a href="#4">Creating a model</a></li><li><a href="#7">Fitting a linear regression model by maximum likelihood</a></li><li><a href="#12">Bayesian parameter inference</a></li><li><a href="#28">Frequentist parameter inference</a></li><li><a href="#31">Using a model for prediction</a></li><li><a href="#36">Prediction with linear regression</a></li><li><a href="#39">Prediction with logistic regression</a></li><li><a href="#46">Visualizing the decision boundaires</a></li><li><a href="#49">Preprocessing, basis function expansion and kernels</a></li><li><a href="#58">Overfitting, regularization and MAP estimation</a></li><li><a href="#70">Cross validation for ridge regression</a></li><li><a href="#74">Bayesian evidence procedure for ridge regression</a></li><li><a href="#75">Empirical Bayes for ridge regression</a></li><li><a href="#77">Variational Bayes for ridge regression</a></li></ul></div><h2>Models<a name="1"></a></h2><p>The following is a list of pmtk models that are designed for supervised learning (in alphabetical order). We have classified the models based on whether they can be used for classification <img src="tutDiscrimClassif_eq35391.png" alt="$y \in \{1,\ldots,C\}$">, regression <img src="tutDiscrimClassif_eq14315.png" alt="$y \in R$">, or both; whether they are generative models of the form <img src="tutDiscrimClassif_eq35640.png" alt="$p(y,x|\theta)$"> or discriminative models of the form <img src="tutDiscrimClassif_eq42833.png" alt="$p(y|x,\theta)$">; and whether they are parametric (so <img src="tutDiscrimClassif_eq49435.png" alt="$\theta$"> has fixed size) or non-parametric (so <img src="tutDiscrimClassif_eq49435.png" alt="$\theta$"> grows as the training set gets larger). We assume y is a low-dimensional scalar. Models for multivariate conditional density estimation (structured output classification/ regression) will be added later.</p><p>
<table border=1>
<TR ALIGN=left>
<td> Model
<td> Description
<td> Classif/regr
<td> Gen/Discr
<td> Param/non
<tr>
<td> discrimAnalysis
<td> Discriminant analysis (linear, quadratic, regularized, shrunken)
<td> Classif
<td> Gen
<td> Param
<tr>
<td> knn
<td> k nearest neighbors
<td> Classif
<td> Gen
<td> Nonparam
<tr>
<td> linreg
<td> Linear regression
<td> Regr
<td> Discrim
<td> Param
<tr>
<td> logreg
<td> Logistic regression
<td> Classif
<td> Discrim
<td> Param
<tr>
<td> mlpClassif
<td> multi-layer perceptron (aka feedforward neural network)
<td> Classif
<td> Discrim
<td> Param
<tr>
<td> mlpRegress
<td> multi-layer perceptron (aka feedforward neural network)
<td> Regr
<td> Discrim
<td> Param
<tr>
<td> naiveBayes
<td> Naive Bayes classifier
<td> Classif
<td> Gen
<td> Param
<tr>
<td> rvm
<td> Relevance vector machine
<td> Both
<td> Discrim
<td> Nonparam
<tr>
<td> svm
<td> Support vector machine
<td> Both
<td> Discrim
<td> Nonparam
</table>
</p><p>More models may be added in the future.</p><h2>Creating a model<a name="4"></a></h2><p>To create a model of type 'foo', use one of the following</p><p>model = fooCreate(...) % manually specify parameters model = fooFit(X, y, ...) % Compute ML or MAP estimate of params model = fooFitBayes(X, y, ...) % Compute posterior of params</p><p>where</p><div><ul><li> '...' refers to optional arguments (see below)</li><li>X  is an N*D design matrix containing the training data,  where N is the number of training cases and D is the number of features.</li><li>y is an N*1 response vector, which can be real-valued (regression),     0/1 or -1/+1 (binary classification), or 1:C (multi-class).</li></ul></div><p>If X contains missing values, represented as NaNs, it is best to use a generative model (although not all models currently support this functionality). NaNs in y correspond to semi-supervised learning, which is not yet supported.</p><p>The resulting model is a Matlab structure; However, we will sometimes call it an 'object', since it behaves like one in many respects.</p><p>In the case of <tt>fooCreate</tt> and <tt>fooFit</tt>, the parameters are point estimates. In the case of <tt>fooFitBayes</tt>, the parameters are represented as distributions, which may be represented parameterically or as a bag of samples. The details will be explained below when we look at specific model classes.</p><h2>Fitting a linear regression model by maximum likelihood<a name="7"></a></h2><p>As an example, we can fit a linear regression model to the caterpillar data set using maximum likelihood estimation as follows (extracted from linregBayesCaterpillar.m):</p><pre class="codeinput">clear <span class="string">all</span>
X = loadData(<span class="string">'caterpillar'</span>); <span class="comment">% from http://www.ceremade.dauphine.fr/~xian/BCS/caterpillar</span>
y = log(X(:,11)); <span class="comment">% log number of nests</span>
X = X(:,1:10);
[model] = linregFit(X, y)
</pre><pre class="codeoutput">model = 
        lambda: 0
             w: [11x1 double]
        sigma2: 0.5852
       preproc: [1x1 struct]
     modelType: 'linreg'
    likelihood: 'gaussian'
</pre><p>Let us check that this matches the usual equation for the MLE (adding a column of 1s to deal with the offset term)</p><pre class="codeinput">X1 = [ones(size(X,1),1) X];
wOLS = X1\y;
assert(approxeq(model.w, wOLS))
</pre><h2>Bayesian parameter inference<a name="12"></a></h2><p>If you fit by the model by MLE, you can examine the value of the estimated parameters by typing <tt>model.params</tt>, where params is the name of the parameter (here w or sigma2). But what if we want to know how much confidence we should have in these estimates? For this, we should use Bayesin inference. We can compute the posterior distribution of the parameters given the data and an uninformative prior as follows:</p><pre class="codeinput">[modelB, logev, postSummary] = linregFitBayes(X, y, <span class="string">'prior'</span>, <span class="string">'uninf'</span>);
</pre><p>Here <tt>modelB</tt> is the model which contains the posterior of the parameters:</p><pre class="codeinput">modelB
</pre><pre class="codeoutput">modelB = 
      preproc: [1x1 struct]
           wN: [11x1 double]
           VN: [11x11 double]
           aN: 11
           bN: 7.5649
    modelType: 'linregBayes'
        prior: 'uninf'
</pre><p>There is no longer a field called <tt>w</tt> or <tt>sigma2</tt> since we are not using point estimation. Instead, modelB contains the parameters of the posterior, which has the following form</p><p><img src="tutDiscrimClassif_eq55655.png" alt="$$p(w,\sigma^2|D)  = N(w|w_N, V_N) IG(\sigma^2|a_N,b_N)$$"></p><p>Since we used an uninformative prior, the posterior mean is the same as the MLE</p><pre class="codeinput">assert(approxeq(wOLS, modelB.wN))
</pre><p>logev is the log evidence, or marginal likelihood, and is a measure of 'goodness of fit' of the overall model:</p><p><img src="tutDiscrimClassif_eq86927.png" alt="$$p(D) = \int \int p(D|w,\sigma^2) p(w,\sigma^2) d w d \sigma^2 $$"></p><p>This can only be computed if we use a proper prior, not an uninformative prior. Hence in this case <tt>logev=[]</tt>.</p><p><tt>postSummary</tt> is a summary of the posterior. It contains the the posterior mean, standard deviation and 95% credible interval of each regression parameters. It also determines if each  coefficient is significantly different from 0, based on whether its 95% CI excludes 0.</p><pre class="codeinput">postSummary
</pre><pre class="codeoutput">postSummary = 
       what: [11x1 double]
     stderr: [11x1 double]
    credint: [11x2 double]
        sig: [1 1 1 0 1 1 0 0 0 0 0]
</pre><p>We can print the posterior summary as a table using the command below. We put a little * next to the significant coefficients.</p><pre class="codeinput">[modelB, logev, postSummary] = linregFitBayes(X, y, <span class="string">'prior'</span>, <span class="string">'uninf'</span>, <span class="keyword">...</span>
  <span class="string">'displaySummary'</span>, true);
</pre><pre class="codeoutput">coeff mean       stddev     95pc CI              sig   
   w0   10.998   3.06027  [   4.652,   17.345]     * 
   w1   -0.004   0.00156  [  -0.008,   -0.001]     * 
   w2   -0.054   0.02190  [  -0.099,   -0.008]     * 
   w3    0.068   0.09947  [  -0.138,    0.274]       
   w4   -1.294   0.56381  [  -2.463,   -0.124]     * 
   w5    0.232   0.10438  [   0.015,    0.448]     * 
   w6   -0.357   1.56646  [  -3.605,    2.892]       
   w7   -0.237   1.00601  [  -2.324,    1.849]       
   w8    0.181   0.23672  [  -0.310,    0.672]       
   w9   -1.285   0.86485  [  -3.079,    0.508]       
  w10   -0.433   0.73487  [  -1.957,    1.091]       

</pre><p>We see that coefficients  0, 1, 2, 4, 5 are "significant" by this measure. (Other methods of testing significance, based on Bayes factors, can also be used, but are a bit more complicated to implement, and one cannot use uninformative priors when using Bayes factors.)</p><p>Note that pmtk currenlty has rather limited support for Bayesian model fitting, and not all Bayesian model fitting procedures currently implement this posterior summary feature.</p><h2>Frequentist parameter inference<a name="28"></a></h2><p>It turns out that in this particular example, the Bayesian analysis is identical to a classical frequentist analysis (because the posterior for linear regression under an uninformative Jeffreys prior is equivalent to the sampling distribution of the MLE). To see this, let us use the <a href="http://www.mathworks.com/products/statistics/">stats toolbox</a> to fit the model and perform a frequentist analysis:</p><pre class="codeinput">X1 = [ones(size(X,1),1), X];
[b, bint] = regress(y, X1);
<span class="comment">% b(j) is coefficient j, bint(j,:) = lower and upper 95% conf interval</span>
assert(approxeq(b, postSummary.what))
assert(approxeq(bint, postSummary.credint))
<span class="keyword">for</span> i=1:length(b)
  fprintf(<span class="string">'%8.3f, [%8.3f, %8.3f]\n'</span>, b(i), bint(i,1), bint(i,2));
<span class="keyword">end</span>
fprintf(<span class="string">'\n'</span>);
</pre><pre class="codeoutput">  10.998, [   4.652,   17.345]
  -0.004, [  -0.008,   -0.001]
  -0.054, [  -0.099,   -0.008]
   0.068, [  -0.138,    0.274]
  -1.294, [  -2.463,   -0.124]
   0.232, [   0.015,    0.448]
  -0.357, [  -3.605,    2.892]
  -0.237, [  -2.324,    1.849]
   0.181, [  -0.310,    0.672]
  -1.285, [  -3.079,    0.508]
  -0.433, [  -1.957,    1.091]

</pre><p>We see that the MLE is the same as the posterior mean, and the 95% frequentist confidence interval is the same as the 95% Bayesian credible interval.</p><p>In general, a Bayesian and frequentist analysis may not give the same results. In pmtk, all inference is Bayesian. However, pmtk supports some non-Bayesian estimation methods, such as cross validation, as we will see below.</p><h2>Using a model for prediction<a name="31"></a></h2><p>In machine learning, we usually care more about prediction than in trying to interpret the fitted parameters (especially since many models of interest are hard to interpret or even strictly unidentifiable).</p><p>Once the model has been created, you can use it to make predictions  as follows</p><pre>[yhat, py] = fooPredict(model, Xtest) % plugin approximation
[yhat, py] = fooPredictBayes(model, Xtest) % posterior predictive</pre><p>Here Xtest is an Ntest*D matrix of test inputs, and yhat is an Ntest*1 vector of predicted responses of the same type as ytrain. For regression this is the predicted mean, for classification this is the predicted mode (most probable class label). The meaning of py depends on the model, as follows:</p><div><ul><li>For regression, py is an Ntest*1 vector of predicted variances.</li><li>For binary classification, py is an Ntest*1 vector of the probability of being in class 1.</li><li>For multi-class, py is an Ntest*C matrix, where py(i,c) = p(y=c|Xtest(i,:),params)</li></ul></div><p>The difference between <tt>predict</tt> and <tt>predictBayes</tt> is as follows. <tt>predict</tt> computes <img src="tutDiscrimClassif_eq19507.png" alt="$p(y|x,\hat{\theta})$">, which "plugs in" a point estimate of the parameters, while <tt>predictBayes</tt> computes</p><p><img src="tutDiscrimClassif_eq20716.png" alt="$$p(y|x,D) = \int p(y|x,\theta) p(\theta|D) d\theta$"></p><p>This is called the (posterior) predictive density. In practice, the Bayesian approach results in similar (often identical) values for yhat, but quite different values for py. In particular, the uncertainty is reflected more accurately in the Bayesian approach, as we illustrate below.</p><h2>Prediction with linear regression<a name="36"></a></h2><p>As an example, consider fitting a linear regression model to some 1d data using MLE and Bayesian methods (using <a href="http://pmtk3.googlecode.com/svn/trunk/demos/otherDemos/supervisedModels/linregPostPredLinearDemo.m">linregPostPredLinearDemo.m</a> ), and then plotting the predictions on a test set (which is just a grid of points in the interval [-7,7])</p><pre class="codeinput">setSeed(1);
[xtrain, ytrain, xtest] =  polyDataMake(<span class="string">'sampling'</span>, <span class="string">'sparse'</span>, <span class="string">'deg'</span>, 2);

fitMethods = {@(x,y) linregFit(x,y), @(x,y) linregFitBayes(x,y)};
predictMethods = {@(x,y) linregPredict(x,y), @(x,y) linregPredictBayes(x,y)};
names = {<span class="string">'MLE'</span>, <span class="string">'Bayes'</span>};

<span class="keyword">for</span> i=1:length(fitMethods)
  model = fitMethods{i}(xtrain, ytrain);
  [mu, v] = predictMethods{i}(model, xtest);
  figure; hold <span class="string">on</span>
  plot(xtest, mu,  <span class="string">'k-'</span>, <span class="string">'linewidth'</span>, 3, <span class="string">'displayname'</span>, <span class="string">'prediction'</span>);
  plot(xtrain,ytrain,<span class="string">'ro'</span>,<span class="string">'markersize'</span>, 14, <span class="string">'linewidth'</span>, 3, <span class="keyword">...</span>
     <span class="string">'displayname'</span>, <span class="string">'training data'</span>);
  NN = length(xtest);
  ndx = 1:5:NN; <span class="comment">% plot subset of errorbars to reduce clutter</span>
  sigma = sqrt(v);
  legend(<span class="string">'location'</span>, <span class="string">'northwest'</span>);
  errorbar(xtest(ndx), mu(ndx), sigma(ndx));
  title(names{i});
<span class="keyword">end</span>
</pre><img vspace="5" hspace="5" src="tutDiscrimClassif_01.png" alt=""> <img vspace="5" hspace="5" src="tutDiscrimClassif_02.png" alt=""> <p>The predicted means (black lines) are the same, but  in the plugin case, the predicted variance is constant, whereas in the Bayesian case, the predicted variance increases as we move further away from the training data, as it should, since our uncertainty increases as we extrapolate further.</p><h2>Prediction with logistic regression<a name="39"></a></h2><p>As another example of this, consider fitting a binary logistic regression model to some SAT scores, where the response is whether the student passed or failed the class. First we compute the MLE and use a plugin approximation for prediction, as is standard practice (from <a href="http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Introduction/logregSATdemo.m">logregSATdemo.m</a> )</p><pre class="codeinput">close <span class="string">all</span>; clear <span class="string">all</span>
stat = loadData(<span class="string">'sat'</span>);  y = stat(:,1); X = stat(:,4);
model = logregFit(X, y);
[yhat, prob] = logregPredict(model, X); <span class="comment">%ok</span>
figure;
plot(X, y, <span class="string">'ko'</span>, <span class="string">'linewidth'</span>, 2, <span class="string">'MarkerSize'</span>, 7, <span class="string">'markerfacecolor'</span>, <span class="string">'k'</span>);
hold <span class="string">on</span>
plot(X, prob, <span class="string">'ro'</span>, <span class="string">'linewidth'</span>, 2,<span class="string">'MarkerSize'</span>, 10)
</pre><img vspace="5" hspace="5" src="tutDiscrimClassif_03.png" alt=""> <p>We see that the probability of passing the class smoothly increases as the SAT score goes up. Now let us fit the model using Bayesian inference with an noninformative Gaussian prior. By default, the fitting procedure uses a Laplace approximation to the posterior. To approximate the predictive density, we can plugin in the posterior mean:</p><p><img src="tutDiscrimClassif_eq03370.png" alt="$$p(y=1|x,D) = \int \sigma(w^T * x) N(w|\mu,\Sigma) dw&#xA;\approx \sigma(w^T \mu)$$"></p><p>However, this gives essentially the same result as plugging in the MLE. To get a measure of confidence in this prediction, we can sample values of w from their posterior (which we have approximated by a Gaussian), use each such sample to make a prediction, and then compute empirical quantiles of this distribution to get a 95% credible interval. This is done using <a href="http://pmtk3.googlecode.com/svn/trunk/toolbox/SupervisedModels/logisticRegression/logregPredictBayes.m">logregPredictBayes.m</a> and gives the results shown below (from <a href="http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Introduction/logregSATdemoBayes.m">logregSATdemoBayes.m</a> )</p><pre class="codeinput">model = logregFitBayes(X, y);
[yhat, prob, pCI] = logregPredictBayes(model, X);
figure;
plot(X, y, <span class="string">'ko'</span>, <span class="string">'linewidth'</span>, 2, <span class="string">'MarkerSize'</span>, 7, <span class="string">'markerfacecolor'</span>, <span class="string">'k'</span>);
hold <span class="string">on</span>
plot(X, prob, <span class="string">'ro'</span>, <span class="string">'linewidth'</span>, 2,<span class="string">'MarkerSize'</span>, 10)
<span class="keyword">for</span> i=1:size(X,1)
  line([X(i,1) X(i,1)], [pCI(i,1) pCI(i,2)]);
<span class="keyword">end</span>
</pre><img vspace="5" hspace="5" src="tutDiscrimClassif_04.png" alt=""> <p>Not all models support Bayesian fitting or prediction (the list will hopefully grow over time, as people add new methods). However, all models provide some measure of confidence on their outputs.</p><h2>Visualizing the decision boundaires<a name="46"></a></h2><p>When comparing classification methods, it is useful to apply them to 2d datasets and to plot the regions of space that get mapped to each class; these are called decision regions, and the boundaries between them are called decision boundaries. We can do this using the <tt>plotDecisionBoundary(X, y, predFn)</tt> function, where predFn(X) takes a test matrix and computes the MAP estimate of the labels for each row. As an example of this, consider the famous XOR dataset. Let us try fitting a logistic regression model to it in the original feature space (from <a href="http://pmtk3.googlecode.com/svn/trunk/demos/otherDemos/supervisedModels/logregXorLinearDemo.m">logregXorLinearDemo.m</a> )</p><pre class="codeinput">close <span class="string">all</span>; clear <span class="string">all</span>;
[X, y] = createXORdata();
model = logregFit(X, y);
plotDecisionBoundary(X, y, @(X)logregPredict(model, X));
yhat = logregPredict(model, X);
errorRate = mean(yhat ~= y)
</pre><pre class="codeoutput">errorRate =
    0.4875
</pre><img vspace="5" hspace="5" src="tutDiscrimClassif_05.png" alt=""> <p>We see that the method performs at chance level, because the data is not linearly separable. We give a simple fix to this problem below, when we discuss basis function expansion.</p><h2>Preprocessing, basis function expansion and kernels<a name="49"></a></h2><p>We are free to preprocess the data in any way we choose before fitting the model. In pmtk, you can create a preprocessor (pp) 'object', and then pass it to the fitting function; the pp will  be applied to the training data before fitting the model, and will be applied again to the test data. The advantage of this approach is that the pp is stored inside the model, which reduces the chance of applying inconsistent transformations to training and test data.</p><p>One common form of preprocessing is basis function expansion. This replaces the original features with a larger set, thus providing an easy way to fit nonlinear models. A popular approach is to define the new feature vector as follows:</p><p><img src="tutDiscrimClassif_eq24062.png" alt="$$\phi(x) = (K(x,\mu_1), ..., K(x,mu_D))$$"></p><p>where the <img src="tutDiscrimClassif_eq14632.png" alt="$\mu_j$"> are 'prototypes' and K(x,\mu) is a 'kernel function', which in this context just means a function of two arguments. A common example is the Gaussian or RBF kernel</p><p><img src="tutDiscrimClassif_eq89797.png" alt="$$K(x,\mu) = \exp(-\frac{||x-\mu||^2}{2\sigma^2})$$"></p><p>where <img src="tutDiscrimClassif_eq24873.png" alt="$\sigma$"> is the 'bandwidth'. (The quantity <img src="tutDiscrimClassif_eq21526.png" alt="$1/\sigma$"> is known as the scale or precision.) Another common example is the polynomial kernel</p><p><img src="tutDiscrimClassif_eq61193.png" alt="$$K(x,\mu) = (1+x^T \mu)^d$$"></p><p>where d is the degree. Often we take the prototypes <img src="tutDiscrimClassif_eq14632.png" alt="$\mu_j$"> to be the training vectors (rows of <img src="tutDiscrimClassif_eq03598.png" alt="$X$">), but we don't have to.</p><p>Below we show an example where we fit the XOR data using kernelized logistic regression, with various kernels and prototypes (from <a href="http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Introduction/logregXorDemo.m">logregXorDemo.m</a> ).</p><pre class="codeinput">clear <span class="string">all</span>; close <span class="string">all</span>
[X, y] = createXORdata();
rbfScale = 1;
polydeg  = 2;
protoTypes = [1 1; 1 5; 5 1; 5 5];
protoTypesStnd = standardizeCols(protoTypes);
kernels = {@(X1, X2)kernelRbfSigma(X1, protoTypesStnd, rbfScale)
           @(X1, X2)kernelRbfSigma(X1, X2, rbfScale)
           @(X1, X2)kernelPoly(X1, X2, polydeg)};
titles  = {<span class="string">'rbf'</span>, <span class="string">'rbf prototypes'</span>, <span class="string">'poly'</span>};
<span class="keyword">for</span> i=1:numel(kernels)
    preproc = preprocessorCreate(<span class="string">'kernelFn'</span>, kernels{i}, <span class="string">'standardizeX'</span>, true, <span class="string">'addOnes'</span>, true);
    model = logregFit(X, y, <span class="string">'preproc'</span>, preproc);
    yhat = logregPredict(model, X);
    errorRate = mean(yhat ~= y);
    fprintf(<span class="string">'Error rate using %s features: %2.f%%\n'</span>, titles{i}, 100*errorRate);
    predictFcn = @(Xtest)logregPredict(model, Xtest);
    plotDecisionBoundary(X, y, predictFcn);
    <span class="keyword">if</span> i==2
       hold <span class="string">on</span>;
       plot(protoTypes(:, 1), protoTypes(:, 2), <span class="string">'*k'</span>, <span class="string">'linewidth'</span>, 2, <span class="string">'markersize'</span>, 10)
    <span class="keyword">end</span>
    title(titles{i});
<span class="keyword">end</span>
</pre><pre class="codeoutput">Error rate using rbf features:  0%
Error rate using rbf prototypes features:  0%
Error rate using poly features:  0%
</pre><img vspace="5" hspace="5" src="tutDiscrimClassif_06.png" alt=""> <img vspace="5" hspace="5" src="tutDiscrimClassif_07.png" alt=""> <img vspace="5" hspace="5" src="tutDiscrimClassif_08.png" alt=""> <p>In the first example, we use an RBF kernel with centers at 4 manually chosen points, shown with black stars. In the second and third examples, we use an RBF and polynomial kernel, centered at all the training data. This only leaves the kernel parameters (<img src="tutDiscrimClassif_eq24873.png" alt="$\sigma$"> and <img src="tutDiscrimClassif_eq51528.png" alt="$d$">) to be specified. Below we discuss how to choose the kernel parameters automatically.</p><h2>Overfitting, regularization and MAP estimation<a name="58"></a></h2><p>Using maximum likelihood to train a model  often results in overfitting. This means that the model fits the training set well, but is overly complex and consequently performs poorly on test data. This is easiest to illustrate in the context of polynomial regression in 1d, as shown below (based on <a href="http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Decision_theory/linregPolyVsRegDemo.m">linregPolyVsRegDemo.m</a> )</p><pre class="codeinput">close <span class="string">all</span>; clear <span class="string">all</span>;
setSeed(0);
n=21;
[xtrain, ytrain, xtest, ytestNoisefree, ytest, sigma2] =<span class="keyword">...</span>
  polyDataMake(<span class="string">'sampling'</span>,<span class="string">'thibaux'</span>,<span class="string">'n'</span>,n);

deg = 14;
ytrain = centerCols(ytrain);
ytest = centerCols(ytest);
pp = preprocessorCreate(<span class="string">'poly'</span>, deg, <span class="string">'rescaleX'</span>, true, <span class="string">'standardizeX'</span>, false, <span class="string">'addOnes'</span>, false);
[pp, Xtrain] = preprocessorApplyToTrain(pp, xtrain);
[Xtest] = preprocessorApplyToTest(pp, xtest);
pp = preprocessorCreate( <span class="string">'standardizeX'</span>, false, <span class="string">'addOnes'</span>, false);

<span class="comment">% Fit model by MLE and plot</span>
model = linregFit(Xtrain, ytrain, <span class="string">'preproc'</span>, pp);
[ypredTest] = linregPredict(model, Xtest);
figure;
scatter(xtrain, ytrain,<span class="string">'b'</span>,<span class="string">'filled'</span>); hold <span class="string">on</span>;
plot(xtest, ypredTest, <span class="string">'k'</span>, <span class="string">'linewidth'</span>, 3);
</pre><img vspace="5" hspace="5" src="tutDiscrimClassif_09.png" alt=""> <p>We see that the fitted function is very 'wiggly', and fits the noise. This is common when we have very little data compared to the number of parameters (in this example, we have N=21 data points, and 15 parameters, since we fit a degree 14 polynomial).</p><p>Using Bayesian inference with an uninformative prior does not help. What we need is an informative prior, that encodes our preference for simpler models. A popular away to achieve this is to use a zero-mean spherical Gaussian prior of the form <img src="tutDiscrimClassif_eq54469.png" alt="$p(w) = N(w|0,\alpha^{-1} I)$">, where <img src="tutDiscrimClassif_eq87919.png" alt="$\alpha$"> is the precision (strength) of the prior. This says that, a priori, we expect the regression weights to be small, which means we believe the function is simple/ smooth (not "too wiggly"). We can compute the posterior of w with this prior using a variety of different models/ likelihood functions. But a computationally simpler approach is to use MAP estimation (aka regularization), which just computes the posterior mode, which is given by</p><p><img src="tutDiscrimClassif_eq93496.png" alt="$$\hat{w} = \arg \max_w \log p(w|D) = \arg \max_w \log p(D|w) + \log p(w)$$"></p><p>In the case of a Gaussian likelihood (linear regression) and Gaussian prior, we get</p><p><img src="tutDiscrimClassif_eq66205.png" alt="$$\log p(D|w) + \log p(w) = -\frac{\beta}{2} ||Xw - y||^2 -&#xA; \frac{\alpha}{2} ||w||^2 + \mbox{const}$$"></p><p>where <img src="tutDiscrimClassif_eq35555.png" alt="$\beta=1/\sigma^2$"> is the precision of the measurement noise. If we define <img src="tutDiscrimClassif_eq65989.png" alt="$\lambda = \alpha/ \beta$"> to be the amount of regularization, we can rewrite this as follows:</p><p><img src="tutDiscrimClassif_eq19845.png" alt="$$\hat{w} = \arg \min_w ||Xw - y||^2 + \lambda ||w||^2$$"></p><p>We see that this is a least squares problem with an L2 penalty on the weight vector (this is known as ridge regression). If <img src="tutDiscrimClassif_eq23351.png" alt="$\lambda$"> is too small, the model will overfit (since the function is too wiggly), but if it is too big, the model will underfit (since the function is too smooth). This is illustrated below, where we examine the mean squared error on the training and  test sets as a function of <img src="tutDiscrimClassif_eq23351.png" alt="$\lambda$">. This illustrates the characteristic U-shape on the test set (based on <a href="http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Decision_theory/linregPolyVsRegDemo.m">linregPolyVsRegDemo.m</a> ).</p><pre class="codeinput">lambdas = logspace(-10,1.3,10);
NL = length(lambdas);
printNdx = round(linspace(2, NL-1, 3));
testMse = zeros(1,NL); trainMse = zeros(1,NL);
<span class="keyword">for</span> k=1:NL
  lambda = lambdas(k);
  [model] = linregFit(Xtrain, ytrain, <span class="string">'lambda'</span>, lambda, <span class="string">'preproc'</span>, pp);
  [ypredTest, s2] = linregPredict(model, Xtest);
  ypredTrain = linregPredict(model, Xtrain);
  testMse(k) = mean((ypredTest - ytest).^2);
  trainMse(k) = mean((ypredTrain - ytrain).^2);
<span class="keyword">end</span>


hlam=figure; hold <span class="string">on</span>
ndx =  log(lambdas); <span class="comment">% 1:length(lambdas);</span>
plot(ndx, trainMse, <span class="string">'bs:'</span>, <span class="string">'linewidth'</span>, 2, <span class="string">'markersize'</span>, 12);
plot(ndx, testMse, <span class="string">'rx-'</span>, <span class="string">'linewidth'</span>, 2, <span class="string">'markersize'</span>, 12);
legend(<span class="string">'train mse'</span>, <span class="string">'test mse'</span>, <span class="string">'location'</span>, <span class="string">'northwest'</span>)
xlabel(<span class="string">'log lambda'</span>)
title(<span class="string">'mean squared error'</span>)
</pre><img vspace="5" hspace="5" src="tutDiscrimClassif_10.png" alt=""> <p>Below we print the fitted function for certain chosen lambdas</p><pre class="codeinput"><span class="keyword">for</span> k=printNdx
  lambda = lambdas(k);
  [model] = linregFit(Xtrain, ytrain, <span class="string">'lambda'</span>, lambda, <span class="string">'preproc'</span>, pp);
  [ypredTest, s2] = linregPredict(model, Xtest);
  ypredTrain = linregPredict(model, Xtrain);
  sig = sqrt(s2);
  figure;
  scatter(xtrain, ytrain,<span class="string">'b'</span>,<span class="string">'filled'</span>);
  hold <span class="string">on</span>;
  plot(xtest, ypredTest, <span class="string">'k'</span>, <span class="string">'linewidth'</span>, 3);
  plot(xtest, ypredTest + sig, <span class="string">'b:'</span>);
  plot(xtest, ypredTest - sig, <span class="string">'b:'</span>);
  title(sprintf(<span class="string">'ln lambda %5.3f'</span>, log(lambda)))
<span class="keyword">end</span>
</pre><img vspace="5" hspace="5" src="tutDiscrimClassif_11.png" alt=""> <img vspace="5" hspace="5" src="tutDiscrimClassif_12.png" alt=""> <img vspace="5" hspace="5" src="tutDiscrimClassif_13.png" alt=""> <h2>Cross validation for ridge regression<a name="70"></a></h2><p>One simple way to choose regularization parameters is cross validation. Below we show how to estimate the expected loss for a ridge regression model as we vary the regularizer. We use the <a href="http://matlabtools.googlecode.com/svn/trunk/stats/cvEstimate.m">cvEstimate.m</a> function, which can be used to estimate the frequentist risk of any estimation procedure (here each procedure corresponds to ridge regression with a different value of lambda) (based on <a href="http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Decision_theory/linregPolyVsRegDemo.m">linregPolyVsRegDemo.m</a> )</p><pre class="codeinput"><span class="keyword">for</span> k=1:NL
  lambda = lambdas(k);
  fitFn = @(Xtr,ytr) linregFit(Xtr, ytr, <span class="string">'lambda'</span>, lambda, <span class="string">'preproc'</span>, pp);
  predFn = @(mod, Xte) linregPredict(mod, Xte);
  lossFn = @(yhat, yte)  mean((yhat - yte).^2);
  N = size(Xtrain, 1);
  <span class="comment">%nfolds = N; % LOOCV</span>
  nfolds = 5;
  <span class="comment">% since the data is sorted left to right, we must randomize the order</span>
  [mu(k), se(k)] = cvEstimate(fitFn, predFn, lossFn, Xtrain, ytrain, nfolds, <span class="keyword">...</span>
    <span class="string">'randomizeOrder'</span>, true);
<span class="keyword">end</span>
</pre><p>We can plot the results as shown below. We see that it exhibits a U-shape similar to the test error. The vertical line denotes the best value.</p><pre class="codeinput">figure; hold <span class="string">on</span>
ndx =  log(lambdas); <span class="comment">% 1:length(lambdas);</span>
xlabel(<span class="string">'log lambda'</span>)
ylabel(<span class="string">'mse'</span>)
errorbar(ndx, mu, se, <span class="string">'ko-'</span>,<span class="string">'linewidth'</span>, 2, <span class="string">'markersize'</span>, 12 );
title(sprintf(<span class="string">'%d-fold cross validation, ntrain = %d'</span>, nfolds, N))
set(gca,<span class="string">'yscale'</span>,<span class="string">'log'</span>)
<span class="comment">% draw vertical line at best value</span>
dof = 1./(eps+lambdas);
idx_opt  = argmin(mu);
verticalLine(ndx(idx_opt), <span class="string">'color'</span>,<span class="string">'b'</span>, <span class="string">'linewidth'</span>,2);
</pre><img vspace="5" hspace="5" src="tutDiscrimClassif_14.png" alt=""> <h2>Bayesian evidence procedure for ridge regression<a name="74"></a></h2><p>An alternative to cross validation is to to compute log evidence for each value of alpha, as shown below. (To simplify things, we use the known noise variance) When we plot the log evidence vs alpha, it exhibits the same (inverted) U shape as the test error. (based on <a href="http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Decision_theory/linregPolyVsRegDemo.m">linregPolyVsRegDemo.m</a> )</p><pre class="codeinput">beta = 1/sigma2;
alphas = beta * lambdas;

<span class="keyword">for</span> k=1:NL
  lambda = lambdas(k);
  [model, logev(k)] = linregFitBayes(Xtrain, ytrain, <span class="string">'preproc'</span>, pp, <span class="keyword">...</span>
    <span class="string">'prior'</span>, <span class="string">'gauss'</span>, <span class="string">'alpha'</span>, alphas(k), <span class="string">'beta'</span>, beta);
  ypredTest = linregPredictBayes(model, Xtest);
  ypredTrain = linregPredictBayes(model, Xtrain);
  testMseB(k) = mean((ypredTest - ytest).^2);
  trainMseB(k) = mean((ypredTrain - ytrain).^2);
<span class="keyword">end</span>
<span class="comment">% Sanity check - Bayes with fixed sigma should be same as ridge</span>
assert(approxeq(testMseB, testMse))
assert(approxeq(trainMseB, trainMse))

<span class="comment">% Now we plot the log evidence vs alpha.</span>
figLogev = figure;
plot(log(alphas), logev, <span class="string">'ko-'</span>, <span class="string">'linewidth'</span>, 2, <span class="string">'markersize'</span>, 12);
xlabel(<span class="string">'log alpha'</span>)
title(<span class="string">'log evidence'</span>)
</pre><img vspace="5" hspace="5" src="tutDiscrimClassif_15.png" alt=""> <h2>Empirical Bayes for ridge regression<a name="75"></a></h2><p>The main benefit of the Bayesian approach is that we can use numerical optimization to pick the regularizer, rather than performing a discrete search over a finite grid of values. We just specify that the prior is of type 'eb', which stands for empirical Bayes. This is illustrated below. The vertical line corresponds to the best ML-II estimate. (This feature uses <a href="http://www1.aston.ac.uk/eas/research/groups/ncrg/resources/netlab/">netlab</a>.) (The code below is based on <a href="http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Decision_theory/linregPolyVsRegDemo.m">linregPolyVsRegDemo.m</a> )</p><pre class="codeinput">[modelEB, logevEB] = linregFitBayes(Xtrain, ytrain, <span class="string">'preproc'</span>, pp, <span class="string">'prior'</span>, <span class="string">'eb'</span>);
alphaEB = modelEB.netlab.alpha;
figure(figLogev);
verticalLine(log(alphaEB), <span class="string">'linewidth'</span>, 3, <span class="string">'color'</span>, <span class="string">'r'</span>);
</pre><img vspace="5" hspace="5" src="tutDiscrimClassif_16.png" alt=""> <h2>Variational Bayes for ridge regression<a name="77"></a></h2><p>An alternative to EB is to use variational Bayes to infer the posterior over <img src="tutDiscrimClassif_eq87919.png" alt="$\alpha$"> and <img src="tutDiscrimClassif_eq42727.png" alt="$\beta$">. This is illustrated below. The posterior mean value for <img src="tutDiscrimClassif_eq87919.png" alt="$\alpha$"> is shown by the blue line. We see this is very similar to the ML-II (EB) estimate, since we used a vague prior. (The code below is based on <a href="http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Decision_theory/linregPolyVsRegDemo.m">linregPolyVsRegDemo.m</a> )</p><pre class="codeinput">[modelVB, logevVB] = linregFitBayes(Xtrain, ytrain, <span class="string">'preproc'</span>, pp, <span class="string">'prior'</span>, <span class="string">'vb'</span>);
alphaVB = modelVB.expectAlpha;
figure(figLogev);
verticalLine(log(alphaVB), <span class="string">'linewidth'</span>, 3, <span class="string">'color'</span>, <span class="string">'b'</span>);
</pre><img vspace="5" hspace="5" src="tutDiscrimClassif_17.png" alt=""> <p class="footer"><br>
      Published with MATLAB&reg; 7.9<br></p></div><!--
##### SOURCE BEGIN #####
%% Supervised learning using discriminative models in pmtk3
% _This page was auto-generated by publishing_
% <http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/tutDiscrimClassif.m>.
%
%% Models
% The following is a list of pmtk models that are designed for 
% supervised learning (in alphabetical order).
% We have classified the models based on whether they can be used for
% classification $y \in \{1,\ldots,C\}$,
% regression $y \in R$, or both; whether they are generative models of
% the form $p(y,x|\theta)$ or discriminative models of the form $p(y|x,\theta)$;
% and whether they are parametric (so $\theta$ has fixed size) or
% non-parametric (so $\theta$ grows as the training set gets larger).
% We assume y is a low-dimensional scalar.
% Models for multivariate conditional density estimation (structured output
% classification/ regression) will be added later.
%
%%
% <html>
% <table border=1>
% <TR ALIGN=left>
% <td> Model 
% <td> Description
% <td> Classif/regr
% <td> Gen/Discr
% <td> Param/non
% <tr>
% <td> discrimAnalysis
% <td> Discriminant analysis (linear, quadratic, regularized, shrunken) 
% <td> Classif
% <td> Gen
% <td> Param
% <tr>
% <td> knn
% <td> k nearest neighbors
% <td> Classif
% <td> Gen
% <td> Nonparam
% <tr>
% <td> linreg
% <td> Linear regression
% <td> Regr
% <td> Discrim
% <td> Param
% <tr>
% <td> logreg
% <td> Logistic regression 
% <td> Classif
% <td> Discrim
% <td> Param
% <tr>
% <td> mlpClassif
% <td> multi-layer perceptron (aka feedforward neural network)
% <td> Classif
% <td> Discrim
% <td> Param
% <tr>
% <td> mlpRegress
% <td> multi-layer perceptron (aka feedforward neural network)
% <td> Regr
% <td> Discrim
% <td> Param
% <tr>
% <td> naiveBayes
% <td> Naive Bayes classifier
% <td> Classif
% <td> Gen
% <td> Param
% <tr> 
% <td> rvm
% <td> Relevance vector machine
% <td> Both
% <td> Discrim
% <td> Nonparam
% <tr>
% <td> svm
% <td> Support vector machine
% <td> Both
% <td> Discrim
% <td> Nonparam
% </table>
% </html>
%%
% More models may be added in the future.
%
%
%% Creating a model
% To create a model of type 'foo', use one of the following
%%
% model = fooCreate(...) % manually specify parameters
% model = fooFit(X, y, ...) % Compute ML or MAP estimate of params
% model = fooFitBayes(X, y, ...) % Compute posterior of params
%%
% where
%
% *  '...' refers to optional arguments (see below)
% * X  is an N*D design matrix containing the training data,
%  where N is the number of training cases and D is the number of features.
% * y is an N*1 response vector, which can be real-valued (regression),
%     0/1 or -1/+1 (binary classification), or 1:C (multi-class).
%
% If X contains missing values, represented as NaNs,
% it is best to use a generative model (although not all
% models currently support this functionality).
% NaNs in y correspond to semi-supervised learning, which
% is not yet supported.
%
% The resulting model is a Matlab structure;
% However, we will sometimes call it an 'object',
% since it behaves like one in many respects.
%
% In the case of |fooCreate| and |fooFit|, the parameters are point estimates.
% In the case of |fooFitBayes|, the parameters are represented as
% distributions, which may be represented parameterically
% or as a bag of samples. The details will be explained below
% when we look at specific model classes.
%
%% Fitting a linear regression model by maximum likelihood
% As an example, we can fit a linear regression model to the caterpillar
% data set using maximum likelihood estimation
% as follows (extracted from linregBayesCaterpillar.m):
%%
clear all
X = loadData('caterpillar'); % from http://www.ceremade.dauphine.fr/~xian/BCS/caterpillar
y = log(X(:,11)); % log number of nests
X = X(:,1:10);
[model] = linregFit(X, y) 
%%
% Let us check that this matches the usual equation for the MLE
% (adding a column of 1s to deal with the offset term)
%%
X1 = [ones(size(X,1),1) X];
wOLS = X1\y;
assert(approxeq(model.w, wOLS))
%%


%% Bayesian parameter inference
% If you fit by the model by MLE, you can examine the
% value of the estimated parameters by typing |model.params|,
% where params is the name of the parameter (here w or sigma2).
% But what if we want to know how much confidence
% we should have in these estimates?
% For this, we should use Bayesin inference.
% We can compute the posterior distribution of the
% parameters given the data and an uninformative prior
% as follows:
%%
[modelB, logev, postSummary] = linregFitBayes(X, y, 'prior', 'uninf');
%%
% Here |modelB| is the model which contains
% the posterior of the parameters:
%%
modelB
%%
% There is no longer a field called |w| or |sigma2|
% since we are not using point estimation.
% Instead, modelB contains the parameters
% of the posterior,
% which has the following form
%%
% $$p(w,\sigma^2|D)  = N(w|w_N, V_N) IG(\sigma^2|a_N,b_N)$$
%%
% Since we used an uninformative prior,
% the posterior mean is the same as the MLE
%%
assert(approxeq(wOLS, modelB.wN))
%%
%
% logev is the log evidence, or marginal likelihood,
% and is a measure of 'goodness of fit' of the overall model:
%%
% $$p(D) = \int \int p(D|w,\sigma^2) p(w,\sigma^2) d w d \sigma^2 $$
%%
% This can only be computed if we use a proper prior,
% not an uninformative prior.
% Hence in this case |logev=[]|.
%%
% |postSummary| is a summary of the posterior.
% It contains the 
% the posterior mean, standard deviation and 95% credible interval
% of each regression parameters.
% It also determines if each 
%  coefficient is significantly different from 0,
% based on whether its 95% CI excludes 0.
%%
postSummary
%%
% We can print the posterior summary as a table using the command
% below. We put 
% a little * next to the significant coefficients.
%%
[modelB, logev, postSummary] = linregFitBayes(X, y, 'prior', 'uninf', ...
  'displaySummary', true);
%%
% We see that coefficients  0, 1, 2, 4, 5 are "significant"
% by this measure. (Other methods of testing significance, based on Bayes factors, can also be
% used, but are a bit more complicated to implement, and one cannot use uninformative
% priors when using Bayes factors.)
%
% Note that pmtk currenlty has rather limited support for Bayesian
% model fitting, and not all Bayesian model fitting procedures currently implement
% this posterior summary feature.

%% Frequentist parameter inference
% It turns out that in this particular example, the Bayesian analysis is
% identical to a classical frequentist analysis (because the posterior
% for linear regression under an uninformative Jeffreys prior is equivalent
% to the sampling distribution of the MLE). To see this, let us use the
% <http://www.mathworks.com/products/statistics/ stats toolbox> to fit the model and perform a frequentist analysis:
%%
X1 = [ones(size(X,1),1), X];
[b, bint] = regress(y, X1);
% b(j) is coefficient j, bint(j,:) = lower and upper 95% conf interval
assert(approxeq(b, postSummary.what))
assert(approxeq(bint, postSummary.credint))
for i=1:length(b)
  fprintf('%8.3f, [%8.3f, %8.3f]\n', b(i), bint(i,1), bint(i,2));
end
fprintf('\n');
%%
% We see that the MLE is the same as the posterior mean,
% and the 95% frequentist confidence interval is the same as the 95%
% Bayesian credible interval.
%
%
% In general, a Bayesian and frequentist analysis may not give the same
% results. In pmtk, all inference is Bayesian.
% However, pmtk supports some non-Bayesian estimation methods,
% such as cross validation, as we will see below.



%% Using a model for prediction
% In machine learning, we usually care more about prediction than in trying
% to interpret the fitted parameters (especially since many models of
% interest are hard to interpret or even strictly unidentifiable).
%
% Once the model has been created, you can use it to make predictions
%  as follows
%
%%
%  [yhat, py] = fooPredict(model, Xtest) % plugin approximation
%  [yhat, py] = fooPredictBayes(model, Xtest) % posterior predictive
%%
% Here Xtest is an Ntest*D matrix of test inputs,
% and yhat is an Ntest*1 vector of predicted responses of the same type
% as ytrain.
% For regression this is the predicted mean, for classification this is the
% predicted mode (most probable class label).
% The meaning of py depends on the model, as follows:
%   
% * For regression, py is an Ntest*1 vector of predicted variances.
% * For binary classification, py is an Ntest*1 vector of the probability of being in class 1.
% * For multi-class, py is an Ntest*C matrix, where py(i,c) = p(y=c|Xtest(i,:),params)
%
% The difference between |predict| and |predictBayes| is as follows.
% |predict| computes $p(y|x,\hat{\theta})$, which "plugs in" a point estimate
% of the parameters, while |predictBayes| computes
%%
% $$p(y|x,D) = \int p(y|x,\theta) p(\theta|D) d\theta$
%%
% This is called the (posterior) predictive density.
% In practice, the Bayesian approach results in similar (often identical)
% values for yhat, but quite different values for py. In particular, the
% uncertainty is reflected more accurately in the Bayesian approach, as we
% illustrate below.

%% Prediction with linear regression
%
% As an example, consider fitting a linear regression model to some 1d data
% using MLE and Bayesian methods (using <http://pmtk3.googlecode.com/svn/trunk/demos/otherDemos/supervisedModels/linregPostPredLinearDemo.m linregPostPredLinearDemo.m> ),
% and then plotting the predictions on a test set (which is just a grid of
% points in the interval [-7,7])
%%
setSeed(1);
[xtrain, ytrain, xtest] =  polyDataMake('sampling', 'sparse', 'deg', 2);

fitMethods = {@(x,y) linregFit(x,y), @(x,y) linregFitBayes(x,y)};
predictMethods = {@(x,y) linregPredict(x,y), @(x,y) linregPredictBayes(x,y)};
names = {'MLE', 'Bayes'};

for i=1:length(fitMethods)
  model = fitMethods{i}(xtrain, ytrain);
  [mu, v] = predictMethods{i}(model, xtest);
  figure; hold on
  plot(xtest, mu,  'k-', 'linewidth', 3, 'displayname', 'prediction');
  plot(xtrain,ytrain,'ro','markersize', 14, 'linewidth', 3, ...
     'displayname', 'training data');
  NN = length(xtest);
  ndx = 1:5:NN; % plot subset of errorbars to reduce clutter
  sigma = sqrt(v);
  legend('location', 'northwest');
  errorbar(xtest(ndx), mu(ndx), sigma(ndx));
  title(names{i});
end
%%
% The predicted means (black lines) are the same, but  in the plugin case, the predicted
% variance is constant, whereas in the Bayesian case, the predicted
% variance increases as we move further away from the training data, as it
% should, since our uncertainty increases as we extrapolate further.
%


%% Prediction with logistic regression
% As another example of this, consider fitting a binary logistic regression
% model to some SAT scores, where the response is whether the student
% passed or failed the class. First we compute the MLE and use a plugin
% approximation for prediction, as is standard practice (from <http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Introduction/logregSATdemo.m logregSATdemo.m> )
%
%%
close all; clear all
stat = loadData('sat');  y = stat(:,1); X = stat(:,4);
model = logregFit(X, y);
[yhat, prob] = logregPredict(model, X); %ok
figure;
plot(X, y, 'ko', 'linewidth', 2, 'MarkerSize', 7, 'markerfacecolor', 'k');
hold on
plot(X, prob, 'ro', 'linewidth', 2,'MarkerSize', 10)

%%
% We see that the probability of passing the class smoothly increases as
% the SAT score goes up.
% Now let us fit the model using Bayesian inference with an noninformative
% Gaussian prior. By default, the fitting procedure uses a Laplace
% approximation to the posterior. To approximate the predictive density, we
% can plugin in the posterior mean:
%%
% $$p(y=1|x,D) = \int \sigma(w^T * x) N(w|\mu,\Sigma) dw 
% \approx \sigma(w^T \mu)$$
%%
% However, this gives essentially the same result as plugging in the MLE.
% To get a measure of confidence in this prediction, we can sample values
% of w from their posterior (which we have approximated by a Gaussian), use each such sample to make a
% prediction, and then compute empirical quantiles of this distribution to
% get a 95% credible interval.
% This is done using <http://pmtk3.googlecode.com/svn/trunk/toolbox/SupervisedModels/logisticRegression/logregPredictBayes.m logregPredictBayes.m> and gives the results shown below
% (from <http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Introduction/logregSATdemoBayes.m logregSATdemoBayes.m> )

%%
model = logregFitBayes(X, y);
[yhat, prob, pCI] = logregPredictBayes(model, X);
figure;
plot(X, y, 'ko', 'linewidth', 2, 'MarkerSize', 7, 'markerfacecolor', 'k');
hold on
plot(X, prob, 'ro', 'linewidth', 2,'MarkerSize', 10)
for i=1:size(X,1)
  line([X(i,1) X(i,1)], [pCI(i,1) pCI(i,2)]);
end

%%
%
% Not all models support Bayesian fitting or prediction (the list will hopefully grow
% over time, as people add new methods).
% However, all models provide some measure of confidence on their outputs.
%

%% Visualizing the decision boundaires
% When comparing classification methods, it is useful to apply them to 2d
% datasets and to plot the regions of space that get mapped to each class;
% these are called decision regions, and the boundaries between them are called decision
% boundaries. We can do this using the |plotDecisionBoundary(X, y, predFn)|
% function, where predFn(X) takes a test matrix and computes the MAP
% estimate of the labels for each row.
% As an example of this, consider the famous XOR dataset.
% Let us try fitting a logistic regression model to it in the original
% feature space (from <http://pmtk3.googlecode.com/svn/trunk/demos/otherDemos/supervisedModels/logregXorLinearDemo.m logregXorLinearDemo.m> )
%%
close all; clear all;
[X, y] = createXORdata();
model = logregFit(X, y);
plotDecisionBoundary(X, y, @(X)logregPredict(model, X));
yhat = logregPredict(model, X);
errorRate = mean(yhat ~= y)

%%
% We see that the method performs at chance level, because the data is not
% linearly separable. We give a simple fix to this problem below, when we
% discuss basis function expansion.

%% Preprocessing, basis function expansion and kernels
% We are free to preprocess the data in any way we choose before fitting the model.
% In pmtk, you can create a preprocessor (pp) 'object', and then pass it to the fitting function;
% the pp will  be applied to the training data before fitting the model, and will be applied again to the test data.
% The advantage of this approach is that the pp is stored inside the model,
% which reduces the chance of applying inconsistent transformations to
% training and test data.
%
% One common form of preprocessing is basis function expansion.
% This replaces the original features with a larger set, thus providing an easy way to fit nonlinear models.
% A popular approach is to define the new feature vector as follows:
%%
% $$\phi(x) = (K(x,\mu_1), ..., K(x,mu_D))$$
%%
% where the $\mu_j$ are 'prototypes'
% and K(x,\mu) is a 'kernel function', which in this context just means a function of two arguments.
% A common example is the Gaussian or RBF kernel
%%
% $$K(x,\mu) = \exp(-\frac{||x-\mu||^2}{2\sigma^2})$$
%%
% where $\sigma$ is the 'bandwidth'. (The quantity $1/\sigma$ is known as
% the scale or precision.)
% Another common example is the polynomial kernel
%%
% $$K(x,\mu) = (1+x^T \mu)^d$$
%%
% where d is the degree.
% Often we take the prototypes $\mu_j$ to be the training vectors (rows of $X$), but we don't have to.
%
% Below we show an example where we fit the XOR data using kernelized
% logistic regression, with various kernels and prototypes
% (from <http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Introduction/logregXorDemo.m logregXorDemo.m> ).
%%
clear all; close all
[X, y] = createXORdata();
rbfScale = 1;
polydeg  = 2;
protoTypes = [1 1; 1 5; 5 1; 5 5];
protoTypesStnd = standardizeCols(protoTypes);
kernels = {@(X1, X2)kernelRbfSigma(X1, protoTypesStnd, rbfScale)
           @(X1, X2)kernelRbfSigma(X1, X2, rbfScale)
           @(X1, X2)kernelPoly(X1, X2, polydeg)};
titles  = {'rbf', 'rbf prototypes', 'poly'};
for i=1:numel(kernels)
    preproc = preprocessorCreate('kernelFn', kernels{i}, 'standardizeX', true, 'addOnes', true);
    model = logregFit(X, y, 'preproc', preproc);
    yhat = logregPredict(model, X);
    errorRate = mean(yhat ~= y);
    fprintf('Error rate using %s features: %2.f%%\n', titles{i}, 100*errorRate);
    predictFcn = @(Xtest)logregPredict(model, Xtest);
    plotDecisionBoundary(X, y, predictFcn);
    if i==2
       hold on; 
       plot(protoTypes(:, 1), protoTypes(:, 2), '*k', 'linewidth', 2, 'markersize', 10)
    end
    title(titles{i});
end
%%
% In the first example, we use an RBF kernel with centers at 4
% manually chosen points, shown with black stars.
% In the second and third examples, we use an RBF and polynomial kernel,
% centered at all the training data. This only leaves the kernel parameters
% ($\sigma$ and $d$) to be specified.
% Below we discuss how to choose the kernel parameters automatically.
%

%% Overfitting, regularization and MAP estimation
% Using maximum likelihood to train a model  often results in overfitting.
% This means that the model fits the training set well, but is overly complex
% and consequently performs poorly on test data. This is easiest to
% illustrate in the context of polynomial regression in 1d, as shown below (based on
% <http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Decision_theory/linregPolyVsRegDemo.m linregPolyVsRegDemo.m> )
%%

close all; clear all;
setSeed(0);
n=21;
[xtrain, ytrain, xtest, ytestNoisefree, ytest, sigma2] =...
  polyDataMake('sampling','thibaux','n',n);

deg = 14;
ytrain = centerCols(ytrain);
ytest = centerCols(ytest);
pp = preprocessorCreate('poly', deg, 'rescaleX', true, 'standardizeX', false, 'addOnes', false);
[pp, Xtrain] = preprocessorApplyToTrain(pp, xtrain);
[Xtest] = preprocessorApplyToTest(pp, xtest);
pp = preprocessorCreate( 'standardizeX', false, 'addOnes', false);

% Fit model by MLE and plot
model = linregFit(Xtrain, ytrain, 'preproc', pp);
[ypredTest] = linregPredict(model, Xtest);
figure;
scatter(xtrain, ytrain,'b','filled'); hold on;
plot(xtest, ypredTest, 'k', 'linewidth', 3);

%%
% We see that the fitted function is very 'wiggly', and fits the noise.
% This is common when we have very little data compared to the number of parameters
% (in this example, we have 
% N=21 data points, and 15 parameters, since we fit a degree 14 polynomial).
%
% Using Bayesian inference with an uninformative prior does not help.
% What we need is an informative prior, that encodes our preference for
% simpler models. A popular away to achieve this is to use a zero-mean spherical
% Gaussian prior of the form $p(w) = N(w|0,\alpha^{-1} I)$,
% where $\alpha$ is the precision (strength) of the prior. This says
% that, a priori, we expect the regression weights to be small, which
% means we believe the function is simple/ smooth (not "too wiggly").
% We can compute the posterior of w with this prior using a variety of
% different models/ likelihood functions. But a computationally simpler
% approach is to use MAP estimation (aka regularization), which just computes the posterior
% mode, which is given by
%%
% $$\hat{w} = \arg \max_w \log p(w|D) = \arg \max_w \log p(D|w) + \log p(w)$$
%%
% In the case of a Gaussian likelihood (linear regression) and Gaussian
% prior, we get
%%
% $$\log p(D|w) + \log p(w) = -\frac{\beta}{2} ||Xw - y||^2 - 
%  \frac{\alpha}{2} ||w||^2 + \mbox{const}$$
%%
% where $\beta=1/\sigma^2$ is the precision of the measurement noise.
% If we define $\lambda = \alpha/ \beta$ to be the amount of regularization,
% we can rewrite this as follows:
%%
% $$\hat{w} = \arg \min_w ||Xw - y||^2 + \lambda ||w||^2$$
%%
% We see that this is a least squares problem with an L2 penalty on the
% weight vector (this is known as ridge regression).
% If $\lambda$ is too small, the model will overfit (since the function
% is too wiggly), but if it is too big, the model will underfit
% (since the function is too smooth). This is illustrated below, where we
% examine the mean squared error on the training and  test sets as a function
% of $\lambda$. This illustrates the characteristic U-shape on the test
% set
% (based on <http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Decision_theory/linregPolyVsRegDemo.m linregPolyVsRegDemo.m> ).

%%
lambdas = logspace(-10,1.3,10);
NL = length(lambdas);
printNdx = round(linspace(2, NL-1, 3));
testMse = zeros(1,NL); trainMse = zeros(1,NL);
for k=1:NL
  lambda = lambdas(k);
  [model] = linregFit(Xtrain, ytrain, 'lambda', lambda, 'preproc', pp);
  [ypredTest, s2] = linregPredict(model, Xtest);
  ypredTrain = linregPredict(model, Xtrain);
  testMse(k) = mean((ypredTest - ytest).^2);
  trainMse(k) = mean((ypredTrain - ytrain).^2);
end


hlam=figure; hold on
ndx =  log(lambdas); % 1:length(lambdas);
plot(ndx, trainMse, 'bs:', 'linewidth', 2, 'markersize', 12);
plot(ndx, testMse, 'rx-', 'linewidth', 2, 'markersize', 12);
legend('train mse', 'test mse', 'location', 'northwest')
xlabel('log lambda')
title('mean squared error')

%%
% Below we print the fitted function for certain chosen lambdas
%%
for k=printNdx
  lambda = lambdas(k);
  [model] = linregFit(Xtrain, ytrain, 'lambda', lambda, 'preproc', pp);
  [ypredTest, s2] = linregPredict(model, Xtest);
  ypredTrain = linregPredict(model, Xtrain);
  sig = sqrt(s2);
  figure;
  scatter(xtrain, ytrain,'b','filled');
  hold on;
  plot(xtest, ypredTest, 'k', 'linewidth', 3);
  plot(xtest, ypredTest + sig, 'b:');
  plot(xtest, ypredTest - sig, 'b:');
  title(sprintf('ln lambda %5.3f', log(lambda)))
end


%% Cross validation for ridge regression
% One simple way to choose regularization parameters is cross validation.
% Below we show how to estimate the expected loss for a ridge
% regression model as we vary the regularizer.
% We use the <http://matlabtools.googlecode.com/svn/trunk/stats/cvEstimate.m cvEstimate.m> function, which can be used
% to estimate the frequentist risk of any estimation procedure
% (here each procedure corresponds to ridge regression with a
% different value of lambda)
% (based on <http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Decision_theory/linregPolyVsRegDemo.m linregPolyVsRegDemo.m> )
%%
for k=1:NL
  lambda = lambdas(k);
  fitFn = @(Xtr,ytr) linregFit(Xtr, ytr, 'lambda', lambda, 'preproc', pp);
  predFn = @(mod, Xte) linregPredict(mod, Xte);
  lossFn = @(yhat, yte)  mean((yhat - yte).^2);
  N = size(Xtrain, 1);
  %nfolds = N; % LOOCV
  nfolds = 5;
  % since the data is sorted left to right, we must randomize the order
  [mu(k), se(k)] = cvEstimate(fitFn, predFn, lossFn, Xtrain, ytrain, nfolds, ...
    'randomizeOrder', true);
end

%%
% We can plot the results as shown below.
% We see that it exhibits a U-shape similar to the test error.
% The vertical line denotes the best value.
%%
figure; hold on
ndx =  log(lambdas); % 1:length(lambdas);
xlabel('log lambda')
ylabel('mse')
errorbar(ndx, mu, se, 'ko-','linewidth', 2, 'markersize', 12 );
title(sprintf('%d-fold cross validation, ntrain = %d', nfolds, N))
set(gca,'yscale','log')
% draw vertical line at best value
dof = 1./(eps+lambdas);
idx_opt  = argmin(mu);
verticalLine(ndx(idx_opt), 'color','b', 'linewidth',2);

%% Bayesian evidence procedure for ridge regression
% An alternative to cross validation is to
% to compute log evidence for each value of alpha, as shown below.
% (To simplify things, we use the known noise variance)
% When we plot the log evidence vs alpha,
% it exhibits the same (inverted) U shape as the test error.
% (based on <http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Decision_theory/linregPolyVsRegDemo.m linregPolyVsRegDemo.m> )

beta = 1/sigma2;
alphas = beta * lambdas;

for k=1:NL
  lambda = lambdas(k);
  [model, logev(k)] = linregFitBayes(Xtrain, ytrain, 'preproc', pp, ...
    'prior', 'gauss', 'alpha', alphas(k), 'beta', beta);
  ypredTest = linregPredictBayes(model, Xtest);
  ypredTrain = linregPredictBayes(model, Xtrain);
  testMseB(k) = mean((ypredTest - ytest).^2);
  trainMseB(k) = mean((ypredTrain - ytrain).^2);
end
% Sanity check - Bayes with fixed sigma should be same as ridge
assert(approxeq(testMseB, testMse))
assert(approxeq(trainMseB, trainMse))

% Now we plot the log evidence vs alpha.
figLogev = figure;
plot(log(alphas), logev, 'ko-', 'linewidth', 2, 'markersize', 12);
xlabel('log alpha')
title('log evidence')

%% Empirical Bayes for ridge regression
% The main benefit of the Bayesian approach is that we can 
% use numerical optimization to pick the regularizer,
% rather than performing a discrete search over a finite grid
% of values. We just specify that the prior is of type
% 'eb', which stands for empirical Bayes. This is illustrated below.
% The vertical line corresponds to the best ML-II estimate.
% (This feature uses
% <http://www1.aston.ac.uk/eas/research/groups/ncrg/resources/netlab/
% netlab>.)
% (The code below is based on <http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Decision_theory/linregPolyVsRegDemo.m linregPolyVsRegDemo.m> )
%%
[modelEB, logevEB] = linregFitBayes(Xtrain, ytrain, 'preproc', pp, 'prior', 'eb');
alphaEB = modelEB.netlab.alpha;
figure(figLogev);
verticalLine(log(alphaEB), 'linewidth', 3, 'color', 'r');

%% Variational Bayes for ridge regression
% An alternative to EB is to use variational Bayes to infer
% the posterior over $\alpha$ and $\beta$. This is illustrated
% below. The posterior mean value for $\alpha$ is
% shown by the blue line. We see this is very similar
% to the ML-II (EB) estimate, since we used a vague prior.
% (The code below is based on <http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Decision_theory/linregPolyVsRegDemo.m linregPolyVsRegDemo.m> )
%%
[modelVB, logevVB] = linregFitBayes(Xtrain, ytrain, 'preproc', pp, 'prior', 'vb');
alphaVB = modelVB.expectAlpha;
figure(figLogev);
verticalLine(log(alphaVB), 'linewidth', 3, 'color', 'b');




##### SOURCE END #####
--></body></html>