
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML is auto-generated from an M-file.
To make changes, update the M-file and republish this document.
      --><title>Tutorial on  pmtk3</title><meta name="generator" content="MATLAB 7.9"><meta name="date" content="2010-08-27"><meta name="m-file" content="tutFoundations"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head><body><div class="content"><h1>Tutorial on  pmtk3</h1><!--introduction--><p><i>This page was auto-generated by publishing</i> <a href="http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/tutFoundations.m">http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/tutFoundations.m</a>.</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Overall design of PMTK3</a></li><li><a href="#6">Conditional (supervised) models</a></li><li><a href="#9">Unconditional (unsupervised) models</a></li><li><a href="#12">Basic models</a></li><li><a href="#27">Latent variable models</a></li><li><a href="#30">Graphical models</a></li><li><a href="#33">Summary of models and methods</a></li><li><a href="#38">Bayesian methods</a></li><li><a href="#45">Passing in optional arguments</a></li><li><a href="#54">Generally useful Matlab functions</a></li></ul></div><h2>Overall design of PMTK3<a name="1"></a></h2><p>PMTK3 has an object oriented design. That is, it can be thought of as defining a series of 'classes', representing different kinds of probabilistic models. Each class  supports various 'methods', which perform certain operations. These methods are often implemented in a functional way.</p><p>We don't actually use Matlab's object oriented system, because this does not work in Octave. In addition, some users find such code harder to understand, and it can be slower than non OO code. Instead, each model 'object' (an instance of a model 'class') is actually just a structure, containing various fields. And each method is just a regular function, whose name begins with the class name. The first argument to a method is a model struct.</p><p>As an example, below we create a 2d Gaussian model and then draw 5 samples from it</p><pre class="codeinput">m = gaussCreate([0 0], eye(2))
setSeed(0); X = gaussSample(m, 5)
</pre><pre class="codeoutput">m = 
           mu: [0 0]
        Sigma: [2x2 double]
    modelType: 'gauss'
X =
   -0.4326   -1.6656
    0.1253    0.2877
   -1.1465    1.1909
    1.1892   -0.0376
    0.3273    0.1746
</pre><p>The function 'gaussCreate' is called a constructor, since it creates an instance of the class. The function 'gaussSample' is a method. The methods that are supported depend on the type of model; details are given below.</p><p>To provide some overall structure, we group the model classes into two major types:</p><div><ul><li>unconditional / unsupervised</li><li>conditional / supervised</li></ul></div><p>These support different functions, as we explain below.</p><h2>Conditional (supervised) models<a name="6"></a></h2><p>This is a model of the form <img src="tutFoundations_eq31060.png" alt="$p(y|x, \theta)$">, where x is the set of covariats/ inputs, and y is the response. Currently we require y to be a scalar. If <img src="tutFoundations_eq14315.png" alt="$y \in R$">, we are performing regression; if <img src="tutFoundations_eq35391.png" alt="$y \in \{1,\ldots,C\}$">, we are performing classification. We discuss supervised models in more detail <a href="http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/html/tutSupervised.html">here</a>.</p><p>All conditional models should support the functions listed below. In the table,  'foo' is the name of the model class and '...' refers to optional or model-specific arguments (these will be explained later).</p><p>
<TABLE BORDER=3 CELLPADDING=5 WIDTH="100%" >
<TR ALIGN=left>
<TH WIDTH=40%  BGCOLOR=#00CCFF><FONT COLOR=000000>Method</FONT></TH>
<TH WIDTH=60% BGCOLOR=#00CCFF><FONT COLOR=000000>Description</FONT></TH>
</TR>
<tr>
<td> m = fooCreate(...)
<td> Constructor
<tr>
<td> m = fooFit(X, y, ...)
<td> Constructor. Usually computes the MLE
or MAP parameter estimate, using various priors and  fitting algorithms.
X is an N*D design matrix, where
N is the number of training cases, and D is the dimensionality
of the distribution being fit.
y is the N*1 response vector.
<tr>
<td> [yhat, py] = fooPredict(m, X, ...)
<td> The meaning of the outputs depends on the model class.
For classification, yhat(i) = argmax p(y|X(i,:), m),
and py(i,c) = p(y=c|X(i,:), m).
(Note that some models cannot
produce probabilistic outputs. In such cases, py may be
undefined.)
For regression, yhat(i) = E[y|X(i,:),m]
and py(i) = Var[y|X(i,:), m].
<tr>
<td>
<tr>
</table>
</p><p>Most of the work occurs inside the fitting function. See <a href="http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/html/tutSupervised.html">here</a> for details.</p><h2>Unconditional (unsupervised) models<a name="9"></a></h2><p>An unconditional model is of the form <img src="tutFoundations_eq02877.png" alt="$p(y|\theta)$">, where y is potentially vector valued. Such models support the following functions, although in some cases, some functions may not yet have been implemented for a particular model class.</p><p>
<TABLE BORDER=3 CELLPADDING=5 WIDTH="100%" >
<TR ALIGN=left>
<TH WIDTH=40%  BGCOLOR=#00CCFF><FONT COLOR=000000>Method</FONT></TH>
<TH WIDTH=60% BGCOLOR=#00CCFF><FONT COLOR=000000>Description</FONT></TH>
</TR>
<td>  m = fooCreate(...)
<td> Constructor. Allows user to specify the parameters 'by hand',
as well as specifying optional arguments to be used by fitting
and/or inference routines.
<tr>
<td> m = fooFit(X,  ...)
<td> Constructor. Usually computes the MLE
or MAP parameter estimate, using various priors and  fitting algorithms.
X is an N*D design matrix.
For some models, X may contain NaN's, representing missing values.
<tr>
<td>  X = fooSample(m, N)
<td> X(i,:) = sample from model m, i=1:N
<tr>
<td>  L = fooLogprob(m, X)
<td> L(i) = log p(X(i,:)| m)
For some models, X may contain NaN's, representing missing values.
<tr>
</table>
</p><p>Unconditional models are subdivided into various subtypes, as follows:</p><div><ul><li>basic (standard parametric distributions e.g., Gauss)</li><li>latent (mixture models, latent factor models, HMMs etc)</li><li>graphical (models which require specifying a graph structure)</li></ul></div><p>We discuss these  below.</p><h2>Basic models<a name="12"></a></h2><p>Basic models support the functions listed above. We have already illustrated the <tt>gaussCreate</tt> constructor and <tt>gaussSample</tt> method. We now discuss some other methods.</p><p>First, as a piece of useful shorthand, some widely used basic models (such as Gaussian) let you write <tt>fooSample(params,N)</tt> instead of <tt>fooSample(fooCreate(params),N)</tt>. For example</p><pre class="codeinput">setSeed(0); gaussSample(zeros(2,1), eye(2), 5)
</pre><pre class="codeoutput">ans =
   -0.4326   -1.6656
    0.1253    0.2877
   -1.1465    1.1909
    1.1892   -0.0376
    0.3273    0.1746
</pre><p>Now we illustrate model fitting. First we use MLE to fit a D=50 dimensional Gaussian to N = 1000 data points.</p><pre class="codeinput">N = 1000; D = 50;
X = rand(N,D);
m = gaussFit(X);
</pre><p>Let us check that the resulting  estimate  matches the usual MLE formula:</p><pre class="codeinput">assert(approxeq(m.mu, mean(X)))
assert(approxeq(m.Sigma, cov(X, 1)))
</pre><p>Let us also check that <img src="tutFoundations_eq13128.png" alt="$\hat{\Sigma}$"> is well conditioned</p><pre class="codeinput">cond(m.Sigma)
</pre><pre class="codeoutput">ans =
    2.3152
</pre><p>Now consider what happens when N=50.</p><pre class="codeinput">m2 = gaussFit(X(1:50,:));
cond(m2.Sigma)
</pre><pre class="codeoutput">ans =
  1.1240e+017
</pre><p>We see that the resulting matrix is close to singular. MLE only works well when <img src="tutFoundations_eq93225.png" alt="$N \gg D$">. In all other cases, we should use MAP estimation. We can do this as follows, using a simple vague data-dependent prior:</p><pre class="codeinput">m3 = gaussFit(X(1:50,:), <span class="string">'map'</span>);
cond(m3.Sigma)
</pre><pre class="codeoutput">ans =
  177.0245
</pre><p>See <a href="http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/The_multivariate_Gaussian_and_friends/shrinkcovDemo.m">shrinkCovDemo</a> for a more extensive demo of regularized estimation of covariance matrices.</p><p>Now let us consider another useful operation: evaluating the log likelihood of a dataset:</p><pre class="codeinput">L = sum(gaussLogprob(m3, X))
</pre><pre class="codeoutput">L =
 -2.8987e+007
</pre><p>These methods behave similarly on other models. A list of all the basic models can be found <a href="http://pmtk3.googlecode.com/svn/trunk/docs/modelLists/modelList.html">here</a>. A complete list of the methods implemented by each basic class is shown <a href="http://pmtk3.googlecode.com/svn/trunk/docs/modelsByMethods/basicModels.html">here</a>.</p><h2>Latent variable models<a name="27"></a></h2><p>A list of all the LVMs can be found <a href="http://pmtk3.googlecode.com/svn/trunk/docs/modelLists/modelList.html">here</a> From a functional point of view, an LVM supports all the methods for a generic unconditional model, plus the following functions.</p><p>
<TABLE BORDER=3 CELLPADDING=5 WIDTH="100%" >
<TR ALIGN=left>
<TH WIDTH=40%  BGCOLOR=#00CCFF><FONT COLOR=000000>Method</FONT></TH>
<TH WIDTH=60% BGCOLOR=#00CCFF><FONT COLOR=000000>Description</FONT></TH>
</TR>
<td> [z, pz] = infer(m, X).
<td> This is an unsupervised version of predict.
If the LVM has discrete latent variables z,
z(i) = argmax p(z|X(i,:), m)
and pz(i,k) = p(z=k|X(i,:), m).
If the LVM has continuous latent variables z,
z(i,:) = E(z|X(i,:), m)
and pz(i,:,:) = Cov(z|X(i,:), m).
<tr>
</table>
</p><p>We discuss LVMs in more detail <a href="http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/html/tutLVM.html">here</a>.</p><h2>Graphical models<a name="30"></a></h2><p>A list of all the GMs can be found <a href="http://pmtk3.googlecode.com/svn/trunk/docs/modelLists/modelList.html">here</a> From a functional point of view, a GM supports all the methods for a generic unconditional model, plus the following functions.</p><p>
<TABLE BORDER=3 CELLPADDING=5 WIDTH="100%" >
<TR ALIGN=left>
<TH WIDTH=40%  BGCOLOR=#00CCFF><FONT COLOR=000000>Method</FONT></TH>
<TH WIDTH=60% BGCOLOR=#00CCFF><FONT COLOR=000000>Description</FONT></TH>
</TR>
<td> [bel] = inferNodes(m, evidence).
<td> Here bel{i}(k) is the probability node i is in state k,
given the evidence. The evidence can be specified
in several different ways. See
<http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/html/tutGM.html
here> for details.
<tr>
<td> yhat = map(m, evidence)
<td> Computes argmax p(y|ev, m), which is a (joint) posterior mode.
<tr>
<td> Ghat = fitStruct(m, evidence)
<td> Find a MAP estimate of the graph structure (not yet implemented).
</table>
</p><p>We discuss graphical models in more detail <a href="http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/html/tutGM.html">here</a>.</p><h2>Summary of models and methods<a name="33"></a></h2><p>A summary of the abstract classes and their main methods is shown below</p><p>
<img src="http://pmtk3.googlecode.com/svn/trunk/docs/classSystem/modelsMethodsTree.png">.
</p><p>A summary of the concrete classes (which are leaves in the class hierarchy) is shown below.</p><p>
<img src="http://pmtk3.googlecode.com/svn/trunk/docs/classSystem/modelsConcreteTree.png">.
</p><h2>Bayesian methods<a name="38"></a></h2><p>So far, we have been focusing on fitting models using ML or MAP parameter estimation. Furthermore, prediction and inference has used the plugin approximation, treating the parameters as known constants. PMTK3 has some limited support for Bayesian methods of model fitting/ prediction, as we now explain.</p><p>For supervised models, just use <tt>fooFitBayes</tt> which computes <img src="tutFoundations_eq87332.png" alt="$p(\theta|D)$">, and <tt>fooPredictBayes</tt>, which computes</p><p><img src="tutFoundations_eq51280.png" alt="$$p(y|x,D) = \int p(y|x,\theta) p(\theta|D) d\theta$$"></p><p>For unsupervised models, instead of <tt>fooInfer</tt> use <tt>fooInferBayes</tt>, which computes</p><p><img src="tutFoundations_eq35284.png" alt="$$p(z|x,D) = \int p(z|x,\theta) p(\theta|D) d\theta$$"></p><p>where z are the latent variables and x are the observed variables. Also, instead of <tt>fooLogprob</tt> use <tt>fooLogprobBayes</tt>, which computes</p><p><img src="tutFoundations_eq58905.png" alt="$$L(i) = \int p(X(i,:) | \theta) p(\theta|D) d\theta$$"></p><p>We will give examples of these methods later.</p><p>'Under the hood', things are a bit more complicated in the Bayesian context compared to MAP estimation, since we don't store model parameters but a posterior distribution over model parameters. Furthermore, the form of this posterior may differ across inference algorithms, e.g., it might be a Gaussian approximation, or a bag of samples. However, from a user's point of view, these things should not matter.</p><h2>Passing in optional arguments<a name="45"></a></h2><p>Many functions take a large number of optional arguments. For example, linregFit (which fit a linear regression model) has the following interface</p><pre>[model] = linregFit(X, y, varargin)</pre><p>varargin represents a variable number of arguments. The optional arguments, and their default values, are printed when you type help('linregFit').</p><pre class="codeinput">help <span class="string">linregFit</span>
</pre><pre class="codeoutput">  Fit a linear regression model by MLE or MAP estimation
  INPUTS
  X             ... N*D design matrix
  y             ... N*1 response vector
  OPTIONAL INPUTS:
  regType       ... L1, L2, none, scad (only used if likelihood is 'gaussian')
  likelihood    ... ['gaussian'], 'student', 'huber'
  lambda        ... regularizer
  fitOptions    ... optional  args (a cell array) to fitFn
  preproc       ... a struct, passed to preprocessorApplyToTtrain
 
  OUTPUTS:
  model         ... a struct, which you can pass directly to linregPredict
 %

</pre><p>The meaning of these arguments will be explained later.</p><p>There are two ways to specify the optional arguments: 1) a set of name, value pairs eg.</p><pre class="codeinput">clear <span class="string">all</span>
N=10; D = 2; X = rand(N, D); y = rand(N,1);
m1 = linregFit(X, y, <span class="string">'regType'</span>, <span class="string">'L2'</span>, <span class="string">'lambda'</span>, 2)
</pre><pre class="codeoutput">m1 = 
        lambda: 2
             w: [3x1 double]
        sigma2: 0.0022
       preproc: [1x1 struct]
     modelType: 'linreg'
    likelihood: 'gaussian'
</pre><p>2) a struct, where the fields are named after the optional arguments, eg.</p><pre class="codeinput">s.regType = <span class="string">'L2'</span>;
s.lambda = 2;
m2 = linregFit(X, y, s)
</pre><pre class="codeoutput">m2 = 
        lambda: 2
             w: [3x1 double]
        sigma2: 0.0022
       preproc: [1x1 struct]
     modelType: 'linreg'
    likelihood: 'gaussian'
</pre><p>As a sanity check, let us check these are equal</p><pre class="codeinput">assert(approxeq(m1.w, m2.w))
</pre><p>Internally, these optional arguments are processed using <a href="http://matlabtools.googlecode.com/svn/trunk/util/process_options.m">process_options.m</a>, written by Mark Paskin, and <a href="http://matlabtools.googlecode.com/svn/trunk/util/prepareArgs.m">prepareArgs.m</a>, written by Matt Dunham. For more detals, see <a href="http://yagtom.googlecode.com/svn/trunk/html/writingFunctions.html#40">this entry</a> in our Matlab tutorial.</p><h2>Generally useful Matlab functions<a name="54"></a></h2><p>PMTK uses a large number of  functions that are useful for many different purposes outside of machine learning. Many of these are stored in our <a href="http://code.google.com/p/matlabtools/">matlabTools</a> site. Others are built-in to Matlab, as explained in our <a href="http://code.google.com/p/yagtom/">Matlab tutorial</a>. We mention just a few of the most useful ones below</p><div><ul><li>whoCallsMe('foo'): list all files that call foo.m</li><li>wwhich('foo*'): which with wildcards. This prints full filenames of all files starting   with the string 'foo'. Can also use wwhich('*foo*') etc.</li><li>edit('foo'): open foo.m in editor, lets you look at source code   (be careful not to change things accidently!). This also works   for built-in Matlab functions.</li><li>help('foo'): prints any initial documentation for foo.m that     the implementer may have provided; all builtin Matlab functions     are properly documented. Unfortuntately that is not the case for     all the PMTK functions... but you can always read the source.</li></ul></div><p class="footer"><br>
      Published with MATLAB&reg; 7.9<br></p></div><!--
##### SOURCE BEGIN #####
%% Tutorial on  pmtk3
% _This page was auto-generated by publishing_
% <http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/tutFoundations.m>.


%% Overall design of PMTK3
% PMTK3 has an object oriented design.
% That is, it can be thought of as defining a series of 'classes',
% representing different kinds of probabilistic models.
% Each class  supports
% various 'methods', which perform certain operations.
% These methods are often implemented in a functional way.
%
% We don't actually use Matlab's object oriented system,
% because this does not work in Octave.
% In addition, some users find such code harder to understand,
% and it can be slower than non OO code.
% Instead, each model 'object' (an instance of a model 'class')
% is actually just a structure, containing various fields.
% And each method is just a regular function, whose name begins
% with the class name. The first argument to a method 
% is a model struct. 
%
% As an example, below we create a 2d Gaussian model
% and then draw 5 samples from it
%%
m = gaussCreate([0 0], eye(2))
setSeed(0); X = gaussSample(m, 5)
%%
% The function 'gaussCreate' is called a constructor,
% since it creates an instance of the class.
% The function 'gaussSample' is a method.
% The methods that are supported depend on the type of model;
% details are given below.
%
% To provide some overall structure, we group the model
% classes into two major types:
%%
% * unconditional / unsupervised
% * conditional / supervised
%%
% These support different functions, as we explain below.

%% Conditional (supervised) models 
% This is a model of the form $p(y|x, \theta)$,
% where x is the set of covariats/ inputs,
% and y is the response. Currently we require y to be a scalar.
% If $y \in R$, we are performing regression;
% if $y \in \{1,\ldots,C\}$, we are performing classification.
% We discuss supervised models in more detail 
% <http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/html/tutSupervised.html here>.
%
% All conditional models should support the functions listed below.
% In the table,  'foo' is the name of the model class
% and '...' refers to optional or model-specific arguments
% (these will be explained later).
%%
% <html>
% <TABLE BORDER=3 CELLPADDING=5 WIDTH="100%" >
% <TR ALIGN=left>
% <TH WIDTH=40%  BGCOLOR=#00CCFF><FONT COLOR=000000>Method</FONT></TH>
% <TH WIDTH=60% BGCOLOR=#00CCFF><FONT COLOR=000000>Description</FONT></TH>
% </TR>
% <tr>
% <td> m = fooCreate(...)
% <td> Constructor
% <tr>
% <td> m = fooFit(X, y, ...)
% <td> Constructor. Usually computes the MLE
% or MAP parameter estimate, using various priors and  fitting algorithms.
% X is an N*D design matrix, where
% N is the number of training cases, and D is the dimensionality
% of the distribution being fit. 
% y is the N*1 response vector.
% <tr>
% <td> [yhat, py] = fooPredict(m, X, ...)
% <td> The meaning of the outputs depends on the model class.
% For classification, yhat(i) = argmax p(y|X(i,:), m),
% and py(i,c) = p(y=c|X(i,:), m). 
% (Note that some models cannot
% produce probabilistic outputs. In such cases, py may be
% undefined.)
% For regression, yhat(i) = E[y|X(i,:),m]
% and py(i) = Var[y|X(i,:), m]. 
% <tr>
% <td>
% <tr>
% </table>
% </html>
%%
% Most of the work occurs inside the fitting function.
% See 
% <http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/html/tutSupervised.html here>
% for details.

%% Unconditional (unsupervised) models
% An unconditional model is of the form $p(y|\theta)$,
% where y is potentially vector valued.
% Such models support the following functions,
% although in some cases, some functions may not
% yet have been implemented for a particular model class.
%%
% <html>
% <TABLE BORDER=3 CELLPADDING=5 WIDTH="100%" >
% <TR ALIGN=left>
% <TH WIDTH=40%  BGCOLOR=#00CCFF><FONT COLOR=000000>Method</FONT></TH>
% <TH WIDTH=60% BGCOLOR=#00CCFF><FONT COLOR=000000>Description</FONT></TH>
% </TR>
% <td>  m = fooCreate(...)
% <td> Constructor. Allows user to specify the parameters 'by hand',
% as well as specifying optional arguments to be used by fitting
% and/or inference routines.
% <tr>
% <td> m = fooFit(X,  ...)
% <td> Constructor. Usually computes the MLE
% or MAP parameter estimate, using various priors and  fitting algorithms.
% X is an N*D design matrix.
% For some models, X may contain NaN's, representing missing values.
% <tr>
% <td>  X = fooSample(m, N)
% <td> X(i,:) = sample from model m, i=1:N
% <tr>
% <td>  L = fooLogprob(m, X)
% <td> L(i) = log p(X(i,:)| m) 
% For some models, X may contain NaN's, representing missing values.
% <tr>
% </table>
% </html>
%%
% Unconditional models are subdivided
% into various subtypes, as follows:
%
% * basic (standard parametric distributions e.g., Gauss)
% * latent (mixture models, latent factor models, HMMs etc)
% * graphical (models which require specifying a graph structure)
%
% We discuss these  below.

%% Basic models
% Basic models support the functions listed above.
% We have already illustrated the |gaussCreate| constructor
% and |gaussSample| method.
% We now discuss some other methods.
%
% First, as a piece of useful shorthand,
% some widely used basic models (such as Gaussian)
% let you write |fooSample(params,N)|
% instead of |fooSample(fooCreate(params),N)|.
% For example
%%
setSeed(0); gaussSample(zeros(2,1), eye(2), 5)
%%
% Now we illustrate model fitting.
% First we use MLE to fit a D=50 dimensional Gaussian
% to N = 1000 data points.
%%
N = 1000; D = 50;
X = rand(N,D);
m = gaussFit(X);
%%
% Let us check that the resulting  estimate
%  matches the usual MLE formula:
%%
assert(approxeq(m.mu, mean(X)))
assert(approxeq(m.Sigma, cov(X, 1)))
%%
% Let us also check that $\hat{\Sigma}$ is well conditioned
%%
cond(m.Sigma)
%%
% Now consider what happens when N=50.
%%
m2 = gaussFit(X(1:50,:));
cond(m2.Sigma)
%% 
% We see that the resulting matrix is close
% to singular. MLE only works well when $N \gg D$. In all
% other cases, we should use MAP estimation.
% We can do this as follows, using a simple vague data-dependent prior:
%%
m3 = gaussFit(X(1:50,:), 'map');
cond(m3.Sigma)
%%
% See
% <http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/The_multivariate_Gaussian_and_friends/shrinkcovDemo.m shrinkCovDemo>
% for a more extensive demo of regularized estimation
% of covariance matrices.
%
% Now let us consider another useful operation:
% evaluating the log likelihood of a dataset:
%%
L = sum(gaussLogprob(m3, X))
%%
% These methods behave similarly on other models.
% A list of all the basic models can be found
% <http://pmtk3.googlecode.com/svn/trunk/docs/modelLists/modelList.html
% here>.
% A complete list of the methods
% implemented by each basic class is shown
% <http://pmtk3.googlecode.com/svn/trunk/docs/modelsByMethods/basicModels.html here>.


%% Latent variable models
% A list of all the LVMs can be found
% <http://pmtk3.googlecode.com/svn/trunk/docs/modelLists/modelList.html
% here>
% From a functional point of view, an LVM supports
% all the methods for a generic unconditional model,
% plus the following functions.
%%
% <html>
% <TABLE BORDER=3 CELLPADDING=5 WIDTH="100%" >
% <TR ALIGN=left>
% <TH WIDTH=40%  BGCOLOR=#00CCFF><FONT COLOR=000000>Method</FONT></TH>
% <TH WIDTH=60% BGCOLOR=#00CCFF><FONT COLOR=000000>Description</FONT></TH>
% </TR>
% <td> [z, pz] = infer(m, X).
% <td> This is an unsupervised version of predict.
% If the LVM has discrete latent variables z,
% z(i) = argmax p(z|X(i,:), m)
% and pz(i,k) = p(z=k|X(i,:), m).
% If the LVM has continuous latent variables z,
% z(i,:) = E(z|X(i,:), m)
% and pz(i,:,:) = Cov(z|X(i,:), m).
% <tr>
% </table>
% </html>
%%
% We discuss LVMs in more detail 
% <http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/html/tutLVM.html
% here>.

%% Graphical models
% A list of all the GMs can be found
% <http://pmtk3.googlecode.com/svn/trunk/docs/modelLists/modelList.html
% here>
% From a functional point of view, a GM supports
% all the methods for a generic unconditional model,
% plus the following functions.
%%
% <html>
% <TABLE BORDER=3 CELLPADDING=5 WIDTH="100%" >
% <TR ALIGN=left>
% <TH WIDTH=40%  BGCOLOR=#00CCFF><FONT COLOR=000000>Method</FONT></TH>
% <TH WIDTH=60% BGCOLOR=#00CCFF><FONT COLOR=000000>Description</FONT></TH>
% </TR>
% <td> [bel] = inferNodes(m, evidence).
% <td> Here bel{i}(k) is the probability node i is in state k,
% given the evidence. The evidence can be specified
% in several different ways. See 
% <http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/html/tutGM.html
% here> for details.
% <tr> 
% <td> yhat = map(m, evidence)
% <td> Computes argmax p(y|ev, m), which is a (joint) posterior mode.
% <tr> 
% <td> Ghat = fitStruct(m, evidence)
% <td> Find a MAP estimate of the graph structure (not yet implemented).
% </table>
% </html>
%%
% We discuss graphical models in more detail
% <http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/html/tutGM.html
% here>.
%% Summary of models and methods
% 
% A summary of the abstract classes and their main methods is shown below
%%
% <html>
% <img src="http://pmtk3.googlecode.com/svn/trunk/docs/classSystem/modelsMethodsTree.png">.
% </html>
%%
% A summary of the concrete classes (which are leaves in the class
% hierarchy) is shown below.
%%
% <html>
% <img src="http://pmtk3.googlecode.com/svn/trunk/docs/classSystem/modelsConcreteTree.png">.
% </html>
%%

%% Bayesian methods
% So far, we have been focusing on fitting models
% using ML or MAP parameter estimation.
% Furthermore, prediction and inference has used
% the plugin approximation, treating the parameters
% as known constants.
% PMTK3 has some limited support for Bayesian methods
% of model fitting/ prediction, as we now explain.
%
% For supervised models, just use |fooFitBayes|
% which computes $p(\theta|D)$,
% and |fooPredictBayes|, which computes
%%
% $$p(y|x,D) = \int p(y|x,\theta) p(\theta|D) d\theta$$
%%
%
% For unsupervised models, instead of |fooInfer| use
% |fooInferBayes|, which computes
%%
% $$p(z|x,D) = \int p(z|x,\theta) p(\theta|D) d\theta$$
%%
% where z are the latent variables and x are the observed variables.
% Also, instead of |fooLogprob| use |fooLogprobBayes|, which computes
%%
% $$L(i) = \int p(X(i,:) | \theta) p(\theta|D) d\theta$$
%%
% We will give examples of these methods later.
%
% 'Under the hood', things are a bit more complicated in
% the Bayesian context compared to MAP estimation, since we don't store model
% parameters but a posterior distribution over model 
% parameters. Furthermore, the form of this posterior
% may differ across inference algorithms, e.g., it might
% be a Gaussian approximation, or a bag of samples.
% However, from a user's point of view, these things
% should not matter. 
% 

%% Passing in optional arguments
% Many functions take a large number of optional arguments.
% For example, linregFit (which fit a linear regression model)
% has the following interface
%
%  [model] = linregFit(X, y, varargin)
%
% varargin represents a variable number of arguments.
% The optional arguments, and their default values, are printed
% when you type help('linregFit').
%%
help linregFit
%%
% The meaning of these arguments will be explained
% later.
%
% There are two ways to specify the optional arguments:
% 1) a set of name, value pairs eg.
%%
clear all
N=10; D = 2; X = rand(N, D); y = rand(N,1);
m1 = linregFit(X, y, 'regType', 'L2', 'lambda', 2)
%%
% 2) a struct, where the fields are named after the optional
% arguments, eg.
%%
s.regType = 'L2'; 
s.lambda = 2;
m2 = linregFit(X, y, s)
%%
% As a sanity check, let us check these are equal
%%
assert(approxeq(m1.w, m2.w))
%%
% Internally, these optional arguments are processed using
% <http://matlabtools.googlecode.com/svn/trunk/util/process_options.m
% process_options.m>, written by Mark Paskin, and
% <http://matlabtools.googlecode.com/svn/trunk/util/prepareArgs.m
% prepareArgs.m>, written by Matt Dunham.
% For more detals, see
% <http://yagtom.googlecode.com/svn/trunk/html/writingFunctions.html#40
% this entry> in our Matlab tutorial.


%% Generally useful Matlab functions
% PMTK uses a large number of  functions
% that are useful for many different purposes outside of
% machine learning. Many of these are stored
% in our <http://code.google.com/p/matlabtools/ matlabTools> site.
% Others are built-in to Matlab, as explained
% in our <http://code.google.com/p/yagtom/ Matlab tutorial>.
% We mention just a few of the most useful ones below
%
% * whoCallsMe('foo'): list all files that call foo.m
% * wwhich('foo*'): which with wildcards. This prints full filenames of all files starting
%   with the string 'foo'. Can also use wwhich('*foo*') etc.
% * edit('foo'): open foo.m in editor, lets you look at source code
%   (be careful not to change things accidently!). This also works
%   for built-in Matlab functions. 
% * help('foo'): prints any initial documentation for foo.m that
%     the implementer may have provided; all builtin Matlab functions
%     are properly documented. Unfortuntately that is not the case for
%     all the PMTK functions... but you can always read the source.


##### SOURCE END #####
--></body></html>