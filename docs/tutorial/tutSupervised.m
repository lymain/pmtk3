%% Tutorial on supervised learning using pmtk3
% _This page was auto-generated by publishing_
% <http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/tutSupervised.m>.
% _Do not edit it directly!_
%
% We provide a description of the main methods for fitting and using univariate
% conditional density models of the form $p(y|x,\theta)$ and $p(y,x|\theta)$
% where y in R for regression and y in {1,...,C} for classification,
% and where x is some kind of feature vector. 
%
%% Models
% The following is a list of pmtk models that are designed for 
% supervised learning.
%
% * 'linreg' (linear regression)
% * 'logreg' (logistic regression  binary and multiclass)
% * 'mlp' (multilayer perceptron, aka feedforward neural network)
% * 'naiveBayesBer' (NB with Bernoulli features)
% * 'naiveBayesGauss' (NB with Gaussian features)
% * 'discrimAnalysis' (LDA or QDA)
% * 'RDA' (regularized LDA)
%
%
%% Methods
% Below we describe the main 'methods' that can be applied to these models.
% Note that these are just Matlab functions, but we will sometimes call
% them methods since they behave like object-oriented methods.
%
% Note that *not all models support all methods*.
% To find out if a model of type foo supports method bar,
% just type |help fooBar|. If you get an error, then you know
% that foo does not implement bar.
%
%% Creating a model
% To create a model of type 'foo', use one of the following
%%
%  model = fooCreate(...) % manually specified parameters
% model = fooFit(X, y, ...) % Compute ML or MAP estimate of params
% model = fooFitBayes(X, y, ...) % Compute posterior on params
%%
% where
%
%  *  '...' refers to optional arguments (see below)
%  * X  is an N*D design matrix containing the training data,
%  where N is the number of training cases and D is the number of features.
%  * y is an N*1 response vector, which can be real-valued (regression),
%     0/1 or -1/+1 (binary classification), or 1:C (multi-class).
%
% The resulting model is a Matlab structure, rather than an object.
% However, we will sometimes call it an object, since it behaves like one.
% In the case of fooCreate and fooFit, the parameters are point estimates.
% In the case of fooFitBayes, the parameters are represented as
% distributions; this is often represented parametrically in terms
% of the hyper-parameters. The details will be explained below
% when we look at specific model classes.
%
%% Using a model for prediction
% Once the model has been created, you can use it to make predictions
% (using a plug-in estimate of the parameters) as follows
%
%%
%  [yhat, py] = fooPredict(model, Xtest)
%%
% Here yhat is an Ntest*1 vector of predicted responses of the same type
% as ytrain, where Ntest is the number of rows in Xtest.
% For regression this is the predicted mean, for classification this is the predicted mode.
% The meaning of py depends on the model, as follows:
%   
%   * For regression, py is an Ntest*1 vector of predicted variances.
%   * For binary classification, py is an Ntest*1 vector of the probability of being in class 1.
%   * For multi-class, py is an Ntest*C matrix, where py(i,c) = p(y=c|Xtest(i,:),params)
%