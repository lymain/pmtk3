
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML is auto-generated from an M-file.
To make changes, update the M-file and republish this document.
      --><title>Ridge Regression with Polynomial Basis Expansion</title><meta name="generator" content="MATLAB 7.10"><meta name="date" content="2010-05-24"><meta name="m-file" content="linregCvPolyVsRegDemo"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head><body><div class="content"><h1>Ridge Regression with Polynomial Basis Expansion</h1><!--introduction--><p>Compare effect of regularizer strength</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#2">Make the data</a></li><li><a href="#3">Basis function expansion</a></li><li><a href="#4">Now compare CV with train/test error</a></li><li><a href="#5">draw vertical line at best value</a></li></ul></div><pre class="codeinput">ns = [21, 100];
<span class="keyword">for</span> ni=1:length(ns)
</pre><h2>Make the data<a name="2"></a></h2><pre class="codeinput">    n = ns(ni);
    xtrain = linspace(0, 20, n)';
    setSeed(0);
    xtest = (0:0.1:20)';
    sigma2 = 3;
    w = [-1.5; 1/9];
    fun = @(x) w(1)*x + w(2)*x.^2;
    ytrain = fun(xtrain) + randn(size(xtrain,1),1)*sqrt(sigma2);
    ytestNoisefree = fun(xtest);
    ytest = ytestNoisefree +  randn(size(xtest,1),1)*sqrt(sigma2);
</pre><h2>Basis function expansion<a name="3"></a></h2><pre class="codeinput">    deg = 14;
    [Xtrain] = rescaleData(xtrain);
    Xtrain = degexpand(Xtrain, deg, false);
    [Xtest] = rescaleData(xtest);
    Xtest = degexpand(Xtest, deg, false);
</pre><h2>Now compare CV with train/test error<a name="4"></a></h2><pre class="codeinput">    lambdas = logspace(-2, 1.5, 9);
    NL = length(lambdas);
    logev = zeros(1, NL);
    testMse = zeros(1, NL);
    trainMse = zeros(1, NL);
    mu = zeros(1, NL);
    se = zeros(1, NL);
    <span class="keyword">for</span> k=1:NL
        lambda = lambdas(k);
        fitFn = @(Xtr,ytr) linregFit(Xtr, ytr, <span class="string">'lambda'</span>, lambda);
        predFn = @(w, Xte) linregPredict(w, Xte);
        lossFn = @(yhat, yte)  mean((yhat - yte).^2);
        [mu(k), se(k)] = cvEstimate(fitFn, predFn, lossFn, Xtrain, ytrain, 5);
        model = linregFit(Xtrain, ytrain, <span class="string">'lambda'</span>, lambda);
        ypredTest = linregPredict(model, Xtest);
        ypredTrain = linregPredict(model, Xtrain);
        testMse(k) = mean((ypredTest - ytest).^2);
        trainMse(k) = mean((ypredTrain - ytrain).^2);
    <span class="keyword">end</span>


    figure; hold <span class="string">on</span>
    ndx =  log(lambdas); <span class="comment">% 1:length(lambdas);</span>
    plot(ndx, trainMse, <span class="string">'bs:'</span>, <span class="string">'linewidth'</span>, 2, <span class="string">'markersize'</span>, 12);
    plot(ndx, testMse, <span class="string">'rx-'</span>, <span class="string">'linewidth'</span>, 2, <span class="string">'markersize'</span>, 12);
    xlabel(<span class="string">'log regularizer'</span>)
    ylabel(<span class="string">'mse'</span>)
    errorbar(ndx, mu, se, <span class="string">'ko-'</span>,<span class="string">'linewidth'</span>, 2, <span class="string">'markersize'</span>, 12 );
    legend(<span class="string">'train'</span>, <span class="string">'test'</span>, <span class="string">'5-CV'</span>)
    title(sprintf(<span class="string">'ridge regression, ntrain = %d'</span>, n))
    set(gca,<span class="string">'yscale'</span>,<span class="string">'log'</span>)
    ylabel(<span class="string">'log mse'</span>)
</pre><img vspace="5" hspace="5" src="linregCvPolyVsRegDemo_01.png" alt=""> <img vspace="5" hspace="5" src="linregCvPolyVsRegDemo_03.png" alt=""> <h2>draw vertical line at best value<a name="5"></a></h2><pre class="codeinput">    dof = 1./(eps+lambdas);
    idx_opt = oneStdErrorRule(mu, se, dof);
    ylim = get(gca, <span class="string">'ylim'</span>);
    x = ndx(idx_opt);
    h=line([x x], ylim);
    set(h, <span class="string">'color'</span>, <span class="string">'k'</span>, <span class="string">'linewidth'</span>, 2);
    printPmtkFigure(sprintf(<span class="string">'linregCVPolyVsReg%d-mse'</span>, n))
</pre><img vspace="5" hspace="5" src="linregCvPolyVsRegDemo_02.png" alt=""> <img vspace="5" hspace="5" src="linregCvPolyVsRegDemo_04.png" alt=""> <pre class="codeinput"><span class="keyword">end</span>
</pre><p class="footer"><br>
      Published with MATLAB&reg; 7.10<br></p></div><!--
##### SOURCE BEGIN #####
%% Ridge Regression with Polynomial Basis Expansion
% Compare effect of regularizer strength
%%
ns = [21, 100];
for ni=1:length(ns)
    %% Make the data
    n = ns(ni);
    xtrain = linspace(0, 20, n)';
    setSeed(0);
    xtest = (0:0.1:20)';
    sigma2 = 3;
    w = [-1.5; 1/9];
    fun = @(x) w(1)*x + w(2)*x.^2;
    ytrain = fun(xtrain) + randn(size(xtrain,1),1)*sqrt(sigma2);
    ytestNoisefree = fun(xtest);
    ytest = ytestNoisefree +  randn(size(xtest,1),1)*sqrt(sigma2);
    %% Basis function expansion
    deg = 14;
    [Xtrain] = rescaleData(xtrain);
    Xtrain = degexpand(Xtrain, deg, false);
    [Xtest] = rescaleData(xtest);
    Xtest = degexpand(Xtest, deg, false);
    %% Now compare CV with train/test error
    lambdas = logspace(-2, 1.5, 9);
    NL = length(lambdas);
    logev = zeros(1, NL);
    testMse = zeros(1, NL);
    trainMse = zeros(1, NL);
    mu = zeros(1, NL);
    se = zeros(1, NL);
    for k=1:NL
        lambda = lambdas(k);
        fitFn = @(Xtr,ytr) linregFit(Xtr, ytr, 'lambda', lambda);
        predFn = @(w, Xte) linregPredict(w, Xte);
        lossFn = @(yhat, yte)  mean((yhat - yte).^2);
        [mu(k), se(k)] = cvEstimate(fitFn, predFn, lossFn, Xtrain, ytrain, 5);
        model = linregFit(Xtrain, ytrain, 'lambda', lambda);
        ypredTest = linregPredict(model, Xtest);
        ypredTrain = linregPredict(model, Xtrain);
        testMse(k) = mean((ypredTest - ytest).^2);
        trainMse(k) = mean((ypredTrain - ytrain).^2);
    end
    
    
    figure; hold on
    ndx =  log(lambdas); % 1:length(lambdas);
    plot(ndx, trainMse, 'bs:', 'linewidth', 2, 'markersize', 12);
    plot(ndx, testMse, 'rx-', 'linewidth', 2, 'markersize', 12);
    xlabel('log regularizer')
    ylabel('mse')
    errorbar(ndx, mu, se, 'ko-','linewidth', 2, 'markersize', 12 );
    legend('train', 'test', '5-CV')
    title(sprintf('ridge regression, ntrain = %d', n))
    set(gca,'yscale','log')
    ylabel('log mse')
    
    %% draw vertical line at best value
    dof = 1./(eps+lambdas);
    idx_opt = oneStdErrorRule(mu, se, dof);
    ylim = get(gca, 'ylim');
    x = ndx(idx_opt);
    h=line([x x], ylim);
    set(h, 'color', 'k', 'linewidth', 2);
    printPmtkFigure(sprintf('linregCVPolyVsReg%d-mse', n))
   
end

##### SOURCE END #####
--></body></html>