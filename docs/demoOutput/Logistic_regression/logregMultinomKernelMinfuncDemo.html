
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML is auto-generated from an M-file.
To make changes, update the M-file and republish this document.
      --><title>logregMultinomKernelMinfuncDemo</title><meta name="generator" content="MATLAB 7.10"><meta name="date" content="2010-05-24"><meta name="m-file" content="logregMultinomKernelMinfuncDemo"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head><body><div class="content"><h2>Contents</h2><div><ul><li><a href="#1">Minfunc Kernelized Logreg Demo</a></li><li><a href="#3">Linear</a></li><li><a href="#4">Polynomial</a></li><li><a href="#5">RBF</a></li><li><a href="#6">Check against PMTK functions</a></li><li><a href="#7">Compute training errors</a></li></ul></div><h2>Minfunc Kernelized Logreg Demo<a name="1"></a></h2><pre class="codeinput"><span class="comment">%PMTKauthor Mark Schmidt</span>
<span class="comment">%PMTKmodified Kevin Murphy</span>
<span class="comment">%PMTKurl http://people.cs.ubc.ca/~schmidtm/Software/minFunc/minFunc.html#7</span>
<span class="comment">% It is modified by replacing penalizedKernelL2_matrix,</span>
<span class="comment">% which uses sum_c w(:,c)' K w(:,c) as the regularizer,</span>
<span class="comment">% with the simpler penalizedL2, which uses w' w as the regularizer.</span>
<span class="comment">% The key difference is that we only use kernels to do basis funcion</span>
<span class="comment">% expansion on X; we do not change the regularizer.</span>
<span class="comment">% This makes hardly any difference to the training error.</span>
</pre><pre class="codeinput">options.Display = <span class="string">'none'</span>;
setSeed(0);
nClasses = 5;
<span class="comment">%nInstances = 1000;</span>
nInstances = 100;
nVars = 2;

[X,y] = makeData(<span class="string">'multinomialNonlinear'</span>,nInstances,nVars,nClasses);

figure;
[n,p] = size(X);
colors = getColorsRGB;
hold <span class="string">on</span>
<span class="keyword">for</span> c = 1:nClasses
   <span class="keyword">if</span> p == 3
      plot(X(y==c,2),X(y==c,3),<span class="string">'.'</span>,<span class="string">'color'</span>,colors(c,:));
   <span class="keyword">else</span>
      plot(X(y==c,1),X(y==c,2),<span class="string">'.'</span>,<span class="string">'color'</span>,colors(c,:));
   <span class="keyword">end</span>
<span class="keyword">end</span>
lambda = 1e-2;
</pre><img vspace="5" hspace="5" src="logregMultinomKernelMinfuncDemo_01.png" alt=""> <h2>Linear<a name="3"></a></h2><pre class="codeinput">funObj = @(w)SoftmaxLoss2(w,X,y,nClasses);
fprintf(<span class="string">'Training linear multinomial logistic regression model...\n'</span>);
wLinear = minFunc(@penalizedL2,zeros(nVars*(nClasses-1),1),options,funObj,lambda);
wLinear = reshape(wLinear,[nVars nClasses-1]);
wLinear = [wLinear zeros(nVars,1)];
</pre><pre class="codeoutput">Training linear multinomial logistic regression model...
</pre><h2>Polynomial<a name="4"></a></h2><pre class="codeinput">polyOrder = 2;
Kpoly = kernelPoly(X,X,polyOrder);
funObj = @(u)SoftmaxLoss2(u,Kpoly,y,nClasses);
fprintf(<span class="string">'Training kernel(poly) multinomial logistic regression model...\n'</span>);
uPoly = minFunc(@penalizedL2,randn(nInstances*(nClasses-1),1),options,funObj,lambda);
<span class="comment">%uPoly = minFunc(@penalizedKernelL2_matrix,randn(nInstances*(nClasses-1),1),options,Kpoly,nClasses-1,funObj,lambda);</span>
uPoly = reshape(uPoly,[nInstances nClasses-1]);
uPoly = [uPoly zeros(nInstances,1)];
</pre><pre class="codeoutput">Training kernel(poly) multinomial logistic regression model...
</pre><h2>RBF<a name="5"></a></h2><pre class="codeinput">rbfScale = 1;
Krbf = kernelRbfSigma(X,X,rbfScale);
funObj = @(u)SoftmaxLoss2(u,Krbf,y,nClasses);
fprintf(<span class="string">'Training kernel(rbf) multinomial logistic regression model...\n'</span>);
uRBF = minFunc(@penalizedL2,randn(nInstances*(nClasses-1),1),options,funObj,lambda);
<span class="comment">%uRBF = minFunc(@penalizedKernelL2_matrix,randn(nInstances*(nClasses-1),1),options,Krbf,nClasses-1,funObj,lambda);</span>
uRBF = reshape(uRBF,[nInstances nClasses-1]);
uRBF = [uRBF zeros(nInstances,1)];
</pre><pre class="codeoutput">Training kernel(rbf) multinomial logistic regression model...
</pre><h2>Check against PMTK functions<a name="6"></a></h2><pre class="codeinput">preproc.kernelFn = @(X1, X2)kernelRbfSigma(X1, X2, rbfScale);
preproc.standardizeX = false;
preproc.includeOffset = false;
modelRBF = logregFit(X, y, <span class="string">'lambda'</span>, lambda, <span class="string">'preproc'</span>, preproc,<span class="keyword">...</span>
    <span class="string">'fitOptions'</span>, struct());
wRBF = modelRBF.w;
assert(approxeq(wRBF, uRBF))
</pre><pre class="codeoutput"> Iteration   FunEvals     Step Length    Function Val        Opt Cond
         1          3    9.63700e-003    1.38835e+002    4.61207e+002
         2          4    1.00000e+000    1.28373e+002    3.23486e+002
         3          5    1.00000e+000    1.14303e+002    3.17865e+002
         4          6    1.00000e+000    1.08104e+002    3.07475e+002
         5          7    1.00000e+000    1.02452e+002    3.09276e+002
         6          8    1.00000e+000    9.61473e+001    1.52031e+002
         7          9    1.00000e+000    9.48450e+001    7.94099e+001
         8         10    1.00000e+000    9.40250e+001    9.39168e+001
         9         11    1.00000e+000    9.26562e+001    1.30076e+002
        10         12    1.00000e+000    9.10380e+001    1.31587e+002
        11         13    1.00000e+000    8.72903e+001    1.24001e+002
        12         14    1.00000e+000    8.24974e+001    8.93699e+001
        13         15    1.00000e+000    7.89207e+001    1.65335e+002
        14         16    1.00000e+000    7.75824e+001    5.44365e+001
        15         17    1.00000e+000    7.72322e+001    5.95352e+001
        16         18    1.00000e+000    7.60312e+001    5.05804e+001
        17         19    1.00000e+000    7.43538e+001    4.98504e+001
        18         20    1.00000e+000    7.16363e+001    5.64223e+001
        19         21    1.00000e+000    7.02056e+001    4.03362e+001
        20         22    1.00000e+000    6.95113e+001    1.83987e+001
        21         23    1.00000e+000    6.93240e+001    2.03769e+001
        22         24    1.00000e+000    6.89643e+001    3.02539e+001
        23         25    1.00000e+000    6.85945e+001    3.56420e+001
        24         26    1.00000e+000    6.82552e+001    2.84570e+001
        25         27    1.00000e+000    6.80217e+001    1.01729e+001
        26         28    1.00000e+000    6.79330e+001    1.00454e+001
        27         29    1.00000e+000    6.78984e+001    1.28531e+001
        28         30    1.00000e+000    6.78231e+001    1.51754e+001
        29         31    1.00000e+000    6.77007e+001    1.54700e+001
        30         32    1.00000e+000    6.76641e+001    3.97211e+001
        31         33    1.00000e+000    6.75660e+001    1.11767e+001
        32         34    1.00000e+000    6.75403e+001    5.40205e+000
        33         35    1.00000e+000    6.75226e+001    6.68557e+000
        34         36    1.00000e+000    6.74960e+001    8.81787e+000
        35         37    1.00000e+000    6.74251e+001    1.38772e+001
        36         38    1.00000e+000    6.73581e+001    1.29940e+001
        37         39    1.00000e+000    6.73071e+001    6.97812e+000
        38         40    1.00000e+000    6.72929e+001    2.95746e+000
        39         41    1.00000e+000    6.72893e+001    4.19348e+000
        40         42    1.00000e+000    6.72817e+001    5.52003e+000
        41         43    1.00000e+000    6.72646e+001    7.49507e+000
        42         44    1.00000e+000    6.72355e+001    7.80533e+000
        43         45    1.00000e+000    6.72066e+001    6.11069e+000
        44         46    1.00000e+000    6.71989e+001    5.33311e+000
        45         47    1.00000e+000    6.71968e+001    1.24189e+000
        46         48    1.00000e+000    6.71963e+001    9.54460e-001
        47         49    1.00000e+000    6.71960e+001    1.05231e+000
        48         50    1.00000e+000    6.71949e+001    1.13193e+000
        49         51    1.00000e+000    6.71933e+001    1.42263e+000
        50         52    1.00000e+000    6.71907e+001    1.52100e+000
        51         53    1.00000e+000    6.71888e+001    2.74898e+000
        52         54    1.00000e+000    6.71881e+001    9.24428e-001
        53         55    1.00000e+000    6.71878e+001    2.99588e-001
        54         56    1.00000e+000    6.71878e+001    2.56591e-001
        55         57    1.00000e+000    6.71877e+001    2.83697e-001
        56         58    1.00000e+000    6.71876e+001    2.96461e-001
        57         59    1.00000e+000    6.71875e+001    2.71420e-001
        58         60    1.00000e+000    6.71875e+001    1.73025e-001
        59         61    1.00000e+000    6.71874e+001    1.76432e-001
        60         62    1.00000e+000    6.71874e+001    1.93837e-001
        61         63    1.00000e+000    6.71874e+001    2.53836e-001
        62         64    1.00000e+000    6.71873e+001    2.24027e-001
        63         65    1.00000e+000    6.71873e+001    9.91684e-002
        64         66    1.00000e+000    6.71873e+001    4.87773e-002
        65         67    1.00000e+000    6.71873e+001    5.64876e-002
        66         68    1.00000e+000    6.71873e+001    9.84424e-002
        67         69    1.00000e+000    6.71873e+001    1.58813e-001
        68         70    1.00000e+000    6.71873e+001    2.14117e-001
        69         71    1.00000e+000    6.71873e+001    1.81392e-001
        70         72    1.00000e+000    6.71873e+001    7.79098e-002
        71         73    1.00000e+000    6.71873e+001    1.22891e-002
        72         74    1.00000e+000    6.71873e+001    1.38281e-002
        73         75    1.00000e+000    6.71873e+001    2.09665e-002
        74         76    1.00000e+000    6.71873e+001    3.74126e-002
        75         77    1.00000e+000    6.71873e+001    5.52524e-002
        76         78    1.00000e+000    6.71873e+001    6.25800e-002
        77         79    1.00000e+000    6.71873e+001    3.56207e-002
        78         80    1.00000e+000    6.71873e+001    9.06752e-003
        79         81    1.00000e+000    6.71873e+001    3.01710e-003
        80         82    1.00000e+000    6.71873e+001    4.84124e-003
        81         83    1.00000e+000    6.71873e+001    6.66329e-003
        82         84    1.00000e+000    6.71873e+001    9.44563e-003
        83         85    1.00000e+000    6.71873e+001    1.05041e-002
        84         86    1.00000e+000    6.71873e+001    7.31006e-003
        85         87    1.00000e+000    6.71873e+001    1.18789e-003
        86         88    1.00000e+000    6.71873e+001    1.07412e-003
        87         89    1.00000e+000    6.71873e+001    1.37823e-003
Function Value changing by less than TolX
</pre><h2>Compute training errors<a name="7"></a></h2><pre class="codeinput">[junk yhat] = max(X*wLinear,[],2);
trainErr_linear = sum(y~=yhat)/length(y)
[junk yhat] = max(Kpoly*uPoly,[],2);
trainErr_poly = sum(y~=yhat)/length(y)
[junk yhat] = max(Krbf*uRBF,[],2);
trainErr_rbf = sum(y~=yhat)/length(y)

[yhat2, prob] = logregPredict(modelRBF, X);
assert(isequal(yhat, yhat2))

figure;
plotClassifier(X,y,wLinear,<span class="string">'Linear Multinomial Logistic Regression'</span>);

figure;
plotClassifier(X,y,uPoly,<span class="string">'Kernel-Poly Multinomial Logistic Regression'</span>,@kernelPoly,polyOrder);

figure;
plotClassifier(X,y,uRBF,<span class="string">'Kernel-RBF Multinomial Logistic Regression'</span>,@kernelRbfSigma,rbfScale);
</pre><pre class="codeoutput">trainErr_linear =
    0.2400
trainErr_poly =
    0.1400
trainErr_rbf =
    0.1900
</pre><img vspace="5" hspace="5" src="logregMultinomKernelMinfuncDemo_02.png" alt=""> <img vspace="5" hspace="5" src="logregMultinomKernelMinfuncDemo_03.png" alt=""> <img vspace="5" hspace="5" src="logregMultinomKernelMinfuncDemo_04.png" alt=""> <p class="footer"><br>
      Published with MATLAB&reg; 7.10<br></p></div><!--
##### SOURCE BEGIN #####
%% Minfunc Kernelized Logreg Demo
%PMTKauthor Mark Schmidt
%PMTKmodified Kevin Murphy
%PMTKurl http://people.cs.ubc.ca/~schmidtm/Software/minFunc/minFunc.html#7
% It is modified by replacing penalizedKernelL2_matrix,
% which uses sum_c w(:,c)' K w(:,c) as the regularizer,
% with the simpler penalizedL2, which uses w' w as the regularizer.
% The key difference is that we only use kernels to do basis funcion
% expansion on X; we do not change the regularizer.
% This makes hardly any difference to the training error.
%%
options.Display = 'none';
setSeed(0); 
nClasses = 5;
%nInstances = 1000;
nInstances = 100;
nVars = 2;

[X,y] = makeData('multinomialNonlinear',nInstances,nVars,nClasses);

figure;
[n,p] = size(X);
colors = getColorsRGB;
hold on
for c = 1:nClasses
   if p == 3
      plot(X(y==c,2),X(y==c,3),'.','color',colors(c,:));
   else
      plot(X(y==c,1),X(y==c,2),'.','color',colors(c,:));
   end
end
lambda = 1e-2;
%% Linear
funObj = @(w)SoftmaxLoss2(w,X,y,nClasses);
fprintf('Training linear multinomial logistic regression model...\n');
wLinear = minFunc(@penalizedL2,zeros(nVars*(nClasses-1),1),options,funObj,lambda);
wLinear = reshape(wLinear,[nVars nClasses-1]);
wLinear = [wLinear zeros(nVars,1)];
%% Polynomial
polyOrder = 2;
Kpoly = kernelPoly(X,X,polyOrder);
funObj = @(u)SoftmaxLoss2(u,Kpoly,y,nClasses);
fprintf('Training kernel(poly) multinomial logistic regression model...\n');
uPoly = minFunc(@penalizedL2,randn(nInstances*(nClasses-1),1),options,funObj,lambda);
%uPoly = minFunc(@penalizedKernelL2_matrix,randn(nInstances*(nClasses-1),1),options,Kpoly,nClasses-1,funObj,lambda);
uPoly = reshape(uPoly,[nInstances nClasses-1]);
uPoly = [uPoly zeros(nInstances,1)];
%% RBF
rbfScale = 1;
Krbf = kernelRbfSigma(X,X,rbfScale);
funObj = @(u)SoftmaxLoss2(u,Krbf,y,nClasses);
fprintf('Training kernel(rbf) multinomial logistic regression model...\n');
uRBF = minFunc(@penalizedL2,randn(nInstances*(nClasses-1),1),options,funObj,lambda);
%uRBF = minFunc(@penalizedKernelL2_matrix,randn(nInstances*(nClasses-1),1),options,Krbf,nClasses-1,funObj,lambda);
uRBF = reshape(uRBF,[nInstances nClasses-1]);
uRBF = [uRBF zeros(nInstances,1)];
%% Check against PMTK functions
preproc.kernelFn = @(X1, X2)kernelRbfSigma(X1, X2, rbfScale);
preproc.standardizeX = false; 
preproc.includeOffset = false;
modelRBF = logregFit(X, y, 'lambda', lambda, 'preproc', preproc,...
    'fitOptions', struct());
wRBF = modelRBF.w;
assert(approxeq(wRBF, uRBF))
%% Compute training errors
[junk yhat] = max(X*wLinear,[],2);
trainErr_linear = sum(y~=yhat)/length(y)
[junk yhat] = max(Kpoly*uPoly,[],2);
trainErr_poly = sum(y~=yhat)/length(y)
[junk yhat] = max(Krbf*uRBF,[],2);
trainErr_rbf = sum(y~=yhat)/length(y)

[yhat2, prob] = logregPredict(modelRBF, X);
assert(isequal(yhat, yhat2))

figure;
plotClassifier(X,y,wLinear,'Linear Multinomial Logistic Regression');

figure;
plotClassifier(X,y,uPoly,'Kernel-Poly Multinomial Logistic Regression',@kernelPoly,polyOrder);

figure;
plotClassifier(X,y,uRBF,'Kernel-RBF Multinomial Logistic Regression',@kernelRbfSigma,rbfScale);


##### SOURCE END #####
--></body></html>