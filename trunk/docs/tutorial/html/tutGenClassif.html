
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML is auto-generated from an M-file.
To make changes, update the M-file and republish this document.
      --><title>tutGenClassif</title><meta name="generator" content="MATLAB 7.9"><meta name="date" content="2010-08-30"><meta name="m-file" content="tutGenClassif"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head><body><div class="content"><h1></h1><!--introduction--><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Supervised learning using generative models in pmtk3</a></li><li><a href="#2">Naive Bayes classifier</a></li><li><a href="#7">Discriminant analysis</a></li><li><a href="#12">QDA/ LDA/ NBC</a></li><li><a href="#15">Regularized discriminant analysis</a></li><li><a href="#16">Nearest shrunken centroid</a></li><li><a href="#18">K-nearest neighbor classifier</a></li></ul></div><h2>Supervised learning using generative models in pmtk3<a name="1"></a></h2><p><i>This page was auto-generated by publishing</i> <a href="http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/tutGenClassif.m">http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/tutGenClassif.m</a>.</p><p>Generative models for classification/ regression are joint models of the labels and features of the form <img src="tutGenClassif_eq35640.png" alt="$p(y,x|\theta)$">. We consider various examples below.</p><h2>Naive Bayes classifier<a name="2"></a></h2><p>We now a simple kind of generative classifier called naive Bayes, which is a model of the form</p><p><img src="tutGenClassif_eq36665.png" alt="$$p(y,x|\theta) = p(y|\pi) \prod_{j=1}^D p(x_j|y,\theta)$$"></p><p>We can fit and predict with this model using <a href="http://pmtk3.googlecode.com/svn/trunk/toolbox/SupervisedModels/naiveBayes/naiveBayesFit.m">naiveBayesFit.m</a> and naiveBayesPredict.m. For simplicity, the current implementation assumes  all the features are binary, so <img src="tutGenClassif_eq85950.png" alt="$p(x_j|y=c,\theta) = Ber(x_j|\theta_{jc})$">. It fits by MAP estimation with a vague Dirichlet prior (add-one-smoothing).</p><p>Below is an example (from naiveBayesBowDemo.m) which fits a model to some bag of words data, and then classifies a test set.</p><pre class="codeinput">loadData(<span class="string">'XwindowsDocData'</span>); <span class="comment">% 900x600, 2 classes Xwindows vs MSwindows</span>
Xtrain = xtrain; Xtest = xtest;
model = naiveBayesFit(Xtrain, ytrain);
ypred_train = naiveBayesPredict(model, Xtrain);
err_train = mean(zeroOneLossFn(ytrain, ypred_train));
ypred_test = naiveBayesPredict(model, Xtest);
err_test = mean(zeroOneLossFn(ytest, ypred_test));
fprintf(<span class="string">'misclassification rates  on train = %5.2f pc, on test = %5.2f pc\n'</span>, <span class="keyword">...</span>
    err_train*100, err_test*100);
</pre><pre class="codeoutput">misclassification rates  on train =  8.33 pc, on test = 18.67 pc
</pre><p>See also <a href="http://pmtk3.googlecode.com/svn/trunk/demos/otherDemos/naiveBayesMnistDemo.m">naiveBayesMnistDemo.m</a> for application of NBC to classify binarized MNIST digits.</p><p>It is simple to modify NBC to handle missing data in X at training and test time; this is left as an exercise to the reader.</p><h2>Discriminant analysis<a name="7"></a></h2><p>Discriminant analysis is a generative classifier where the class conditional density  is a multivariate Gaussian:</p><p><img src="tutGenClassif_eq68632.png" alt="$$p(y=c,x|\theta) = \mbox{discrete}(y|\pi) N(x|\mu_c,\Sigma_c)$$"></p><p>PMTK supports the following variants of this model:</p><p>
<table border=1>
<TR ALIGN=left>
<td> Type
<td> Description
<tr>
<td> QDA
<td> $\Sigma_c$ is different for each class.
Induces quadratic decision boundaries.
<tr>
<td> LDA (linear)
<td>  $\Sigma_c=\Sigma$ is the same (tied) for each class.
Induces linear decision boundaries.
<tr>
<td> diag
<td> $\Sigma_c$ is diagonal, so the features are conditionally
independent; this is an example of a naive Bayes classifier.
Induces linear decision boundaries.
<tr>
<td> RDA
<td> Regularized LDA; uses MAP estimation for $\Sigma$.
<tr>
<td> shrunkenCentroids
<td> Diagonal LDA with L1 shrinkage on offsets (see below)
</table>
</p><p>We give more details below.</p><h2>QDA/ LDA/ NBC<a name="12"></a></h2><p>Below we give an example (from discrimAnalaysisFisherIrisDemo.m) of how to fit a QDA/LDA/ diagDA model. We apply it to a subset of the Fisher Iris dataset.</p><pre class="codeinput">loadData(<span class="string">'fisheriris'</span>)
X = meas(51:end, 1:2);  <span class="comment">% for illustrations use 2 species, 2 features</span>
labels = species(51:end);
[y, support] = canonizeLabels(labels);
types = {<span class="string">'quadratic'</span>, <span class="string">'linear'</span>, <span class="string">'diag'</span>};
<span class="keyword">for</span> tt=1:length(types)
  model = discrimAnalysisFit(X, y, types{tt});
  h = plotDecisionBoundary(X, y, @(Xtest)discrimAnalysisPredict(model, Xtest));
  title(sprintf(<span class="string">'Discrim. analysis of type %s'</span>, types{tt}));
  <span class="keyword">if</span> ~isOctave
    legend(h, support, <span class="string">'Location'</span>, <span class="string">'NorthWest'</span>);
    set(gca, <span class="string">'Xtick'</span>, 5:8, <span class="string">'Ytick'</span>, 2:0.5:4);
  <span class="keyword">end</span>
  xlabel(<span class="string">'X_1'</span>); ylabel(<span class="string">'X_2'</span>);
<span class="keyword">end</span>
</pre><img vspace="5" hspace="5" src="tutGenClassif_01.png" alt=""> <img vspace="5" hspace="5" src="tutGenClassif_02.png" alt=""> <img vspace="5" hspace="5" src="tutGenClassif_03.png" alt=""> <h2>Regularized discriminant analysis<a name="15"></a></h2><p>When fitting a discriminant analysis model we will encounter numerical problems when estimating <img src="tutGenClassif_eq53484.png" alt="$\Sigma$"> when N &lt; D, even if we use a tied  covariance matrix (i.e., one shared across classes, a method known as linear discriminant analysis). A simple solution is to use a Wishart prior to compute a MAP estimate of <img src="tutGenClassif_eq53484.png" alt="$\Sigma$">. This is called regularized discriminant analysis, and can be fit using <tt>discrimAnalysisFit(X, y, 'rda', lambda)</tt>, where <tt>lambda</tt> controls the amount of regularization. See <a href="http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Generative_models_for_classification_and_regression/cancerHighDimClassifDemo.m">cancerHighDimClassifDemo.m</a> for an example, which reproduces table 18.1 from "Elements of statistical learning" 2nd edn p656. (We don't run this demo here since it is a bit slow, since the data has 16,000 features.)</p><h2>Nearest shrunken centroid<a name="16"></a></h2><p>Consider a naive Bayes model in which the diagonal covariance is tied. This has O(D) parameters for the covariance, but O(C D) for the mean. To prevent overfitting, we can shrink the class-conditional means towards the overall mean; this technique is called nearest shrunken centroids. We can fit this model using <tt>discrimAnalysisFit(X, y, 'shrunkenCentroids', lambda)</tt>. We given an example of this below (from  shrunkenCentroidsSRBCTdemo.m), where we apply the method to the SRBCT gene microarray dataset, which has N=144 training examples, D=16063 features and C=4 classes. This roughly reproduces figure 18.4 from "Elements of statistical learning" 2nd ed.</p><pre class="codeinput">close <span class="string">all</span>; clear <span class="string">all</span>;
loadData(<span class="string">'srbct'</span>);

Xtest = Xtest(~isnan(ytest), :);
ytest = ytest(~isnan(ytest));

fitFn = @(X,y,lam)  discrimAnalysisFit(X, y, <span class="string">'shrunkenCentroids'</span>, <span class="string">'lambda'</span>,lam);
predictFn = @(model, X)  discrimAnalysisPredict(model, X);

lambdas = linspace(0, 8, 20);
nTrain = length(ytrain);
nTest = length(ytest);
<span class="keyword">for</span> i=1:length(lambdas)
    model = fitFn(Xtrain, ytrain, lambdas(i));
    yhatTrain = predictFn(model, Xtrain);
    yhatTest = predictFn(model, Xtest);
    errTrain(i) = sum(zeroOneLossFn(yhatTrain, ytrain))/nTrain;
    errTest(i) = sum(zeroOneLossFn(yhatTest, ytest))/nTest;
    numgenes(i) = sum(model.shrunkenCentroids(:) ~= 0);
<span class="keyword">end</span>

figure;
plot(lambdas, errTrain, <span class="string">'gx-'</span>, lambdas, errTest, <span class="string">'bo--'</span>,<span class="keyword">...</span>
  <span class="string">'MarkerSize'</span>, 10, <span class="string">'linewidth'</span>, 2)
legend(<span class="string">'Training'</span>, <span class="string">'Test'</span>, <span class="string">'Location'</span>, <span class="string">'northwest'</span>);
xlabel(<span class="string">'Amount of shrinkage'</span>)
ylabel(<span class="string">'misclassification rate'</span>)
title(<span class="string">'SRBCT data'</span>)
</pre><img vspace="5" hspace="5" src="tutGenClassif_04.png" alt=""> <h2>K-nearest neighbor classifier<a name="18"></a></h2><p>One can view a KNN classifier as a generative classifier where the class conditional density is a non-parametric kernel density estimator. Below we give an example where we apply a 1-NN classifier to a subset of the MNIST digit set (from mnistKNNdemo.m: see <a href="http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Introduction/mnist1NNdemo.m">mnist1NNdemo.m</a> for special-purpose code that can handle the full dataset).</p><pre class="codeinput">loadData(<span class="string">'mnistAll'</span>);
trainndx = 1:10000;
testndx =  1:1000;
ntrain = length(trainndx);
ntest = length(testndx);
Xtrain = double(reshape(mnist.train_images(:,:,trainndx),28*28,ntrain)');
Xtest  = double(reshape(mnist.test_images(:,:,testndx),28*28,ntest)');
ytrain = (mnist.train_labels(trainndx));
ytest  = (mnist.test_labels(testndx));
clear <span class="string">mnist</span> <span class="string">trainndx</span> <span class="string">testndx</span>; <span class="comment">% save space</span>

m = knnFit(Xtrain, ytrain, 1);
ypred = knnPredict(m, Xtest);
errorRate = mean(ypred ~= ytest);
fprintf(<span class="string">'Error Rate: %.2f%%\n'</span>,100*errorRate);
</pre><pre class="codeoutput">Error Rate: 8.00%
</pre><p class="footer"><br>
      Published with MATLAB&reg; 7.9<br></p></div><!--
##### SOURCE BEGIN #####

%% Supervised learning using generative models in pmtk3
% _This page was auto-generated by publishing_
% <http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/tutGenClassif.m>.
%
% Generative models for classification/ regression are joint models of the labels
% and features of the form $p(y,x|\theta)$.
% We consider various examples below.

%% Naive Bayes classifier
% We now a simple kind of generative classifier
% called naive Bayes, which is a model of the form
%%
% $$p(y,x|\theta) = p(y|\pi) \prod_{j=1}^D p(x_j|y,\theta)$$
%%
% We can fit and predict with this model using
% <http://pmtk3.googlecode.com/svn/trunk/toolbox/SupervisedModels/naiveBayes/naiveBayesFit.m naiveBayesFit.m> and naiveBayesPredict.m.
% For simplicity, the current implementation
% assumes  all the features are binary,
% so $p(x_j|y=c,\theta) = Ber(x_j|\theta_{jc})$.
% It fits by MAP estimation
% with a vague Dirichlet prior (add-one-smoothing).
%
% Below is an example (from naiveBayesBowDemo.m)
% which fits a model to some bag of words data,
% and then classifies a test set.
%%
loadData('XwindowsDocData'); % 900x600, 2 classes Xwindows vs MSwindows
Xtrain = xtrain; Xtest = xtest;
model = naiveBayesFit(Xtrain, ytrain);
ypred_train = naiveBayesPredict(model, Xtrain);
err_train = mean(zeroOneLossFn(ytrain, ypred_train));
ypred_test = naiveBayesPredict(model, Xtest);
err_test = mean(zeroOneLossFn(ytest, ypred_test));
fprintf('misclassification rates  on train = %5.2f pc, on test = %5.2f pc\n', ...
    err_train*100, err_test*100);
%%
% See also <http://pmtk3.googlecode.com/svn/trunk/demos/otherDemos/naiveBayesMnistDemo.m naiveBayesMnistDemo.m> for application of NBC
% to classify binarized MNIST digits.
%
% It is simple to modify NBC to handle missing data in X
% at training and test time; this is left as an exercise
% to the reader.

%% Discriminant analysis
% Discriminant analysis is a generative classifier where
% the class conditional density  is a multivariate Gaussian:
%%
% $$p(y=c,x|\theta) = \mbox{discrete}(y|\pi) N(x|\mu_c,\Sigma_c)$$
%%
% PMTK supports the following variants of this model:
%%
% <html>
% <table border=1>
% <TR ALIGN=left>
% <td> Type
% <td> Description
% <tr>
% <td> QDA
% <td> $\Sigma_c$ is different for each class.
% Induces quadratic decision boundaries.
% <tr>
% <td> LDA (linear)
% <td>  $\Sigma_c=\Sigma$ is the same (tied) for each class.
% Induces linear decision boundaries.
% <tr>
% <td> diag
% <td> $\Sigma_c$ is diagonal, so the features are conditionally
% independent; this is an example of a naive Bayes classifier.
% Induces linear decision boundaries.
% <tr>
% <td> RDA
% <td> Regularized LDA; uses MAP estimation for $\Sigma$.
% <tr>
% <td> shrunkenCentroids
% <td> Diagonal LDA with L1 shrinkage on offsets (see below)
% </table>
% </html>
%%
% We give more details below.

%% QDA/ LDA/ NBC
% Below we give an example (from discrimAnalaysisFisherIrisDemo.m)
% of how to fit a QDA/LDA/ diagDA model.
% We apply it to a subset of the Fisher Iris dataset.
%%
loadData('fisheriris')
X = meas(51:end, 1:2);  % for illustrations use 2 species, 2 features
labels = species(51:end);
[y, support] = canonizeLabels(labels);
types = {'quadratic', 'linear', 'diag'};
for tt=1:length(types)
  model = discrimAnalysisFit(X, y, types{tt});
  h = plotDecisionBoundary(X, y, @(Xtest)discrimAnalysisPredict(model, Xtest));
  title(sprintf('Discrim. analysis of type %s', types{tt}));
  if ~isOctave
    legend(h, support, 'Location', 'NorthWest');
    set(gca, 'Xtick', 5:8, 'Ytick', 2:0.5:4);
  end
  xlabel('X_1'); ylabel('X_2');
end
%%

%% Regularized discriminant analysis
% When fitting a discriminant analysis model 
% we will encounter numerical problems
% when estimating $\Sigma$ when N < D, even if we use
% a tied  covariance matrix (i.e., one shared across classes, a method
% known as linear discriminant analysis).
% A simple solution is to use a Wishart prior to compute a MAP
% estimate of $\Sigma$. This is called regularized discriminant analysis,
% and can be fit using |discrimAnalysisFit(X, y, 'rda', lambda)|,
% where |lambda| controls the amount of regularization.
% See <http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Generative_models_for_classification_and_regression/cancerHighDimClassifDemo.m cancerHighDimClassifDemo.m>
% for an example, which reproduces
% table 18.1 from "Elements of statistical learning" 2nd edn p656.
% (We don't run this demo here since it is a bit slow,
% since the data has 16,000 features.)
%
%% Nearest shrunken centroid
% Consider a naive Bayes model in which the diagonal covariance
% is tied. This has O(D) parameters for the covariance, but O(C D) for the mean.
% To prevent overfitting, we can shrink the class-conditional means towards
% the overall mean; this technique is called nearest shrunken centroids. We
% can fit this model using |discrimAnalysisFit(X, y, 'shrunkenCentroids',
% lambda)|. We given an example of this below (from
%  shrunkenCentroidsSRBCTdemo.m),
% where we apply the method to
% the SRBCT gene microarray dataset, which has N=144 training examples,
% D=16063 features and C=4 classes.
% This roughly reproduces figure 18.4 from "Elements of statistical
% learning" 2nd ed.
%%
close all; clear all;
loadData('srbct');

Xtest = Xtest(~isnan(ytest), :);
ytest = ytest(~isnan(ytest));

fitFn = @(X,y,lam)  discrimAnalysisFit(X, y, 'shrunkenCentroids', 'lambda',lam);
predictFn = @(model, X)  discrimAnalysisPredict(model, X);

lambdas = linspace(0, 8, 20);
nTrain = length(ytrain);
nTest = length(ytest);
for i=1:length(lambdas)
    model = fitFn(Xtrain, ytrain, lambdas(i));
    yhatTrain = predictFn(model, Xtrain);
    yhatTest = predictFn(model, Xtest);
    errTrain(i) = sum(zeroOneLossFn(yhatTrain, ytrain))/nTrain;
    errTest(i) = sum(zeroOneLossFn(yhatTest, ytest))/nTest;
    numgenes(i) = sum(model.shrunkenCentroids(:) ~= 0);
end

figure;
plot(lambdas, errTrain, 'gx-', lambdas, errTest, 'boREPLACE_WITH_DASH_DASH',...
  'MarkerSize', 10, 'linewidth', 2)
legend('Training', 'Test', 'Location', 'northwest');
xlabel('Amount of shrinkage')
ylabel('misclassification rate')
title('SRBCT data')

%% K-nearest neighbor classifier
% One can view a KNN classifier as a generative
% classifier where the class conditional density is a non-parametric
% kernel density estimator.
% Below we give an example where we apply
% a 1-NN classifier to a subset of the MNIST digit set
% (from mnistKNNdemo.m: see <http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Introduction/mnist1NNdemo.m mnist1NNdemo.m> for special-purpose
% code that can handle the full dataset).
%%
loadData('mnistAll');
trainndx = 1:10000;
testndx =  1:1000;
ntrain = length(trainndx);
ntest = length(testndx);
Xtrain = double(reshape(mnist.train_images(:,:,trainndx),28*28,ntrain)');
Xtest  = double(reshape(mnist.test_images(:,:,testndx),28*28,ntest)');
ytrain = (mnist.train_labels(trainndx));
ytest  = (mnist.test_labels(testndx));
clear mnist trainndx testndx; % save space

m = knnFit(Xtrain, ytrain, 1);
ypred = knnPredict(m, Xtest);
errorRate = mean(ypred ~= ytest);
fprintf('Error Rate: %.2f%%\n',100*errorRate);
%%

##### SOURCE END #####
--></body></html>