
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML is auto-generated from an M-file.
To make changes, update the M-file and republish this document.
      --><title>Supervised learning using generative models in pmtk3</title><meta name="generator" content="MATLAB 7.9"><meta name="date" content="2010-09-01"><meta name="m-file" content="tutGenClassif"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head><body><div class="content"><h1>Supervised learning using generative models in pmtk3</h1><p><i>This page was auto-generated by publishing</i> <a href="http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/tutGenClassif.m">tutGenClassif.m</a></p><p>(HMMs are discussed in more detail <a href="http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/html/tutLVM.html">here</a>.) (HMMs are discussed in more detail <a href="tutLVM.html">here</a>.)</p><pre class="codeinput"><span class="comment">%{
</span><span class="comment">% Generative models for classification/ regression are joint models of the
</span><span class="comment">% outputs and inputs
</span><span class="comment">% of the form $p(y,x|\theta)$.
</span><span class="comment">% We consider various examples below.
</span><span class="comment">%
</span><span class="comment">%% Naive Bayes classifier
</span><span class="comment">% We now a simple kind of generative classifier
</span><span class="comment">% called naive Bayes, which is a model of the form
</span><span class="comment">%%
</span><span class="comment">% $$p(y,x|\theta) = p(y|\pi) \prod_{j=1}^D p(x_j|y,\theta)$$
</span><span class="comment">%%
</span><span class="comment">% We can fit and predict with this model using
</span><span class="comment">% &lt;http://pmtk3.googlecode.com/svn/trunk/toolbox/SupervisedModels/naiveBayes/naiveBayesFit.m naiveBayesFit.m&gt; and  &lt;http://pmtk3.googlecode.com/svn/trunk/toolbox/SupervisedModels/naiveBayes/naiveBayesPredict.m naiveBayesPredict.m&gt;
</span><span class="comment">% For simplicity, the current implementation
</span><span class="comment">% assumes  all the features are binary,
</span><span class="comment">% so $p(x_j|y=c,\theta) = Ber(x_j|\theta_{jc})$.
</span><span class="comment">% It fits by MAP estimation
</span><span class="comment">% with a vague Dirichlet prior (add-one-smoothing).
</span><span class="comment">% Typically results are not too sensitive to the
</span><span class="comment">% setting of this prior (unlike discriminative models).
</span><span class="comment">%
</span><span class="comment">% Below is an example (from &lt;http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Generative_models_for_classification_and_regression/naiveBayesBowDemo.m naiveBayesBowDemo.m&gt; )
</span><span class="comment">% which fits a model to some bag of words data,
</span><span class="comment">% and then classifies a test set.
</span><span class="comment">%%
</span><span class="comment">loadData('XwindowsDocData'); % 900x600, 2 classes Xwindows vs MSwindows
</span><span class="comment">Xtrain = xtrain; Xtest = xtest;
</span><span class="comment">model = naiveBayesFit(Xtrain, ytrain);
</span><span class="comment">ypred_train = naiveBayesPredict(model, Xtrain);
</span><span class="comment">err_train = mean(zeroOneLossFn(ytrain, ypred_train));
</span><span class="comment">ypred_test = naiveBayesPredict(model, Xtest);
</span><span class="comment">err_test = mean(zeroOneLossFn(ytest, ypred_test));
</span><span class="comment">fprintf('misclassification rates  on train = %5.2f pc, on test = %5.2f pc\n', ...
</span><span class="comment">    err_train*100, err_test*100);
</span><span class="comment">%%
</span><span class="comment">% See also &lt;http://pmtk3.googlecode.com/svn/trunk/demos/otherDemos/supervisedModels/naiveBayesMnistDemo.m naiveBayesMnistDemo.m&gt; for application of NBC
</span><span class="comment">% to classify binarized MNIST digits.
</span><span class="comment">%
</span><span class="comment">% It is simple to modify NBC to handle missing data in X
</span><span class="comment">% at training and test time; this is left as an exercise
</span><span class="comment">% to the reader.
</span><span class="comment">
</span><span class="comment">%% Discriminant analysis
</span><span class="comment">% Discriminant analysis is a generative classifier where
</span><span class="comment">% the class conditional density  is a multivariate Gaussian:
</span><span class="comment">%%
</span><span class="comment">% $$p(y=c,x|\theta) = \mbox{discrete}(y|\pi) N(x|\mu_c,\Sigma_c)$$
</span><span class="comment">%%
</span><span class="comment">% PMTK supports the following variants of this model:
</span><span class="comment">%%
</span><span class="comment">% &lt;html&gt;
</span><span class="comment">% &lt;table border=1&gt;
</span><span class="comment">% &lt;TR ALIGN=left&gt;
</span><span class="comment">% &lt;td&gt; Type
</span><span class="comment">% &lt;td&gt; Description
</span><span class="comment">% &lt;tr&gt;
</span><span class="comment">% &lt;td&gt; QDA
</span><span class="comment">% &lt;td&gt; &lt;img src="x0x5CSigma_c.png"&gt; is different for each class.
</span><span class="comment">% Induces quadratic decision boundaries.
</span><span class="comment">% &lt;tr&gt;
</span><span class="comment">% &lt;td&gt; LDA
</span><span class="comment">% &lt;td&gt;  &lt;img src="x0x5CSigma_c0x3D0x5CSigma.png"&gt; is the same (tied) for each class.
</span><span class="comment">% Induces linear decision boundaries.
</span><span class="comment">% &lt;tr&gt;
</span><span class="comment">% &lt;td&gt; DDA
</span><span class="comment">% &lt;td&gt; &lt;img src="x0x5CSigma_c.png"&gt; is diagonal, so the features are conditionally
</span><span class="comment">% independent; this is an example of a naive Bayes classifier.
</span><span class="comment">% Induces linear decision boundaries.
</span><span class="comment">% &lt;tr&gt;
</span><span class="comment">% &lt;td&gt; RDA
</span><span class="comment">% &lt;td&gt; Regularized LDA; uses MAP estimation for &lt;img src="x0x5CSigma.png"&gt;.
</span><span class="comment">% &lt;tr&gt;
</span><span class="comment">% &lt;td&gt; shrunkenCentroids
</span><span class="comment">% &lt;td&gt; Diagonal LDA with L1 shrinkage on offsets (see below)
</span><span class="comment">% &lt;/table&gt;
</span><span class="comment">% &lt;/html&gt;
</span><span class="comment">%%
</span><span class="comment">% We give more details below.
</span><span class="comment">
</span><span class="comment">%% QDA/ LDA/ NBC
</span><span class="comment">% Below we give an example (from &lt;http://pmtk3.googlecode.com/svn/trunk/demos/otherDemos/supervisedModels/discrimAnalysisFisherIrisDemo.m discrimAnalysisFisherIrisDemo.m&gt; )
</span><span class="comment">% of how to fit a QDA/LDA/ diagDA model.
</span><span class="comment">% We apply it to a subset of the Fisher Iris dataset.
</span><span class="comment">%%
</span><span class="comment">loadData('fisherIrisData')
</span><span class="comment">X = meas(51:end, 1:2);  % for illustrations use 2 species, 2 features
</span><span class="comment">labels = species(51:end);
</span><span class="comment">[y, support] = canonizeLabels(labels);
</span><span class="comment">types = {'quadratic', 'linear', 'diag'};
</span><span class="comment">for tt=1:length(types)
</span><span class="comment">  model = discrimAnalysisFit(X, y, types{tt});
</span><span class="comment">  h = plotDecisionBoundary(X, y, @(Xtest)discrimAnalysisPredict(model, Xtest));
</span><span class="comment">  title(sprintf('Discrim. analysis of type %s', types{tt}));
</span><span class="comment">  if ~isOctave
</span><span class="comment">    legend(h, support, 'Location', 'NorthWest');
</span><span class="comment">    set(gca, 'Xtick', 5:8, 'Ytick', 2:0.5:4);
</span><span class="comment">  end
</span><span class="comment">  xlabel('X_1'); ylabel('X_2');
</span><span class="comment">end
</span><span class="comment">%%
</span><span class="comment">
</span><span class="comment">%% Regularized discriminant analysis
</span><span class="comment">% When fitting a discriminant analysis model
</span><span class="comment">% we will encounter numerical problems
</span><span class="comment">% when estimating $\Sigma$ when N &lt; D, even if we use
</span><span class="comment">% a tied  covariance matrix (i.e., one shared across classes, a method
</span><span class="comment">% known as linear discriminant analysis).
</span><span class="comment">% A simple solution is to use a Wishart prior to compute a MAP
</span><span class="comment">% estimate of $\Sigma$. This is called regularized discriminant analysis,
</span><span class="comment">% and can be fit using |discrimAnalysisFit(X, y, 'rda', lambda)|,
</span><span class="comment">% where |lambda| controls the amount of regularization.
</span><span class="comment">% See &lt;http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Generative_models_for_classification_and_regression/cancerHighDimClassifDemo.m cancerHighDimClassifDemo.m&gt;
</span><span class="comment">% for an example, which reproduces
</span><span class="comment">% table 18.1 from
</span><span class="comment">% &lt;http://www-stat.stanford.edu/~tibs/ElemStatLearn/ Elements of statistical learning&gt;
</span><span class="comment">% 2nd edn p656.
</span><span class="comment">% (We don't run this demo here since it requires computing
</span><span class="comment">% the SVD of Xtrain (which has size 144* 16063, with 14 classes)
</span><span class="comment">% which takes
</span><span class="comment">% more seconds than we are willing to wait (about 40 sec)).
</span><span class="comment">%
</span><span class="comment">
</span><span class="comment">%% Nearest shrunken centroid
</span><span class="comment">% Consider a naive Bayes model in which the diagonal covariance
</span><span class="comment">% is tied. This has O(D) parameters for the covariance, but O(C D) for the mean.
</span><span class="comment">% To prevent overfitting, we can shrink the class-conditional means towards
</span><span class="comment">% the overall mean; this technique is called nearest shrunken centroids. We
</span><span class="comment">% can fit this model using |discrimAnalysisFit(X, y, 'shrunkenCentroids',
</span><span class="comment">% lambda)|. We given an example of this below (from
</span><span class="comment">%  &lt;http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Generative_models_for_classification_and_regression/shrunkenCentroidsSRBCTdemo.m shrunkenCentroidsSRBCTdemo.m&gt; ),
</span><span class="comment">% where we apply the method to
</span><span class="comment">% the SRBCT gene microarray dataset, whose training set
</span><span class="comment">% has size 63*2308 with  C=4 classes.
</span><span class="comment">% This roughly reproduces figure 18.4 from
</span><span class="comment">% &lt;http://www-stat.stanford.edu/~tibs/ElemStatLearn/ Elements of statistical learning&gt;
</span><span class="comment">% 2nd edn p656.
</span><span class="comment">%%
</span><span class="comment">close all; clear all;
</span><span class="comment">loadData('srbct');
</span><span class="comment">
</span><span class="comment">Xtest = Xtest(~isnan(ytest), :);
</span><span class="comment">ytest = ytest(~isnan(ytest));
</span><span class="comment">
</span><span class="comment">fitFn = @(X,y,lam)  discrimAnalysisFit(X, y, 'shrunkenCentroids', 'lambda',lam);
</span><span class="comment">predictFn = @(model, X)  discrimAnalysisPredict(model, X);
</span><span class="comment">
</span><span class="comment">lambdas = linspace(0, 8, 20);
</span><span class="comment">nTrain = length(ytrain);
</span><span class="comment">nTest = length(ytest);
</span><span class="comment">for i=1:length(lambdas)
</span><span class="comment">    model = fitFn(Xtrain, ytrain, lambdas(i));
</span><span class="comment">    yhatTrain = predictFn(model, Xtrain);
</span><span class="comment">    yhatTest = predictFn(model, Xtest);
</span><span class="comment">    errTrain(i) = sum(zeroOneLossFn(yhatTrain, ytrain))/nTrain;
</span><span class="comment">    errTest(i) = sum(zeroOneLossFn(yhatTest, ytest))/nTest;
</span><span class="comment">    numgenes(i) = sum(model.shrunkenCentroids(:) ~= 0);
</span><span class="comment">end
</span><span class="comment">
</span><span class="comment">figure;
</span><span class="comment">plot(lambdas, errTrain, 'gx-', lambdas, errTest, 'bo--',...
</span><span class="comment">  'MarkerSize', 10, 'linewidth', 2)
</span><span class="comment">legend('Training', 'Test', 'Location', 'northwest');
</span><span class="comment">xlabel('Amount of shrinkage')
</span><span class="comment">ylabel('misclassification rate')
</span><span class="comment">title('SRBCT data')
</span><span class="comment">%%
</span><span class="comment">% We can also visualize the MAP (blue) and ML (gray) estimate of the means
</span><span class="comment">% for each class.
</span><span class="comment">%%
</span><span class="comment">bestModel = fitFn(Xtrain, ytrain, 4);
</span><span class="comment">centShrunk = bestModel.shrunkenCentroids;
</span><span class="comment">model = fitFn(Xtrain, ytrain, 0);
</span><span class="comment">centUnshrunk = model.shrunkenCentroids;
</span><span class="comment">
</span><span class="comment">[numGroups D] = size(centShrunk);
</span><span class="comment">for g=1:3 % numGroups
</span><span class="comment">    %subplot(4,1,g);
</span><span class="comment">    figure; hold on;
</span><span class="comment">    plot(1:D, centUnshrunk(g,:), 'Color', [.8 .8 .8]);
</span><span class="comment">    plot(1:D, centShrunk(g,:), 'b', 'LineWidth', 2);
</span><span class="comment">    title(sprintf('Class %d', g));
</span><span class="comment">    hold off;
</span><span class="comment">    printPmtkFigure(sprintf('shrunkenCentroidsClass%d', g))
</span><span class="comment">end
</span><span class="comment">
</span><span class="comment">
</span><span class="comment">%% Robust discriminant analysis
</span><span class="comment">% We can use any joint probability model for the class conditional
</span><span class="comment">% density in a generative classifier.
</span><span class="comment">% To train we just call |generativeClassifierFit(fitFn, X, y)|
</span><span class="comment">% where |fitFn| fits $p(x|y=c,\theta)$.
</span><span class="comment">% To predict we just call
</span><span class="comment">% |[yhat, post] = generativeClassifierPredict(logprobFn, model, Xtest)|,
</span><span class="comment">% where |logprobFn| computes $p(x|y=c,\theta)$.
</span><span class="comment">%
</span><span class="comment">% As a simple example, we can make each class conditional density
</span><span class="comment">% be a Student distribution, to implement robust discriminant
</span><span class="comment">% analysis. Here is part of &lt;http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Generative_models_for_classification_and_regression/robustDiscrimAnalysisBankruptcyDemo.m robustDiscrimAnalysisBankruptcyDemo.m&gt;
</span><span class="comment">% which illustrates the syntax:
</span><span class="comment">%
</span><span class="comment">% |modelS = generativeClassifierFit(@studentFit, Xtrain, ytrain)|
</span><span class="comment">%
</span><span class="comment">% |[yhat] = generativeClassifierPredict(@studentLogprob, modelS, Xtest)|
</span><span class="comment">%
</span><span class="comment">% Now we run the entire demo, which uses Student and Gaussian
</span><span class="comment">% class conditional densities, as well as the QDA code.
</span><span class="comment">% We see that the Student distribution is more robust
</span><span class="comment">% than the Gaussian, and that the QDA code gives the same
</span><span class="comment">% results as using a Gaussian model inside the generative
</span><span class="comment">% classifier code, as it should
</span><span class="comment">%%
</span><span class="comment">robustDiscrimAnalysisBankruptcyDemo
</span><span class="comment">%%
</span><span class="comment">
</span><span class="comment">%% Using HMMs as class conditional densities
</span><span class="comment">% As a more interesting example, consider the problem of classifying time series,
</span><span class="comment">% such as spoken digits.
</span><span class="comment">% Since each data vector has variable length, it is natural
</span><span class="comment">% to use a Markov or hidden Markov model for the class conditional
</span><span class="comment">% densities.
</span><span class="comment">% (HMMs are discussed in more detail
</span><span class="comment">% &lt;http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/html/tutLVM.html
</span><span class="comment">% here&gt;.)
</span><span class="comment">% (HMMs are discussed in more detail
</span><span class="comment">% &lt;tutLVM.html here&gt;.)
</span><span class="comment">% For real-valued data, a linear-Gaussian Markov model is not
</span><span class="comment">% expressive enough, but an HMM with Gaussian emissions
</span><span class="comment">% is quite flexible.
</span><span class="comment">%
</span><span class="comment">% Suppose we have two sequences, corresponding to the spoken words
</span><span class="comment">% "four" and "five". We can train and test the model
</span><span class="comment">% using the code below
</span><span class="comment">% (from &lt;http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Generative_models_for_classification_and_regression/isolatedWordClassificationWithHmmsDemo.m isolatedWordClassificationWithHmmsDemo.m&gt; ).
</span><span class="comment">% It fits two HMMs, one per class.
</span><span class="comment">%%
</span><span class="comment">loadData('data45');
</span><span class="comment">% Xtrain{i} is a 13 x T(i) sequence of MFCC data, where T(i) is the length
</span><span class="comment">nstates = 5;
</span><span class="comment">setSeed(0);
</span><span class="comment">Xtrain = [train4'; train5'];
</span><span class="comment">ytrain = [repmat(4, numel(train4), 1) ; repmat(5, numel(train5), 1)];
</span><span class="comment">[Xtrain, ytrain] = shuffleRows(Xtrain, ytrain);
</span><span class="comment">Xtest = test45';
</span><span class="comment">ytest = labels';
</span><span class="comment">[Xtest, ytest] = shuffleRows(Xtest, ytest);
</span><span class="comment">% Initial Guess for params
</span><span class="comment">pi0 = [1, 0, 0, 0, 0];
</span><span class="comment">transmat0 = normalize(diag(ones(nstates, 1)) + ...
</span><span class="comment">            diag(ones(nstates-1, 1), 1), 2);
</span><span class="comment">% Fit
</span><span class="comment">fitArgs = {'pi0', pi0, 'trans0', transmat0, 'maxIter', 10, 'verbose', true};
</span><span class="comment">fitFn   = @(X)hmmFit(X, nstates, 'gauss', fitArgs{:});
</span><span class="comment">model = generativeClassifierFit(fitFn, Xtrain, ytrain);
</span><span class="comment">% Predict
</span><span class="comment">logprobFn = @hmmLogprob;
</span><span class="comment">[yhat, post] = generativeClassifierPredict(logprobFn, model, Xtest);
</span><span class="comment">nerrors = sum(yhat ~= ytest)
</span><span class="comment">%%
</span><span class="comment">
</span><span class="comment">%% K-nearest neighbor classifier
</span><span class="comment">% One can view a KNN classifier as a generative
</span><span class="comment">% classifier where the class conditional density is a non-parametric
</span><span class="comment">% kernel density estimator.
</span><span class="comment">% Below we give an example where we apply
</span><span class="comment">% a 1-NN classifier to a subset of the MNIST digit set
</span><span class="comment">% (from &lt;http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Introduction/mnistKNNdemo.m mnistKNNdemo.m&gt; : see &lt;http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Introduction/mnist1NNdemo.m mnist1NNdemo.m&gt; for special-purpose
</span><span class="comment">% code that can handle the full dataset).
</span><span class="comment">%%
</span><span class="comment">loadData('mnistAll');
</span><span class="comment">trainndx = 1:10000;
</span><span class="comment">testndx =  1:1000;
</span><span class="comment">ntrain = length(trainndx);
</span><span class="comment">ntest = length(testndx);
</span><span class="comment">Xtrain = double(reshape(mnist.train_images(:,:,trainndx),28*28,ntrain)');
</span><span class="comment">Xtest  = double(reshape(mnist.test_images(:,:,testndx),28*28,ntest)');
</span><span class="comment">ytrain = (mnist.train_labels(trainndx));
</span><span class="comment">ytest  = (mnist.test_labels(testndx));
</span><span class="comment">clear mnist trainndx testndx; % save space
</span><span class="comment">
</span><span class="comment">m = knnFit(Xtrain, ytrain, 1);
</span><span class="comment">ypred = knnPredict(m, Xtest);
</span><span class="comment">errorRate = mean(ypred ~= ytest);
</span><span class="comment">fprintf('Error Rate: %.2f%%\n',100*errorRate);
</span><span class="comment">%%
</span><span class="comment">% Below are the test error rates and running times (train + test)
</span><span class="comment">% for 1NN on different sizes of training and test data
</span><span class="comment">% (generated using &lt;http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Introduction/mnist1NNdemo.m mnist1NNdemo.m&gt;  ).
</span><span class="comment">% Note that the standard training set is 60k
</span><span class="comment">% and the standard test set is 10k.
</span><span class="comment">% Reassuringly, our error rate of 3.09% for 1NN on this standard
</span><span class="comment">% train/ test split is the same as that
</span><span class="comment">% reported  by Kenneth Wilder at
</span><span class="comment">% &lt;http://yann.lecun.com/exdb/mnist/ this league table&gt;.
</span><span class="comment">%%
</span><span class="comment">% &lt;html&gt;
</span><span class="comment">% &lt;table border=1&gt;
</span><span class="comment">% &lt;TR ALIGN=left&gt;
</span><span class="comment">% &lt;td&gt; Ntrain
</span><span class="comment">% &lt;td&gt; Ntest
</span><span class="comment">% &lt;td&gt; Error rate
</span><span class="comment">% &lt;td&gt; Time
</span><span class="comment">% &lt;tr&gt;
</span><span class="comment">% &lt;td&gt; 60k
</span><span class="comment">% &lt;td&gt; 10k
</span><span class="comment">% &lt;td&gt; 3.09%
</span><span class="comment">% &lt;td&gt; 255s
</span><span class="comment">% &lt;tr&gt;
</span><span class="comment">% &lt;td&gt; 60k
</span><span class="comment">% &lt;td&gt; 1k
</span><span class="comment">% &lt;td&gt; 3.80%
</span><span class="comment">% &lt;td&gt; 8s
</span><span class="comment">% &lt;tr&gt;
</span><span class="comment">% &lt;td&gt;  10k
</span><span class="comment">% &lt;td&gt; 1k
</span><span class="comment">% &lt;td&gt; 8.00%
</span><span class="comment">% &lt;td&gt; 1.39s
</span><span class="comment">% &lt;/table&gt;
</span><span class="comment">% &lt;/html&gt;
</span><span class="comment">%%
</span><span class="comment">% From this, we see that increasing the size of the training
</span><span class="comment">% set dramatically reduces the error rate (perhaps a symptom of
</span><span class="comment">% overfitting?). Also, increasing the size of the test set
</span><span class="comment">% dramatically increases the cost of testing (due to the need
</span><span class="comment">% to loop over mini-batches of examples, to save memory).
</span><span class="comment">
</span><span class="comment">%}</span>
</pre><p class="footer"><br>
      Published with MATLAB&reg; 7.9<br></p></div><!--
##### SOURCE BEGIN #####
%% Supervised learning using generative models in pmtk3
% _This page was auto-generated by publishing_
% <http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/tutGenClassif.m tutGenClassif.m>
%
% (HMMs are discussed in more detail 
% <http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/html/tutLVM.html
% here>.)
% (HMMs are discussed in more detail 
% <tutLVM.html here>.)
%
%{
% Generative models for classification/ regression are joint models of the
% outputs and inputs
% of the form $p(y,x|\theta)$.
% We consider various examples below.
%
%% Naive Bayes classifier
% We now a simple kind of generative classifier
% called naive Bayes, which is a model of the form
%%
% $$p(y,x|\theta) = p(y|\pi) \prod_{j=1}^D p(x_j|y,\theta)$$
%%
% We can fit and predict with this model using
% <http://pmtk3.googlecode.com/svn/trunk/toolbox/SupervisedModels/naiveBayes/naiveBayesFit.m naiveBayesFit.m> and  <http://pmtk3.googlecode.com/svn/trunk/toolbox/SupervisedModels/naiveBayes/naiveBayesPredict.m naiveBayesPredict.m>
% For simplicity, the current implementation
% assumes  all the features are binary,
% so $p(x_j|y=c,\theta) = Ber(x_j|\theta_{jc})$.
% It fits by MAP estimation
% with a vague Dirichlet prior (add-one-smoothing).
% Typically results are not too sensitive to the
% setting of this prior (unlike discriminative models).
%
% Below is an example (from <http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Generative_models_for_classification_and_regression/naiveBayesBowDemo.m naiveBayesBowDemo.m> )
% which fits a model to some bag of words data,
% and then classifies a test set.
%%
loadData('XwindowsDocData'); % 900x600, 2 classes Xwindows vs MSwindows
Xtrain = xtrain; Xtest = xtest;
model = naiveBayesFit(Xtrain, ytrain);
ypred_train = naiveBayesPredict(model, Xtrain);
err_train = mean(zeroOneLossFn(ytrain, ypred_train));
ypred_test = naiveBayesPredict(model, Xtest);
err_test = mean(zeroOneLossFn(ytest, ypred_test));
fprintf('misclassification rates  on train = %5.2f pc, on test = %5.2f pc\n', ...
    err_train*100, err_test*100);
%%
% See also <http://pmtk3.googlecode.com/svn/trunk/demos/otherDemos/supervisedModels/naiveBayesMnistDemo.m naiveBayesMnistDemo.m> for application of NBC
% to classify binarized MNIST digits.
%
% It is simple to modify NBC to handle missing data in X
% at training and test time; this is left as an exercise
% to the reader.

%% Discriminant analysis
% Discriminant analysis is a generative classifier where
% the class conditional density  is a multivariate Gaussian:
%%
% $$p(y=c,x|\theta) = \mbox{discrete}(y|\pi) N(x|\mu_c,\Sigma_c)$$
%%
% PMTK supports the following variants of this model:
%%
% <html>
% <table border=1>
% <TR ALIGN=left>
% <td> Type
% <td> Description
% <tr>
% <td> QDA
% <td> <img src="x0x5CSigma_c.png"> is different for each class.
% Induces quadratic decision boundaries.
% <tr>
% <td> LDA 
% <td>  <img src="x0x5CSigma_c0x3D0x5CSigma.png"> is the same (tied) for each class.
% Induces linear decision boundaries.
% <tr>
% <td> DDA
% <td> <img src="x0x5CSigma_c.png"> is diagonal, so the features are conditionally
% independent; this is an example of a naive Bayes classifier.
% Induces linear decision boundaries.
% <tr>
% <td> RDA
% <td> Regularized LDA; uses MAP estimation for <img src="x0x5CSigma.png">.
% <tr>
% <td> shrunkenCentroids
% <td> Diagonal LDA with L1 shrinkage on offsets (see below)
% </table>
% </html>
%%
% We give more details below.

%% QDA/ LDA/ NBC
% Below we give an example (from <http://pmtk3.googlecode.com/svn/trunk/demos/otherDemos/supervisedModels/discrimAnalysisFisherIrisDemo.m discrimAnalysisFisherIrisDemo.m> )
% of how to fit a QDA/LDA/ diagDA model.
% We apply it to a subset of the Fisher Iris dataset.
%%
loadData('fisherIrisData')
X = meas(51:end, 1:2);  % for illustrations use 2 species, 2 features
labels = species(51:end);
[y, support] = canonizeLabels(labels);
types = {'quadratic', 'linear', 'diag'};
for tt=1:length(types)
  model = discrimAnalysisFit(X, y, types{tt});
  h = plotDecisionBoundary(X, y, @(Xtest)discrimAnalysisPredict(model, Xtest));
  title(sprintf('Discrim. analysis of type %s', types{tt}));
  if ~isOctave
    legend(h, support, 'Location', 'NorthWest');
    set(gca, 'Xtick', 5:8, 'Ytick', 2:0.5:4);
  end
  xlabel('X_1'); ylabel('X_2');
end
%%

%% Regularized discriminant analysis
% When fitting a discriminant analysis model 
% we will encounter numerical problems
% when estimating $\Sigma$ when N < D, even if we use
% a tied  covariance matrix (i.e., one shared across classes, a method
% known as linear discriminant analysis).
% A simple solution is to use a Wishart prior to compute a MAP
% estimate of $\Sigma$. This is called regularized discriminant analysis,
% and can be fit using |discrimAnalysisFit(X, y, 'rda', lambda)|,
% where |lambda| controls the amount of regularization.
% See <http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Generative_models_for_classification_and_regression/cancerHighDimClassifDemo.m cancerHighDimClassifDemo.m>
% for an example, which reproduces
% table 18.1 from
% <http://www-stat.stanford.edu/~tibs/ElemStatLearn/ Elements of statistical learning>
% 2nd edn p656.
% (We don't run this demo here since it requires computing
% the SVD of Xtrain (which has size 144* 16063, with 14 classes)
% which takes
% more seconds than we are willing to wait (about 40 sec)).
%

%% Nearest shrunken centroid
% Consider a naive Bayes model in which the diagonal covariance
% is tied. This has O(D) parameters for the covariance, but O(C D) for the mean.
% To prevent overfitting, we can shrink the class-conditional means towards
% the overall mean; this technique is called nearest shrunken centroids. We
% can fit this model using |discrimAnalysisFit(X, y, 'shrunkenCentroids',
% lambda)|. We given an example of this below (from
%  <http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Generative_models_for_classification_and_regression/shrunkenCentroidsSRBCTdemo.m shrunkenCentroidsSRBCTdemo.m> ),
% where we apply the method to
% the SRBCT gene microarray dataset, whose training set
% has size 63*2308 with  C=4 classes.
% This roughly reproduces figure 18.4 from
% <http://www-stat.stanford.edu/~tibs/ElemStatLearn/ Elements of statistical learning>
% 2nd edn p656.
%%
close all; clear all;
loadData('srbct');

Xtest = Xtest(~isnan(ytest), :);
ytest = ytest(~isnan(ytest));

fitFn = @(X,y,lam)  discrimAnalysisFit(X, y, 'shrunkenCentroids', 'lambda',lam);
predictFn = @(model, X)  discrimAnalysisPredict(model, X);

lambdas = linspace(0, 8, 20);
nTrain = length(ytrain);
nTest = length(ytest);
for i=1:length(lambdas)
    model = fitFn(Xtrain, ytrain, lambdas(i));
    yhatTrain = predictFn(model, Xtrain);
    yhatTest = predictFn(model, Xtest);
    errTrain(i) = sum(zeroOneLossFn(yhatTrain, ytrain))/nTrain;
    errTest(i) = sum(zeroOneLossFn(yhatTest, ytest))/nTest;
    numgenes(i) = sum(model.shrunkenCentroids(:) ~= 0);
end

figure;
plot(lambdas, errTrain, 'gx-', lambdas, errTest, 'boREPLACE_WITH_DASH_DASH',...
  'MarkerSize', 10, 'linewidth', 2)
legend('Training', 'Test', 'Location', 'northwest');
xlabel('Amount of shrinkage')
ylabel('misclassification rate')
title('SRBCT data')
%%
% We can also visualize the MAP (blue) and ML (gray) estimate of the means
% for each class.
%% 
bestModel = fitFn(Xtrain, ytrain, 4);
centShrunk = bestModel.shrunkenCentroids;
model = fitFn(Xtrain, ytrain, 0);
centUnshrunk = model.shrunkenCentroids;

[numGroups D] = size(centShrunk);
for g=1:3 % numGroups
    %subplot(4,1,g);
    figure; hold on;
    plot(1:D, centUnshrunk(g,:), 'Color', [.8 .8 .8]);
    plot(1:D, centShrunk(g,:), 'b', 'LineWidth', 2);
    title(sprintf('Class %d', g));
    hold off;
    printPmtkFigure(sprintf('shrunkenCentroidsClass%d', g))
end


%% Robust discriminant analysis
% We can use any joint probability model for the class conditional
% density in a generative classifier.
% To train we just call |generativeClassifierFit(fitFn, X, y)|
% where |fitFn| fits $p(x|y=c,\theta)$.
% To predict we just call 
% |[yhat, post] = generativeClassifierPredict(logprobFn, model, Xtest)|,
% where |logprobFn| computes $p(x|y=c,\theta)$.
%
% As a simple example, we can make each class conditional density
% be a Student distribution, to implement robust discriminant
% analysis. Here is part of <http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Generative_models_for_classification_and_regression/robustDiscrimAnalysisBankruptcyDemo.m robustDiscrimAnalysisBankruptcyDemo.m> 
% which illustrates the syntax:
%
% |modelS = generativeClassifierFit(@studentFit, Xtrain, ytrain)|
%
% |[yhat] = generativeClassifierPredict(@studentLogprob, modelS, Xtest)|
%
% Now we run the entire demo, which uses Student and Gaussian
% class conditional densities, as well as the QDA code.
% We see that the Student distribution is more robust
% than the Gaussian, and that the QDA code gives the same
% results as using a Gaussian model inside the generative
% classifier code, as it should
%%
robustDiscrimAnalysisBankruptcyDemo
%%

%% Using HMMs as class conditional densities
% As a more interesting example, consider the problem of classifying time series,
% such as spoken digits.
% Since each data vector has variable length, it is natural
% to use a Markov or hidden Markov model for the class conditional
% densities.
% (HMMs are discussed in more detail 
% <http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/html/tutLVM.html
% here>.)
% (HMMs are discussed in more detail 
% <tutLVM.html here>.)
% For real-valued data, a linear-Gaussian Markov model is not
% expressive enough, but an HMM with Gaussian emissions
% is quite flexible.
%
% Suppose we have two sequences, corresponding to the spoken words
% "four" and "five". We can train and test the model
% using the code below
% (from <http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Generative_models_for_classification_and_regression/isolatedWordClassificationWithHmmsDemo.m isolatedWordClassificationWithHmmsDemo.m> ).
% It fits two HMMs, one per class.
%%
loadData('data45'); 
% Xtrain{i} is a 13 x T(i) sequence of MFCC data, where T(i) is the length
nstates = 5;
setSeed(0); 
Xtrain = [train4'; train5'];
ytrain = [repmat(4, numel(train4), 1) ; repmat(5, numel(train5), 1)];
[Xtrain, ytrain] = shuffleRows(Xtrain, ytrain);
Xtest = test45'; 
ytest = labels'; 
[Xtest, ytest] = shuffleRows(Xtest, ytest); 
% Initial Guess for params
pi0 = [1, 0, 0, 0, 0];
transmat0 = normalize(diag(ones(nstates, 1)) + ...
            diag(ones(nstates-1, 1), 1), 2);
% Fit
fitArgs = {'pi0', pi0, 'trans0', transmat0, 'maxIter', 10, 'verbose', true};
fitFn   = @(X)hmmFit(X, nstates, 'gauss', fitArgs{:}); 
model = generativeClassifierFit(fitFn, Xtrain, ytrain); 
% Predict
logprobFn = @hmmLogprob;
[yhat, post] = generativeClassifierPredict(logprobFn, model, Xtest);
nerrors = sum(yhat ~= ytest)
%%

%% K-nearest neighbor classifier
% One can view a KNN classifier as a generative
% classifier where the class conditional density is a non-parametric
% kernel density estimator.
% Below we give an example where we apply
% a 1-NN classifier to a subset of the MNIST digit set
% (from <http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Introduction/mnistKNNdemo.m mnistKNNdemo.m> : see <http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Introduction/mnist1NNdemo.m mnist1NNdemo.m> for special-purpose
% code that can handle the full dataset).
%%
loadData('mnistAll');
trainndx = 1:10000;
testndx =  1:1000;
ntrain = length(trainndx);
ntest = length(testndx);
Xtrain = double(reshape(mnist.train_images(:,:,trainndx),28*28,ntrain)');
Xtest  = double(reshape(mnist.test_images(:,:,testndx),28*28,ntest)');
ytrain = (mnist.train_labels(trainndx));
ytest  = (mnist.test_labels(testndx));
clear mnist trainndx testndx; % save space

m = knnFit(Xtrain, ytrain, 1);
ypred = knnPredict(m, Xtest);
errorRate = mean(ypred ~= ytest);
fprintf('Error Rate: %.2f%%\n',100*errorRate);
%%
% Below are the test error rates and running times (train + test)
% for 1NN on different sizes of training and test data
% (generated using <http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Introduction/mnist1NNdemo.m mnist1NNdemo.m>  ).
% Note that the standard training set is 60k 
% and the standard test set is 10k.
% Reassuringly, our error rate of 3.09% for 1NN on this standard
% train/ test split is the same as that
% reported  by Kenneth Wilder at
% <http://yann.lecun.com/exdb/mnist/ this league table>.
%%
% <html>
% <table border=1>
% <TR ALIGN=left>
% <td> Ntrain
% <td> Ntest
% <td> Error rate
% <td> Time
% <tr>
% <td> 60k
% <td> 10k
% <td> 3.09%
% <td> 255s
% <tr>
% <td> 60k
% <td> 1k
% <td> 3.80%
% <td> 8s
% <tr>
% <td>  10k
% <td> 1k
% <td> 8.00%
% <td> 1.39s
% </table>
% </html>
%%
% From this, we see that increasing the size of the training
% set dramatically reduces the error rate (perhaps a symptom of
% overfitting?). Also, increasing the size of the test set
% dramatically increases the cost of testing (due to the need
% to loop over mini-batches of examples, to save memory).

%}

##### SOURCE END #####
--></body></html>