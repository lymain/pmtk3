
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML is auto-generated from an M-file.
To make changes, update the M-file and republish this document.
      --><title>tutSupervised</title><meta name="generator" content="MATLAB 7.9"><meta name="date" content="2010-06-10"><meta name="m-file" content="tutSupervised"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head><body><div class="content"><h2>Contents</h2><div><ul><li><a href="#1">Tutorial on supervised learning using pmtk3</a></li><li><a href="#2">Overfitting, regularization and MAP estimation</a></li></ul></div><h2>Tutorial on supervised learning using pmtk3<a name="1"></a></h2><p><i>This page was auto-generated by publishing</i> <a href="http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/tutSupervised.m">http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/tutSupervised.m</a>.</p><p>We provide a description of the main methods for fitting and using univariate conditional density models of the form <img src="tutSupervised_eq42833.png" alt="$p(y|x,\theta)$"> and <img src="tutSupervised_eq35640.png" alt="$p(y,x|\theta)$"> where y in R for regression and y in {1,...,C} for classification, and where x is some kind of feature vector.</p><pre class="codeinput"><span class="comment">%{
</span><span class="comment">%% Models
</span><span class="comment">% The following is a list of pmtk models that are designed for
</span><span class="comment">% supervised learning (in alphabetical order)
</span><span class="comment">%
</span><span class="comment">% * 'discrimAnalysis' (linear, quadratic or regularized discriminant analysis)
</span><span class="comment">% * 'linreg' (linear regression)
</span><span class="comment">% * 'logreg' (logistic regression,  binary and multiclass)
</span><span class="comment">% * 'mlp' (multilayer perceptron, aka feedforward neural network)
</span><span class="comment">% * 'naiveBayes' (Naive Bayes classifier)
</span><span class="comment">% * 'svm' (support vector machine)
</span><span class="comment">%
</span><span class="comment">% More models may be added in the future.
</span><span class="comment">%
</span><span class="comment">% naiveBayes and discrimAnalysis are generative classifiers, which have the
</span><span class="comment">% form $p(y,x|\theta)$ for y in {1,...,C}. The remaining models are discriminative
</span><span class="comment">% models.
</span><span class="comment">%
</span><span class="comment">%% Methods
</span><span class="comment">% Below we describe the main 'methods' that can be applied to these models.
</span><span class="comment">% Note that these are just Matlab functions, but we will sometimes call
</span><span class="comment">% them methods since they behave like object-oriented methods.
</span><span class="comment">%
</span><span class="comment">% Note that *not all models support all methods*.
</span><span class="comment">% To find out if a model of type foo supports method bar,
</span><span class="comment">% just type |help fooBar|. If you get an error, then you know
</span><span class="comment">% that foo does not implement bar.
</span><span class="comment">%
</span><span class="comment">%% Creating a model
</span><span class="comment">% To create a model of type 'foo', use one of the following
</span><span class="comment">%%
</span><span class="comment">%  model = fooCreate(...) % manually specify parameters
</span><span class="comment">% model = fooFit(X, y, ...) % Compute ML or MAP estimate of params
</span><span class="comment">% model = fooFitBayes(X, y, ...) % Compute posterior of params
</span><span class="comment">%%
</span><span class="comment">% where
</span><span class="comment">%
</span><span class="comment">% *  '...' refers to optional arguments (see below)
</span><span class="comment">% * X  is an N*D design matrix containing the training data,
</span><span class="comment">%  where N is the number of training cases and D is the number of features.
</span><span class="comment">% * y is an N*1 response vector, which can be real-valued (regression),
</span><span class="comment">%     0/1 or -1/+1 (binary classification), or 1:C (multi-class).
</span><span class="comment">%
</span><span class="comment">% It is straightforward to handle NaNs (missing values) in X when fitting a
</span><span class="comment">% generative model; however this functionality is not yet supported.
</span><span class="comment">% NaNs in y correspond to semi-supervised learning, which
</span><span class="comment">% is not yet supported.
</span><span class="comment">%
</span><span class="comment">% The resulting model is a Matlab structure, rather than an object.
</span><span class="comment">% However, we will sometimes call it an object, since it behaves like one.
</span><span class="comment">%
</span><span class="comment">% In the case of fooCreate and fooFit, the parameters are point estimates.
</span><span class="comment">% In the case of fooFitBayes, the parameters are represented as
</span><span class="comment">% distributions; this is often represented parametrically in terms
</span><span class="comment">% of the hyper-parameters. The details will be explained below
</span><span class="comment">% when we look at specific model classes.
</span><span class="comment">%
</span><span class="comment">%
</span><span class="comment">% As an example, we can fit a linear regression model to the caterpillar
</span><span class="comment">% data set as follows (using |linregBayesCaterpillar|):
</span><span class="comment">%%
</span><span class="comment">clear all
</span><span class="comment">X = loadData('caterpillar'); % from http://www.ceremade.dauphine.fr/~xian/BCS/caterpillar
</span><span class="comment">y = log(X(:,11)); % log number of nests
</span><span class="comment">X = X(:,1:10);
</span><span class="comment">[model] = linregFit(X, y) % MLE
</span><span class="comment">%%
</span><span class="comment">% Or we can use Bayesian methods
</span><span class="comment">[modelB] = linregFitBayes(X, y, 'prior', 'uninf') % uninformative Jeffreys prior
</span><span class="comment">%%
</span><span class="comment">%% Examining the model's parameters
</span><span class="comment">% If you fit by the model by ML/MAP estimation, you can examine the
</span><span class="comment">% parameters by typing |model.params|, where |params| is the name of the
</span><span class="comment">% parameter you are interested in (this varies from model to model,
</span><span class="comment">% as we saw above).
</span><span class="comment">%
</span><span class="comment">% If you fit the model by Bayesian inference, you can examine
</span><span class="comment">% summaries of the posterior using
</span><span class="comment">%%
</span><span class="comment">%  post = fooParamBayes(model)
</span><span class="comment">%
</span><span class="comment">% This computes the marginals of each parameter, and displays
</span><span class="comment">% the posterior mean, standard deviation and 95% credible interval
</span><span class="comment">% as a latex table.
</span><span class="comment">% A simple way to assess if a coefficient is significantly different from 0
</span><span class="comment">% is to see if its 95% CI excludes 0 or not. If it does, we can put
</span><span class="comment">% a little * next to it.
</span><span class="comment">% For example, here is the output for the linear regression model
</span><span class="comment">%%
</span><span class="comment">linregParamBayes(modelB, 'display', true);
</span><span class="comment">%%
</span><span class="comment">% We see that coefficients  0, 1, 2, 4, 5 are "significant"
</span><span class="comment">% by this measure. (Other methods of testing significance, based on Bayes factors, can also be
</span><span class="comment">% used, but are a bit more complicated, and one cannot use uninformative
</span><span class="comment">% priors in such cases.)
</span><span class="comment">%
</span><span class="comment">% It turns out that in this particular example, the Bayesian analysis is
</span><span class="comment">% identical to a classical frequentist analysis (because the posterior
</span><span class="comment">% for linear regression under an uninformative Jeffreys prior is equivalent
</span><span class="comment">% to the sampling distribution of the MLE). To see this, let us use the
</span><span class="comment">% &lt;http://www.mathworks.com/products/statistics/ stats toolbox&gt; to fit the model and perform a frequentist analysis:
</span><span class="comment">%%
</span><span class="comment">[w, wint] = regress(y, [ones(size(X,1),1) X]);
</span><span class="comment">for i=1:length(w)
</span><span class="comment">  fprintf('%5.3f, [%5.3f, %5.3f]\n', w(i), wint(i,1), wint(i,2));
</span><span class="comment">end
</span><span class="comment">fprintf('\n');
</span><span class="comment">%%
</span><span class="comment">% We see that the MLE is the same as the posterior mean,
</span><span class="comment">% and the 95% confidence interval is the same as the 95% credible interval.
</span><span class="comment">% (If you don't have the stats toolbox, you can use |linregFrequentist|
</span><span class="comment">% instead, which does more or less the same thing.)
</span><span class="comment">%
</span><span class="comment">% In general, a Bayesian and frequentist analysis may not give the same
</span><span class="comment">% results. In pmtk, all inference is Bayesian.
</span><span class="comment">
</span><span class="comment">%% Using a model for prediction
</span><span class="comment">% In machine learning, we usually care more about prediction than in trying
</span><span class="comment">% to interpret the fitted parameters (especially since many models of
</span><span class="comment">% interest are hard to interpret or even strictly unidentifiable).
</span><span class="comment">%
</span><span class="comment">% Once the model has been created, you can use it to make predictions
</span><span class="comment">%  as follows
</span><span class="comment">%
</span><span class="comment">%%
</span><span class="comment">%  [yhat, py] = fooPredict(model, Xtest) % plugin approximation
</span><span class="comment">%  [yhat, py] = fooPredictBayes(model, Xtest) % posterior predictive
</span><span class="comment">%%
</span><span class="comment">% Here yhat is an Ntest*1 vector of predicted responses of the same type
</span><span class="comment">% as ytrain, where Ntest is the number of rows in Xtest.
</span><span class="comment">% For regression this is the predicted mean, for classification this is the
</span><span class="comment">% predicted mode (most probable class label).
</span><span class="comment">% The meaning of py depends on the model, as follows:
</span><span class="comment">%
</span><span class="comment">% * For regression, py is an Ntest*1 vector of predicted variances.
</span><span class="comment">% * For binary classification, py is an Ntest*1 vector of the probability of being in class 1.
</span><span class="comment">% * For multi-class, py is an Ntest*C matrix, where py(i,c) = p(y=c|Xtest(i,:),params)
</span><span class="comment">%
</span><span class="comment">% The difference between |predict| and |predictBayes| is as follows.
</span><span class="comment">% |predict| computes $p(y|x,\hat{\theta})$, which "plugs in" a point estimate
</span><span class="comment">% of the parameters, where |predictBayes| computes $\int p(y|x,\theta)
</span><span class="comment">% p(\theta) d\theta$ ; this is called the (posterior) predictive density.
</span><span class="comment">% In practice, the Bayesian approach results in similar (often identical)
</span><span class="comment">% values for yhat, but quite different values for py. In particular, the
</span><span class="comment">% uncertainty is reflected more accurately in the Bayesian approach.
</span><span class="comment">
</span><span class="comment">%% Prediction with linear regression
</span><span class="comment">%
</span><span class="comment">% As an example, consider fitting a linear regression model to some 1d data
</span><span class="comment">% using MLE and Bayesian methods (using |linregPostPredLinearDemo|),
</span><span class="comment">% and then plotting the predictions on a test set (which is just a grid of
</span><span class="comment">% point in the interval [-7,7])
</span><span class="comment">%%
</span><span class="comment">setSeed(1);
</span><span class="comment">[xtrain, ytrain, xtest] =  polyDataMake('sampling', 'sparse', 'deg', 2);
</span><span class="comment">
</span><span class="comment">fitMethods = {@(x,y) linregFit(x,y), @(x,y) linregFitBayes(x,y)};
</span><span class="comment">predictMethods = {@(x,y) linregPredict(x,y), @(x,y) linregPredictBayes(x,y)};
</span><span class="comment">names = {'MLE', 'Bayes'};
</span><span class="comment">
</span><span class="comment">for i=1:length(fitMethods)
</span><span class="comment">  model = fitMethods{i}(xtrain, ytrain);
</span><span class="comment">  [mu, v] = predictMethods{i}(model, xtest);
</span><span class="comment">  figure; hold on
</span><span class="comment">  plot(xtest, mu,  'k-', 'linewidth', 3, 'displayname', 'prediction');
</span><span class="comment">  plot(xtrain,ytrain,'ro','markersize', 14, 'linewidth', 3, ...
</span><span class="comment">     'displayname', 'training data');
</span><span class="comment">  NN = length(xtest);
</span><span class="comment">  ndx = 1:5:NN; % plot subset of errorbars to reduce clutter
</span><span class="comment">  sigma = sqrt(v);
</span><span class="comment">  legend('location', 'northwest');
</span><span class="comment">  errorbar(xtest(ndx), mu(ndx), sigma(ndx));
</span><span class="comment">  title(names{i});
</span><span class="comment">end
</span><span class="comment">%%
</span><span class="comment">% We see that the main difference is that in the plugin case, the predicted
</span><span class="comment">% variance is constant, whereas in the Bayesian case, the predicted
</span><span class="comment">% variance increases as we move further away from the training data.
</span><span class="comment">%
</span><span class="comment">
</span><span class="comment">
</span><span class="comment">%% Prediction with logistic regression
</span><span class="comment">% As another example of this, consider fitting a binary logistic regression
</span><span class="comment">% model to some SAT scores, where the response is whether the student
</span><span class="comment">% passed or failed the class. First we compute the MLE and use a plugin
</span><span class="comment">% approximation for prediction, as is standard practice (from |logregSATdemo|)
</span><span class="comment">%
</span><span class="comment">%%
</span><span class="comment">close all; clear all
</span><span class="comment">stat = loadData('sat');  y = stat(:,1); X = stat(:,4);
</span><span class="comment">model = logregFit(X, y);
</span><span class="comment">[yhat, prob] = logregPredict(model, X); %ok
</span><span class="comment">figure;
</span><span class="comment">plot(X, y, 'ko', 'linewidth', 2, 'MarkerSize', 7, 'markerfacecolor', 'k');
</span><span class="comment">hold on
</span><span class="comment">plot(X, prob, 'ro', 'linewidth', 2,'MarkerSize', 10)
</span><span class="comment">
</span><span class="comment">%%
</span><span class="comment">% Now we fit the model using Bayesian inference with an noninformative
</span><span class="comment">% Gaussian prior. By default, the fitting procedure uses a Laplace
</span><span class="comment">% approximation to the posterior. To approximate the predictive density, we
</span><span class="comment">% can plugin in the posterior mean:
</span><span class="comment">%%
</span><span class="comment">% $$p(y=1|x,D) = \int \sigma(w^T * x) N(w|\mu,\Sigma) dw
</span><span class="comment">% \approx \sigma(w^T \mu)$$
</span><span class="comment">%%
</span><span class="comment">% However, this gives essentially the same result as plugging in the MLE.
</span><span class="comment">% To get a measure of confidence in this prediction, we can sample values
</span><span class="comment">% of w from their (approximate) posterior, use each such sample to make a
</span><span class="comment">% prediction, and then compute empirical quantiles of this distribution to
</span><span class="comment">% get a 95% credible interval.
</span><span class="comment">% This is done using |logregPredictBayes| and gives the results shown below
</span><span class="comment">% (from |logregSATdemoBayes|)
</span><span class="comment">
</span><span class="comment">%%
</span><span class="comment">model = logregFitBayes(X, y);
</span><span class="comment">[yhat, prob, pCI] = logregPredictBayes(model, X);
</span><span class="comment">figure;
</span><span class="comment">plot(X, y, 'ko', 'linewidth', 2, 'MarkerSize', 7, 'markerfacecolor', 'k');
</span><span class="comment">hold on
</span><span class="comment">plot(X, prob, 'ro', 'linewidth', 2,'MarkerSize', 10)
</span><span class="comment">for i=1:size(X,1)
</span><span class="comment">  line([X(i,1) X(i,1)], [pCI(i,1) pCI(i,3)]);
</span><span class="comment">end
</span><span class="comment">
</span><span class="comment">%%
</span><span class="comment">%
</span><span class="comment">% Not all models support Bayesian fitting or prediction (the list will hopefully grow
</span><span class="comment">% over time, as people add new methods).
</span><span class="comment">% However, all models provide some measure of confidence on their outputs.
</span><span class="comment">%
</span><span class="comment">
</span><span class="comment">%% Visualizing the decision boundaires
</span><span class="comment">% When comparing classification methods, it is useful to apply them to 2d
</span><span class="comment">% datasets and to plot the regions of space that get mapped to each class;
</span><span class="comment">% these are called decision regions, and the boundaries are called decision
</span><span class="comment">% boundaries. We can do this using the |plotDecisionBoundary(X, y, predFn)|
</span><span class="comment">% function, where predFn(X) takes a test matrix and computes the MAP
</span><span class="comment">% estimate of the labels for each row.
</span><span class="comment">% As an example of this, consider the famous XOR dataset.
</span><span class="comment">% Let us try fitting a logistic regression model to it in the original
</span><span class="comment">% feature space (from |logregXorLinearDemo|)
</span><span class="comment">%%
</span><span class="comment">close all; clear all;
</span><span class="comment">[X, y] = createXORdata();
</span><span class="comment">model = logregFit(X, y);
</span><span class="comment">plotDecisionBoundary(X, y, @(X)logregPredict(model, X));
</span><span class="comment">yhat = logregPredict(model, X);
</span><span class="comment">errorRate = mean(yhat ~= y)
</span><span class="comment">
</span><span class="comment">% We see that the method performs at chance level, because the data is not
</span><span class="comment">% linearly separable. We give a simple fix to this problem below, when we
</span><span class="comment">% discuss basis function expansion.
</span><span class="comment">
</span><span class="comment">%% Preprocessing, basis function expansion and kernels
</span><span class="comment">% We are free to preprocess the data in any way we choose before fitting the model.
</span><span class="comment">% In pmtk, you can create a preprocessor (pp) 'object', and then pass it to the fitting function;
</span><span class="comment">% the pp will  be applied to the training data before fitting the model, and will be applied again to the test data.
</span><span class="comment">% The advantage of this approach is that the pp is stored inside the model, which makes sense,
</span><span class="comment">% since it is an integral part of the model.
</span><span class="comment">%
</span><span class="comment">% One common form of preprocessing is basis function expansion.
</span><span class="comment">% This replaces the original features with a larger set, thus permitting us to fit nonlinear models.
</span><span class="comment">% A popular approach is to use kernel functions, and to define the new feature vector as follows:
</span><span class="comment">%%
</span><span class="comment">% $$\phi(x) = (K(x,\mu_1), ..., K(x,mu_D))$$
</span><span class="comment">%%
</span><span class="comment">% where the $\mu_j$ are 'prototypes'
</span><span class="comment">% and K(x,x') is a 'kernel function', which in this context just means a function of two arguments.
</span><span class="comment">% A common example is the Gaussian or RBF kernel
</span><span class="comment">%%
</span><span class="comment">% $$K(x,x') = \exp(-\frac{||x-x'||^2}{2\sigma^2})$$
</span><span class="comment">%%
</span><span class="comment">% where $\sigma$ is the 'bandwidth'.
</span><span class="comment">% Another common example is the polynomial kerne
</span><span class="comment">%%
</span><span class="comment">% $$K(x,x') = (1+x^T x')^d$$
</span><span class="comment">%%
</span><span class="comment">% where d is the degree.
</span><span class="comment">% Often we take the prototypes to be the training vectors, but we don't have to.
</span><span class="comment">%
</span><span class="comment">% Below we show an example where we fit the XOR data using kernelized
</span><span class="comment">% logistic regression, with various kernels and prototypes.
</span><span class="comment">%%
</span><span class="comment">clear all; close all
</span><span class="comment">[X, y] = createXORdata();
</span><span class="comment">rbfScale = 1;
</span><span class="comment">polydeg  = 2;
</span><span class="comment">protoTypes = [1 1; 1 5; 5 1; 5 5];
</span><span class="comment">kernels = {@(X1, X2)kernelRbfSigma(X1, X2, rbfScale)
</span><span class="comment">           @(X1, X2)kernelRbfSigma(X1, protoTypes, rbfScale)
</span><span class="comment">           @(X1, X2)kernelPoly(X1, X2, polydeg)};
</span><span class="comment">titles  = {'rbf', 'rbf prototypes', 'poly'};
</span><span class="comment">for i=1:numel(kernels)
</span><span class="comment">    preproc.kernelFn = kernels{i};
</span><span class="comment">    model = logregFit(X, y, 'preproc', preproc);
</span><span class="comment">    yhat = logregPredict(model, X);
</span><span class="comment">    errorRate = mean(yhat ~= y);
</span><span class="comment">    fprintf('Error rate using %s features: %2.f%%\n', titles{i}, 100*errorRate);
</span><span class="comment">    predictFcn = @(Xtest)logregPredict(model, Xtest);
</span><span class="comment">    plotDecisionBoundary(X, y, predictFcn);
</span><span class="comment">    if i==2
</span><span class="comment">       hold on;
</span><span class="comment">       plot(protoTypes(:, 1), protoTypes(:, 2), '*k', 'linewidth', 2, 'markersize', 10)
</span><span class="comment">    end
</span><span class="comment">    title(titles{i});
</span><span class="comment">end
</span><span class="comment">
</span><span class="comment">% We discuss how to choose the parameters of the kernel functions below.
</span><span class="comment">%}</span>
</pre><h2>Overfitting, regularization and MAP estimation<a name="2"></a></h2><p>Using maximum likelihood to train a model  often results in overfitting. This means that the model fits the training set well, but is overly complex and consequently performs poorly on test data. This is easiest to illustrate in the context of polynomial regression in 1d, as shown below (based on <tt>linregPolyVsReg</tt>)</p><pre class="codeinput">[xtrain, ytrain, xtest, ytestNoisefree, ytest] = polyDataMake(<span class="string">'sampling'</span>,<span class="string">'thibaux'</span>);
deg = 14;
Xtrain = xtrain; Xtest = xtest;
pp = preprocessorCreate(<span class="string">'rescaleX'</span>, true, <span class="string">'poly'</span>, deg);
model = linregFit(Xtrain, ytrain, <span class="string">'preproc'</span>, pp);
[ypredTest] = linregPredict(model, Xtest);
figure;
scatter(xtrain, ytrain,<span class="string">'b'</span>,<span class="string">'filled'</span>); hold <span class="string">on</span>;
plot(xtest, ypredTest, <span class="string">'k'</span>, <span class="string">'linewidth'</span>, 3);
</pre><img vspace="5" hspace="5" src="tutSupervised_01.png" alt=""> <p>We see that the fitted function is very 'wiggly', and fits the noise. This is common when we have very little data compared to the number of parameters (in this example, we have N=21 data points, and 15 parameters, since we fit a degree 14 polynomial).</p><p>Using Bayesian inference with an uninformative prior does not help, since the mean of the posterior predictive distribution can be obtained by plugging in the posterior mean parameter, which is equal to the MLE:</p><p><img src="tutSupervised_eq94705.png" alt="$$ E[y|x,D] = E[ E[y|x,w]| D] = E[ x^T  w | D] = x^T E[w|D]$$"></p><p>What we need is an informative prior, that encodes our preference for simpler models. A popular away to achieve this is to use a zero-mean spherical Gaussian prior of the form <img src="tutSupervised_eq54469.png" alt="$p(w) = N(w|0,\alpha^{-1} I)$">, where <img src="tutSupervised_eq87919.png" alt="$\alpha$"> is the precision (strength) of the prior. This says that, a priori, we expect the regression weights to be small, which means we believe the function is simple/ smooth (not "too wiggly"). We can compute the posterior of w with this prior using a variety of different models/ likelihood functions. But a computationally simpler approach is to use MAP estimation (aka regularization), which just computes the posterior mode, which is given by</p><p><img src="tutSupervised_eq93496.png" alt="$$\hat{w} = \arg \max_w \log p(w|D) = \arg \max_w \log p(D|w) + \log p(w)$$"></p><p>In the case of a Gaussian likelihood (linear regression) and Gaussian prior, we get</p><p><img src="tutSupervised_eq91539.png" alt="$$\log p(D|w) + \log p(w) = -\frac{\beta}{2} ||Xw - y||^2 -&#xA; \frac{\alpha}{2} ||w||^2$$"></p><p>where <img src="tutSupervised_eq35555.png" alt="$\beta=1/\sigma^2$"> is the precision of the measurement noise. If we define <img src="tutSupervised_eq65989.png" alt="$\lambda = \alpha/ \beta$"> to be the amount of regularization, we can rewrite this as follows:</p><p><img src="tutSupervised_eq19845.png" alt="$$\hat{w} = \arg \min_w ||Xw - y||^2 + \lambda ||w||^2$$"></p><p>We see that this is a least squares problem with an L2 penalty on the weight vector (this is known as ridge regression). Below we show how to fit this model for several settings of <img src="tutSupervised_eq23351.png" alt="$\lambda$">. We see that increasing <img src="tutSupervised_eq23351.png" alt="$\lambda$"> results in a smoother fit.</p><pre class="codeinput">lambdas = [0.00001, 0.001];
NL = length(lambdas);
<span class="keyword">for</span> k=1:NL
    lambda = lambdas(k);
    model = linregFit(Xtrain, ytrain, <span class="string">'lambda'</span>, lambda, <span class="string">'preproc'</span>, pp);
    [ypredTest] = linregPredict(model, Xtest);
    figure;
    scatter(xtrain, ytrain,<span class="string">'b'</span>,<span class="string">'filled'</span>);
    hold <span class="string">on</span>;
    plot(xtest, ypredTest, <span class="string">'k'</span>, <span class="string">'linewidth'</span>, 3);
    title(sprintf(<span class="string">'lambda %5.3f'</span>, log10(lambda)))
<span class="keyword">end</span>
</pre><img vspace="5" hspace="5" src="tutSupervised_02.png" alt=""> <img vspace="5" hspace="5" src="tutSupervised_03.png" alt=""> <p>If <img src="tutSupervised_eq23351.png" alt="$\lambda$"> is too small, the model will overfit (since the function is too wiggly), but if it is too big, the model will underfit (since the function is too smooth). This is illustrated below, where we examine the measn squared error on the training and  test sets as a function of <img src="tutSupervised_eq23351.png" alt="$\lambda$">. This illustrates the characteristic U-shape on the test set.</p><pre class="codeinput">lambdas = logspace(-10,1.2,9);
NL = length(lambdas);
 testMse = zeros(1,NL); trainMse = zeros(1,NL);
<span class="keyword">for</span> k=1:NL
    lambda = lambdas(k);
    [model] = linregFit(Xtrain, ytrain, <span class="string">'lambda'</span>, lambda, <span class="string">'preproc'</span>, pp);
    ypredTest = linregPredict(model, Xtest);
    ypredTrain = linregPredict(model, Xtrain);
    testMse(k) = mean((ypredTest - ytest).^2);
    trainMse(k) = mean((ypredTrain - ytrain).^2);
<span class="keyword">end</span>
figure; hold <span class="string">on</span>
ndx =  log(lambdas); <span class="comment">% 1:length(lambdas);</span>
plot(ndx, trainMse, <span class="string">'bs:'</span>, <span class="string">'linewidth'</span>, 2, <span class="string">'markersize'</span>, 12);
plot(ndx, testMse, <span class="string">'rx-'</span>, <span class="string">'linewidth'</span>, 2, <span class="string">'markersize'</span>, 12);
legend(<span class="string">'train mse'</span>, <span class="string">'test mse'</span>, <span class="string">'location'</span>, <span class="string">'northwest'</span>)
xlabel(<span class="string">'log regularizer'</span>)
</pre><img vspace="5" hspace="5" src="tutSupervised_04.png" alt=""> <p class="footer"><br>
      Published with MATLAB&reg; 7.9<br></p></div><!--
##### SOURCE BEGIN #####
%% Tutorial on supervised learning using pmtk3
% _This page was auto-generated by publishing_
% <http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/tutSupervised.m>.
%
% We provide a description of the main methods for fitting and using univariate
% conditional density models of the form $p(y|x,\theta)$ and $p(y,x|\theta)$
% where y in R for regression and y in {1,...,C} for classification,
% and where x is some kind of feature vector. 
%
%{
%% Models
% The following is a list of pmtk models that are designed for 
% supervised learning (in alphabetical order)
%
% * 'discrimAnalysis' (linear, quadratic or regularized discriminant analysis)
% * 'linreg' (linear regression)
% * 'logreg' (logistic regression,  binary and multiclass)
% * 'mlp' (multilayer perceptron, aka feedforward neural network)
% * 'naiveBayes' (Naive Bayes classifier)
% * 'svm' (support vector machine)
%
% More models may be added in the future.
%
% naiveBayes and discrimAnalysis are generative classifiers, which have the
% form $p(y,x|\theta)$ for y in {1,...,C}. The remaining models are discriminative
% models.
%
%% Methods
% Below we describe the main 'methods' that can be applied to these models.
% Note that these are just Matlab functions, but we will sometimes call
% them methods since they behave like object-oriented methods.
%
% Note that *not all models support all methods*.
% To find out if a model of type foo supports method bar,
% just type |help fooBar|. If you get an error, then you know
% that foo does not implement bar.
%
%% Creating a model
% To create a model of type 'foo', use one of the following
%%
%  model = fooCreate(...) % manually specify parameters
% model = fooFit(X, y, ...) % Compute ML or MAP estimate of params
% model = fooFitBayes(X, y, ...) % Compute posterior of params
%%
% where
%
% *  '...' refers to optional arguments (see below)
% * X  is an N*D design matrix containing the training data,
%  where N is the number of training cases and D is the number of features.
% * y is an N*1 response vector, which can be real-valued (regression),
%     0/1 or -1/+1 (binary classification), or 1:C (multi-class).
%
% It is straightforward to handle NaNs (missing values) in X when fitting a
% generative model; however this functionality is not yet supported.
% NaNs in y correspond to semi-supervised learning, which
% is not yet supported.
%
% The resulting model is a Matlab structure, rather than an object.
% However, we will sometimes call it an object, since it behaves like one.
%
% In the case of fooCreate and fooFit, the parameters are point estimates.
% In the case of fooFitBayes, the parameters are represented as
% distributions; this is often represented parametrically in terms
% of the hyper-parameters. The details will be explained below
% when we look at specific model classes.
%
%
% As an example, we can fit a linear regression model to the caterpillar
% data set as follows (using |linregBayesCaterpillar|):
%%
clear all
X = loadData('caterpillar'); % from http://www.ceremade.dauphine.fr/~xian/BCS/caterpillar
y = log(X(:,11)); % log number of nests
X = X(:,1:10);
[model] = linregFit(X, y) % MLE
%%
% Or we can use Bayesian methods
[modelB] = linregFitBayes(X, y, 'prior', 'uninf') % uninformative Jeffreys prior
%%
%% Examining the model's parameters
% If you fit by the model by ML/MAP estimation, you can examine the
% parameters by typing |model.params|, where |params| is the name of the
% parameter you are interested in (this varies from model to model,
% as we saw above).
%
% If you fit the model by Bayesian inference, you can examine
% summaries of the posterior using
%%
%  post = fooParamBayes(model)
%
% This computes the marginals of each parameter, and displays
% the posterior mean, standard deviation and 95% credible interval
% as a latex table.
% A simple way to assess if a coefficient is significantly different from 0
% is to see if its 95% CI excludes 0 or not. If it does, we can put
% a little * next to it.
% For example, here is the output for the linear regression model
%%
linregParamBayes(modelB, 'display', true);
%%
% We see that coefficients  0, 1, 2, 4, 5 are "significant"
% by this measure. (Other methods of testing significance, based on Bayes factors, can also be
% used, but are a bit more complicated, and one cannot use uninformative
% priors in such cases.)
%
% It turns out that in this particular example, the Bayesian analysis is
% identical to a classical frequentist analysis (because the posterior
% for linear regression under an uninformative Jeffreys prior is equivalent
% to the sampling distribution of the MLE). To see this, let us use the
% <http://www.mathworks.com/products/statistics/ stats toolbox> to fit the model and perform a frequentist analysis:
%%
[w, wint] = regress(y, [ones(size(X,1),1) X]);
for i=1:length(w)
  fprintf('%5.3f, [%5.3f, %5.3f]\n', w(i), wint(i,1), wint(i,2));
end
fprintf('\n');
%%
% We see that the MLE is the same as the posterior mean,
% and the 95% confidence interval is the same as the 95% credible interval.
% (If you don't have the stats toolbox, you can use |linregFrequentist|
% instead, which does more or less the same thing.)
%
% In general, a Bayesian and frequentist analysis may not give the same
% results. In pmtk, all inference is Bayesian.

%% Using a model for prediction
% In machine learning, we usually care more about prediction than in trying
% to interpret the fitted parameters (especially since many models of
% interest are hard to interpret or even strictly unidentifiable).
%
% Once the model has been created, you can use it to make predictions
%  as follows
%
%%
%  [yhat, py] = fooPredict(model, Xtest) % plugin approximation
%  [yhat, py] = fooPredictBayes(model, Xtest) % posterior predictive
%%
% Here yhat is an Ntest*1 vector of predicted responses of the same type
% as ytrain, where Ntest is the number of rows in Xtest.
% For regression this is the predicted mean, for classification this is the
% predicted mode (most probable class label).
% The meaning of py depends on the model, as follows:
%   
% * For regression, py is an Ntest*1 vector of predicted variances.
% * For binary classification, py is an Ntest*1 vector of the probability of being in class 1.
% * For multi-class, py is an Ntest*C matrix, where py(i,c) = p(y=c|Xtest(i,:),params)
%
% The difference between |predict| and |predictBayes| is as follows.
% |predict| computes $p(y|x,\hat{\theta})$, which "plugs in" a point estimate
% of the parameters, where |predictBayes| computes $\int p(y|x,\theta)
% p(\theta) d\theta$ ; this is called the (posterior) predictive density.
% In practice, the Bayesian approach results in similar (often identical)
% values for yhat, but quite different values for py. In particular, the
% uncertainty is reflected more accurately in the Bayesian approach.

%% Prediction with linear regression
%
% As an example, consider fitting a linear regression model to some 1d data
% using MLE and Bayesian methods (using |linregPostPredLinearDemo|),
% and then plotting the predictions on a test set (which is just a grid of
% point in the interval [-7,7])
%%
setSeed(1);
[xtrain, ytrain, xtest] =  polyDataMake('sampling', 'sparse', 'deg', 2);

fitMethods = {@(x,y) linregFit(x,y), @(x,y) linregFitBayes(x,y)};
predictMethods = {@(x,y) linregPredict(x,y), @(x,y) linregPredictBayes(x,y)};
names = {'MLE', 'Bayes'};

for i=1:length(fitMethods)
  model = fitMethods{i}(xtrain, ytrain);
  [mu, v] = predictMethods{i}(model, xtest);
  figure; hold on
  plot(xtest, mu,  'k-', 'linewidth', 3, 'displayname', 'prediction');
  plot(xtrain,ytrain,'ro','markersize', 14, 'linewidth', 3, ...
     'displayname', 'training data');
  NN = length(xtest);
  ndx = 1:5:NN; % plot subset of errorbars to reduce clutter
  sigma = sqrt(v);
  legend('location', 'northwest');
  errorbar(xtest(ndx), mu(ndx), sigma(ndx));
  title(names{i});
end
%%
% We see that the main difference is that in the plugin case, the predicted
% variance is constant, whereas in the Bayesian case, the predicted
% variance increases as we move further away from the training data.
%


%% Prediction with logistic regression
% As another example of this, consider fitting a binary logistic regression
% model to some SAT scores, where the response is whether the student
% passed or failed the class. First we compute the MLE and use a plugin
% approximation for prediction, as is standard practice (from |logregSATdemo|)
%
%%
close all; clear all
stat = loadData('sat');  y = stat(:,1); X = stat(:,4);
model = logregFit(X, y);
[yhat, prob] = logregPredict(model, X); %ok
figure;
plot(X, y, 'ko', 'linewidth', 2, 'MarkerSize', 7, 'markerfacecolor', 'k');
hold on
plot(X, prob, 'ro', 'linewidth', 2,'MarkerSize', 10)

%%
% Now we fit the model using Bayesian inference with an noninformative
% Gaussian prior. By default, the fitting procedure uses a Laplace
% approximation to the posterior. To approximate the predictive density, we
% can plugin in the posterior mean:
%%
% $$p(y=1|x,D) = \int \sigma(w^T * x) N(w|\mu,\Sigma) dw 
% \approx \sigma(w^T \mu)$$
%%
% However, this gives essentially the same result as plugging in the MLE.
% To get a measure of confidence in this prediction, we can sample values
% of w from their (approximate) posterior, use each such sample to make a
% prediction, and then compute empirical quantiles of this distribution to
% get a 95% credible interval.
% This is done using |logregPredictBayes| and gives the results shown below
% (from |logregSATdemoBayes|)

%%
model = logregFitBayes(X, y);
[yhat, prob, pCI] = logregPredictBayes(model, X);
figure;
plot(X, y, 'ko', 'linewidth', 2, 'MarkerSize', 7, 'markerfacecolor', 'k');
hold on
plot(X, prob, 'ro', 'linewidth', 2,'MarkerSize', 10)
for i=1:size(X,1)
  line([X(i,1) X(i,1)], [pCI(i,1) pCI(i,3)]);
end

%%
%
% Not all models support Bayesian fitting or prediction (the list will hopefully grow
% over time, as people add new methods).
% However, all models provide some measure of confidence on their outputs.
%

%% Visualizing the decision boundaires
% When comparing classification methods, it is useful to apply them to 2d
% datasets and to plot the regions of space that get mapped to each class;
% these are called decision regions, and the boundaries are called decision
% boundaries. We can do this using the |plotDecisionBoundary(X, y, predFn)|
% function, where predFn(X) takes a test matrix and computes the MAP
% estimate of the labels for each row.
% As an example of this, consider the famous XOR dataset.
% Let us try fitting a logistic regression model to it in the original
% feature space (from |logregXorLinearDemo|)
%%
close all; clear all;
[X, y] = createXORdata();
model = logregFit(X, y);
plotDecisionBoundary(X, y, @(X)logregPredict(model, X));
yhat = logregPredict(model, X);
errorRate = mean(yhat ~= y)

% We see that the method performs at chance level, because the data is not
% linearly separable. We give a simple fix to this problem below, when we
% discuss basis function expansion.

%% Preprocessing, basis function expansion and kernels
% We are free to preprocess the data in any way we choose before fitting the model.
% In pmtk, you can create a preprocessor (pp) 'object', and then pass it to the fitting function;
% the pp will  be applied to the training data before fitting the model, and will be applied again to the test data.
% The advantage of this approach is that the pp is stored inside the model, which makes sense,
% since it is an integral part of the model.
%
% One common form of preprocessing is basis function expansion.
% This replaces the original features with a larger set, thus permitting us to fit nonlinear models.
% A popular approach is to use kernel functions, and to define the new feature vector as follows:
%%
% $$\phi(x) = (K(x,\mu_1), ..., K(x,mu_D))$$
%%
% where the $\mu_j$ are 'prototypes'
% and K(x,x') is a 'kernel function', which in this context just means a function of two arguments.
% A common example is the Gaussian or RBF kernel
%%
% $$K(x,x') = \exp(-\frac{||x-x'||^2}{2\sigma^2})$$
%%
% where $\sigma$ is the 'bandwidth'.
% Another common example is the polynomial kerne
%%
% $$K(x,x') = (1+x^T x')^d$$
%%
% where d is the degree.
% Often we take the prototypes to be the training vectors, but we don't have to.
%
% Below we show an example where we fit the XOR data using kernelized
% logistic regression, with various kernels and prototypes. 
%%
clear all; close all
[X, y] = createXORdata();
rbfScale = 1;
polydeg  = 2;
protoTypes = [1 1; 1 5; 5 1; 5 5];
kernels = {@(X1, X2)kernelRbfSigma(X1, X2, rbfScale)
           @(X1, X2)kernelRbfSigma(X1, protoTypes, rbfScale)
           @(X1, X2)kernelPoly(X1, X2, polydeg)};
titles  = {'rbf', 'rbf prototypes', 'poly'};
for i=1:numel(kernels)
    preproc.kernelFn = kernels{i};
    model = logregFit(X, y, 'preproc', preproc);
    yhat = logregPredict(model, X);
    errorRate = mean(yhat ~= y);
    fprintf('Error rate using %s features: %2.f%%\n', titles{i}, 100*errorRate);
    predictFcn = @(Xtest)logregPredict(model, Xtest);
    plotDecisionBoundary(X, y, predictFcn);
    if i==2
       hold on; 
       plot(protoTypes(:, 1), protoTypes(:, 2), '*k', 'linewidth', 2, 'markersize', 10)
    end
    title(titles{i});
end

% We discuss how to choose the parameters of the kernel functions below.
%}
%% Overfitting, regularization and MAP estimation
% Using maximum likelihood to train a model  often results in overfitting.
% This means that the model fits the training set well, but is overly complex
% and consequently performs poorly on test data. This is easiest to
% illustrate in the context of polynomial regression in 1d, as shown below (based on
% |linregPolyVsReg|)
%%
[xtrain, ytrain, xtest, ytestNoisefree, ytest] = polyDataMake('sampling','thibaux');
deg = 14;
Xtrain = xtrain; Xtest = xtest;
pp = preprocessorCreate('rescaleX', true, 'poly', deg);
model = linregFit(Xtrain, ytrain, 'preproc', pp);
[ypredTest] = linregPredict(model, Xtest);
figure;
scatter(xtrain, ytrain,'b','filled'); hold on;
plot(xtest, ypredTest, 'k', 'linewidth', 3);

%%
% We see that the fitted function is very 'wiggly', and fits the noise.
% This is common when we have very little data compared to the number of parameters
% (in this example, we have 
% N=21 data points, and 15 parameters, since we fit a degree 14 polynomial).
%
% Using Bayesian inference with an uninformative prior does not help, since
% the mean of the posterior predictive distribution can be obtained by
% plugging in the posterior mean parameter, which is equal to the MLE:
%%
% $$ E[y|x,D] = E[ E[y|x,w]| D] = E[ x^T  w | D] = x^T E[w|D]$$
%%
%
% What we need is an informative prior, that encodes our preference for
% simpler models. A popular away to achieve this is to use a zero-mean spherical
% Gaussian prior of the form $p(w) = N(w|0,\alpha^{-1} I)$,
% where $\alpha$ is the precision (strength) of the prior. This says
% that, a priori, we expect the regression weights to be small, which
% means we believe the function is simple/ smooth (not "too wiggly").
% We can compute the posterior of w with this prior using a variety of
% different models/ likelihood functions. But a computationally simpler
% approach is to use MAP estimation (aka regularization), which just computes the posterior
% mode, which is given by
%%
% $$\hat{w} = \arg \max_w \log p(w|D) = \arg \max_w \log p(D|w) + \log p(w)$$
%%
% In the case of a Gaussian likelihood (linear regression) and Gaussian
% prior, we get
%%
% $$\log p(D|w) + \log p(w) = -\frac{\beta}{2} ||Xw - y||^2 - 
%  \frac{\alpha}{2} ||w||^2$$
%%
% where $\beta=1/\sigma^2$ is the precision of the measurement noise.
% If we define $\lambda = \alpha/ \beta$ to be the amount of regularization,
% we can rewrite this as follows:
%%
% $$\hat{w} = \arg \min_w ||Xw - y||^2 + \lambda ||w||^2$$
%%
% We see that this is a least squares problem with an L2 penalty on the
% weight vector (this is known as ridge regression).
% Below we show how to fit this model for several settings of $\lambda$.
% We see that increasing $\lambda$ results in a smoother fit.

%%
lambdas = [0.00001, 0.001];
NL = length(lambdas);
for k=1:NL
    lambda = lambdas(k);
    model = linregFit(Xtrain, ytrain, 'lambda', lambda, 'preproc', pp);
    [ypredTest] = linregPredict(model, Xtest);  
    figure; 
    scatter(xtrain, ytrain,'b','filled');
    hold on;
    plot(xtest, ypredTest, 'k', 'linewidth', 3);
    title(sprintf('lambda %5.3f', log10(lambda)))
end

%%
% If $\lambda$ is too small, the model will overfit (since the function
% is too wiggly), but if it is too big, the model will underfit
% (since the function is too smooth). This is illustrated below, where we
% examine the measn squared error on the training and  test sets as a function
% of $\lambda$. This illustrates the characteristic U-shape on the test
% set.
%%
lambdas = logspace(-10,1.2,9);
NL = length(lambdas);
 testMse = zeros(1,NL); trainMse = zeros(1,NL);
for k=1:NL
    lambda = lambdas(k);
    [model] = linregFit(Xtrain, ytrain, 'lambda', lambda, 'preproc', pp);
    ypredTest = linregPredict(model, Xtest);
    ypredTrain = linregPredict(model, Xtrain);
    testMse(k) = mean((ypredTest - ytest).^2); 
    trainMse(k) = mean((ypredTrain - ytrain).^2);
end
figure; hold on
ndx =  log(lambdas); % 1:length(lambdas);
plot(ndx, trainMse, 'bs:', 'linewidth', 2, 'markersize', 12);
plot(ndx, testMse, 'rx-', 'linewidth', 2, 'markersize', 12);
legend('train mse', 'test mse', 'location', 'northwest')
xlabel('log regularizer')

##### SOURCE END #####
--></body></html>