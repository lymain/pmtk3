
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML is auto-generated from an M-file.
To make changes, update the M-file and republish this document.
      --><title>Tutorial on supervised learning using pmtk3</title><meta name="generator" content="MATLAB 7.9"><meta name="date" content="2010-06-11"><meta name="m-file" content="tutSupervised"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head><body><div class="content"><h1>Tutorial on supervised learning using pmtk3</h1><!--introduction--><p><i>This page was auto-generated by publishing</i> <a href="http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/tutSupervised.m">http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/tutSupervised.m</a>.</p><p>We provide a description of the main models and algorithms supported by pmtk for solving supervised learning problems (i.e., univariate conditional density estimation).</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Models</a></li><li><a href="#4">Methods</a></li><li><a href="#5">Creating a model</a></li><li><a href="#11">Examining the model's parameters</a></li><li><a href="#17">Using a model for prediction</a></li><li><a href="#20">Prediction with linear regression</a></li><li><a href="#23">Prediction with logistic regression</a></li><li><a href="#30">Visualizing the decision boundaires</a></li><li><a href="#32">Preprocessing, basis function expansion and kernels</a></li><li><a href="#40">Overfitting, regularization and MAP estimation</a></li><li><a href="#55">Discriminant analysis</a></li><li><a href="#58">Cross validation</a></li></ul></div><h2>Models<a name="1"></a></h2><p>The following is a list of pmtk models that are designed for supervised learning (in alphabetical order). We have classified the models based on whether they can be used for classification, regression or both; whether they are generative models of the form <img src="tutSupervised_eq35640.png" alt="$p(y,x|\theta)$"> or discriminative models of the form <img src="tutSupervised_eq42833.png" alt="$p(y|x,\theta)$">; and whether they are parametric (so <img src="tutSupervised_eq49435.png" alt="$\theta$"> has fixed size) or non-parametric. Note that, in the classification setting, we assume <img src="tutSupervised_eq58122.png" alt="$y \in \{1,...C\}$">, where C is a relatively small number of classes. Structured-output classifiers based on conditional random fields will be discussed later.</p><p>
<table border=1>
<TR ALIGN=left>
<td> Model </td>
<td> Description</td>
<td> Classif/regr</td>
<td> Gen/Discr</td>
<td> Param/non</td>
<tr>
<td> discrimAnalysis
<td> Discriminant analysis (linear, quadratic, regularized)
<td> Classif
<td> Gen
<td> Param
<tr>
<td> knn
<td> k nearest neighbors
<td> Classif
<td> Gen
<td> Nonparam
<tr>
<td> linreg
<td> Linear regression
<td> Regr
<td> Discrim
<td> Param
<tr>
<td> logreg
<td> Logistic regression
<td> Classif
<td> Discrim
<td> Param
<tr>
<td> mlp
<td> multi-layer perceptron (aka feedforward neural network)
<td> Both
<td> Discrim
<td> Param
<tr>
<td> naiveBayes
<td> Naive Bayes classifier
<td> Classif
<td> Gen
<td> Param
<tr>
<td> svm
<td> Support vector machine
<td> Both
<td> Discrimg
<td> Nonparam
</table>
</p><p>More models may be added in the future.</p><h2>Methods<a name="4"></a></h2><p>Below we describe the main 'methods' that can be applied to these models. Note that these are just Matlab functions, but we will sometimes call them methods since they behave like object-oriented methods.</p><p>Note that <b>not all models support all methods</b>. To find out if a model of type foo supports method bar, just type <tt>help fooBar</tt>. If you get an error, then you know that foo does not implement bar.</p><h2>Creating a model<a name="5"></a></h2><p>To create a model of type 'foo', use one of the following</p><pre>model = fooCreate(...) % manually specify parameters
model = fooFit(X, y, ...) % Compute ML or MAP estimate of params
model = fooFitBayes(X, y, ...) % Compute posterior of params</pre><p>where</p><div><ul><li> '...' refers to optional arguments (see below)</li><li>X  is an N*D design matrix containing the training data,  where N is the number of training cases and D is the number of features.</li><li>y is an N*1 response vector, which can be real-valued (regression),     0/1 or -1/+1 (binary classification), or 1:C (multi-class).</li></ul></div><p>It is straightforward to handle NaNs (missing values) in X when fitting a generative model; however this functionality is not yet supported. NaNs in y correspond to semi-supervised learning, which is not yet supported.</p><p>The resulting model is a Matlab structure, rather than an object. However, we will sometimes call it an object, since it behaves like one.</p><p>In the case of fooCreate and fooFit, the parameters are point estimates. In the case of fooFitBayes, the parameters are represented as distributions; this is often represented parametrically in terms of the hyper-parameters. The details will be explained below when we look at specific model classes.</p><p>As an example, we can fit a linear regression model to the caterpillar data set as follows (using <tt>linregBayesCaterpillar</tt>):</p><pre class="codeinput">clear <span class="string">all</span>
X = loadData(<span class="string">'caterpillar'</span>); <span class="comment">% from http://www.ceremade.dauphine.fr/~xian/BCS/caterpillar</span>
y = log(X(:,11)); <span class="comment">% log number of nests</span>
X = X(:,1:10);
[model] = linregFit(X, y) <span class="comment">% MLE</span>
</pre><pre class="codeoutput">model = 
         w0: -0.7870
          w: [10x1 double]
     lambda: 0
    preproc: [1x1 struct]
     sigma2: 1.1476
</pre><p>Or we can use Bayesian methods</p><pre class="codeinput">[modelB] = linregFitBayes(X, y, <span class="string">'prior'</span>, <span class="string">'uninf'</span>) <span class="comment">% uninformative Jeffreys prior</span>
</pre><pre class="codeoutput">modelB = 
         wN: [11x1 double]
         VN: [11x11 double]
         aN: 11
         bN: 7.5649
    preproc: [1x1 struct]
    addOnes: 1
</pre><h2>Examining the model's parameters<a name="11"></a></h2><p>If you fit by the model by ML/MAP estimation, you can examine the parameters by typing <tt>model.params</tt>, where <tt>params</tt> is the name of the parameter you are interested in (this varies from model to model, as we saw above).</p><p>If you fit the model by Bayesian inference, you can examine summaries of the posterior using</p><pre>post = fooParamBayes(model)</pre><p>This computes the marginals of each parameter, and displays the posterior mean, standard deviation and 95% credible interval as a latex table. A simple way to assess if a coefficient is significantly different from 0 is to see if its 95% CI excludes 0 or not. If it does, we can put a little * next to it. For example, here is the output for the linear regression model</p><pre class="codeinput">linregParamBayes(modelB, <span class="string">'display'</span>, true);
</pre><pre class="codeoutput">coeff &amp; mean &amp; stddev &amp; 95pc CI &amp; sig
w0 &amp; 10.998 &amp; 3.06027 &amp; [4.652, 17.345] &amp; * \\
w1 &amp; -0.004 &amp; 0.00156 &amp; [-0.008, -0.001] &amp; * \\
w2 &amp; -0.054 &amp; 0.02190 &amp; [-0.099, -0.008] &amp; * \\
w3 &amp; 0.068 &amp; 0.09947 &amp; [-0.138, 0.274] &amp;  \\
w4 &amp; -1.294 &amp; 0.56381 &amp; [-2.463, -0.124] &amp; * \\
w5 &amp; 0.232 &amp; 0.10438 &amp; [0.015, 0.448] &amp; * \\
w6 &amp; -0.357 &amp; 1.56646 &amp; [-3.605, 2.892] &amp;  \\
w7 &amp; -0.237 &amp; 1.00601 &amp; [-2.324, 1.849] &amp;  \\
w8 &amp; 0.181 &amp; 0.23672 &amp; [-0.310, 0.672] &amp;  \\
w9 &amp; -1.285 &amp; 0.86485 &amp; [-3.079, 0.508] &amp;  \\
w10 &amp; -0.433 &amp; 0.73487 &amp; [-1.957, 1.091] &amp;  \\

</pre><p>We see that coefficients  0, 1, 2, 4, 5 are "significant" by this measure. (Other methods of testing significance, based on Bayes factors, can also be used, but are a bit more complicated, and one cannot use uninformative priors in such cases.)</p><p>It turns out that in this particular example, the Bayesian analysis is identical to a classical frequentist analysis (because the posterior for linear regression under an uninformative Jeffreys prior is equivalent to the sampling distribution of the MLE). To see this, let us use the <a href="http://www.mathworks.com/products/statistics/">stats toolbox</a> to fit the model and perform a frequentist analysis:</p><pre class="codeinput">[w, wint] = regress(y, [ones(size(X,1),1) X]);
<span class="keyword">for</span> i=1:length(w)
  fprintf(<span class="string">'%5.3f, [%5.3f, %5.3f]\n'</span>, w(i), wint(i,1), wint(i,2));
<span class="keyword">end</span>
fprintf(<span class="string">'\n'</span>);
</pre><pre class="codeoutput">10.998, [4.652, 17.345]
-0.004, [-0.008, -0.001]
-0.054, [-0.099, -0.008]
0.068, [-0.138, 0.274]
-1.294, [-2.463, -0.124]
0.232, [0.015, 0.448]
-0.357, [-3.605, 2.892]
-0.237, [-2.324, 1.849]
0.181, [-0.310, 0.672]
-1.285, [-3.079, 0.508]
-0.433, [-1.957, 1.091]

</pre><p>We see that the MLE is the same as the posterior mean, and the 95% confidence interval is the same as the 95% credible interval. (If you don't have the stats toolbox, you can use <tt>linregFrequentist</tt> instead, which does more or less the same thing.)</p><p>In general, a Bayesian and frequentist analysis may not give the same results. In pmtk, all inference is Bayesian.</p><h2>Using a model for prediction<a name="17"></a></h2><p>In machine learning, we usually care more about prediction than in trying to interpret the fitted parameters (especially since many models of interest are hard to interpret or even strictly unidentifiable).</p><p>Once the model has been created, you can use it to make predictions  as follows</p><pre>[yhat, py] = fooPredict(model, Xtest) % plugin approximation
[yhat, py] = fooPredictBayes(model, Xtest) % posterior predictive</pre><p>Here yhat is an Ntest*1 vector of predicted responses of the same type as ytrain, where Ntest is the number of rows in Xtest. For regression this is the predicted mean, for classification this is the predicted mode (most probable class label). The meaning of py depends on the model, as follows:</p><div><ul><li>For regression, py is an Ntest*1 vector of predicted variances.</li><li>For binary classification, py is an Ntest*1 vector of the probability of being in class 1.</li><li>For multi-class, py is an Ntest*C matrix, where py(i,c) = p(y=c|Xtest(i,:),params)</li></ul></div><p>The difference between <tt>predict</tt> and <tt>predictBayes</tt> is as follows. <tt>predict</tt> computes <img src="tutSupervised_eq19507.png" alt="$p(y|x,\hat{\theta})$">, which "plugs in" a point estimate of the parameters, where <tt>predictBayes</tt> computes <img src="tutSupervised_eq53132.png" alt="$\int p(y|x,\theta) p(\theta) d\theta$"> ; this is called the (posterior) predictive density. In practice, the Bayesian approach results in similar (often identical) values for yhat, but quite different values for py. In particular, the uncertainty is reflected more accurately in the Bayesian approach, as we illustrate below.</p><h2>Prediction with linear regression<a name="20"></a></h2><p>As an example, consider fitting a linear regression model to some 1d data using MLE and Bayesian methods (using <tt>linregPostPredLinearDemo</tt>), and then plotting the predictions on a test set (which is just a grid of point in the interval [-7,7])</p><pre class="codeinput">setSeed(1);
[xtrain, ytrain, xtest] =  polyDataMake(<span class="string">'sampling'</span>, <span class="string">'sparse'</span>, <span class="string">'deg'</span>, 2);

fitMethods = {@(x,y) linregFit(x,y), @(x,y) linregFitBayes(x,y)};
predictMethods = {@(x,y) linregPredict(x,y), @(x,y) linregPredictBayes(x,y)};
names = {<span class="string">'MLE'</span>, <span class="string">'Bayes'</span>};

<span class="keyword">for</span> i=1:length(fitMethods)
  model = fitMethods{i}(xtrain, ytrain);
  [mu, v] = predictMethods{i}(model, xtest);
  figure; hold <span class="string">on</span>
  plot(xtest, mu,  <span class="string">'k-'</span>, <span class="string">'linewidth'</span>, 3, <span class="string">'displayname'</span>, <span class="string">'prediction'</span>);
  plot(xtrain,ytrain,<span class="string">'ro'</span>,<span class="string">'markersize'</span>, 14, <span class="string">'linewidth'</span>, 3, <span class="keyword">...</span>
     <span class="string">'displayname'</span>, <span class="string">'training data'</span>);
  NN = length(xtest);
  ndx = 1:5:NN; <span class="comment">% plot subset of errorbars to reduce clutter</span>
  sigma = sqrt(v);
  legend(<span class="string">'location'</span>, <span class="string">'northwest'</span>);
  errorbar(xtest(ndx), mu(ndx), sigma(ndx));
  title(names{i});
<span class="keyword">end</span>
</pre><img vspace="5" hspace="5" src="tutSupervised_01.png" alt=""> <img vspace="5" hspace="5" src="tutSupervised_02.png" alt=""> <p>We see that the main difference is that in the plugin case, the predicted variance is constant, whereas in the Bayesian case, the predicted variance increases as we move further away from the training data.</p><h2>Prediction with logistic regression<a name="23"></a></h2><p>As another example of this, consider fitting a binary logistic regression model to some SAT scores, where the response is whether the student passed or failed the class. First we compute the MLE and use a plugin approximation for prediction, as is standard practice (from <tt>logregSATdemo</tt>)</p><pre class="codeinput">close <span class="string">all</span>; clear <span class="string">all</span>
stat = loadData(<span class="string">'sat'</span>);  y = stat(:,1); X = stat(:,4);
model = logregFit(X, y);
[yhat, prob] = logregPredict(model, X); <span class="comment">%ok</span>
figure;
plot(X, y, <span class="string">'ko'</span>, <span class="string">'linewidth'</span>, 2, <span class="string">'MarkerSize'</span>, 7, <span class="string">'markerfacecolor'</span>, <span class="string">'k'</span>);
hold <span class="string">on</span>
plot(X, prob, <span class="string">'ro'</span>, <span class="string">'linewidth'</span>, 2,<span class="string">'MarkerSize'</span>, 10)
</pre><img vspace="5" hspace="5" src="tutSupervised_03.png" alt=""> <p>Now we fit the model using Bayesian inference with an noninformative Gaussian prior. By default, the fitting procedure uses a Laplace approximation to the posterior. To approximate the predictive density, we can plugin in the posterior mean:</p><p><img src="tutSupervised_eq03370.png" alt="$$p(y=1|x,D) = \int \sigma(w^T * x) N(w|\mu,\Sigma) dw&#xA;\approx \sigma(w^T \mu)$$"></p><p>However, this gives essentially the same result as plugging in the MLE. To get a measure of confidence in this prediction, we can sample values of w from their (approximate) posterior, use each such sample to make a prediction, and then compute empirical quantiles of this distribution to get a 95% credible interval. This is done using <tt>logregPredictBayes</tt> and gives the results shown below (from <tt>logregSATdemoBayes</tt>)</p><pre class="codeinput">model = logregFitBayes(X, y);
[yhat, prob, pCI] = logregPredictBayes(model, X);
figure;
plot(X, y, <span class="string">'ko'</span>, <span class="string">'linewidth'</span>, 2, <span class="string">'MarkerSize'</span>, 7, <span class="string">'markerfacecolor'</span>, <span class="string">'k'</span>);
hold <span class="string">on</span>
plot(X, prob, <span class="string">'ro'</span>, <span class="string">'linewidth'</span>, 2,<span class="string">'MarkerSize'</span>, 10)
<span class="keyword">for</span> i=1:size(X,1)
  line([X(i,1) X(i,1)], [pCI(i,1) pCI(i,3)]);
<span class="keyword">end</span>
</pre><img vspace="5" hspace="5" src="tutSupervised_04.png" alt=""> <p>Not all models support Bayesian fitting or prediction (the list will hopefully grow over time, as people add new methods). However, all models provide some measure of confidence on their outputs.</p><h2>Visualizing the decision boundaires<a name="30"></a></h2><p>When comparing classification methods, it is useful to apply them to 2d datasets and to plot the regions of space that get mapped to each class; these are called decision regions, and the boundaries are called decision boundaries. We can do this using the <tt>plotDecisionBoundary(X, y, predFn)</tt> function, where predFn(X) takes a test matrix and computes the MAP estimate of the labels for each row. As an example of this, consider the famous XOR dataset. Let us try fitting a logistic regression model to it in the original feature space (from <tt>logregXorLinearDemo</tt>)</p><pre class="codeinput">close <span class="string">all</span>; clear <span class="string">all</span>;
[X, y] = createXORdata();
model = logregFit(X, y);
plotDecisionBoundary(X, y, @(X)logregPredict(model, X));
yhat = logregPredict(model, X);
errorRate = mean(yhat ~= y)

<span class="comment">% We see that the method performs at chance level, because the data is not</span>
<span class="comment">% linearly separable. We give a simple fix to this problem below, when we</span>
<span class="comment">% discuss basis function expansion.</span>
</pre><pre class="codeoutput">errorRate =
    0.4875
</pre><img vspace="5" hspace="5" src="tutSupervised_05.png" alt=""> <h2>Preprocessing, basis function expansion and kernels<a name="32"></a></h2><p>We are free to preprocess the data in any way we choose before fitting the model. In pmtk, you can create a preprocessor (pp) 'object', and then pass it to the fitting function; the pp will  be applied to the training data before fitting the model, and will be applied again to the test data. The advantage of this approach is that the pp is stored inside the model, which makes sense, since it is an integral part of the model.</p><p>One common form of preprocessing is basis function expansion. This replaces the original features with a larger set, thus permitting us to fit nonlinear models. A popular approach is to use kernel functions, and to define the new feature vector as follows:</p><p><img src="tutSupervised_eq24062.png" alt="$$\phi(x) = (K(x,\mu_1), ..., K(x,mu_D))$$"></p><p>where the <img src="tutSupervised_eq14632.png" alt="$\mu_j$"> are 'prototypes' and K(x,x') is a 'kernel function', which in this context just means a function of two arguments. A common example is the Gaussian or RBF kernel</p><p><img src="tutSupervised_eq21253.png" alt="$$K(x,x') = \exp(-\frac{||x-x'||^2}{2\sigma^2})$$"></p><p>where <img src="tutSupervised_eq24873.png" alt="$\sigma$"> is the 'bandwidth'. Another common example is the polynomial kerne</p><p><img src="tutSupervised_eq60368.png" alt="$$K(x,x') = (1+x^T x')^d$$"></p><p>where d is the degree. Often we take the prototypes to be the training vectors, but we don't have to.</p><p>Below we show an example where we fit the XOR data using kernelized logistic regression, with various kernels and prototypes.</p><pre class="codeinput">clear <span class="string">all</span>; close <span class="string">all</span>
[X, y] = createXORdata();
rbfScale = 1;
polydeg  = 2;
protoTypes = [1 1; 1 5; 5 1; 5 5];
kernels = {@(X1, X2)kernelRbfSigma(X1, X2, rbfScale)
           @(X1, X2)kernelRbfSigma(X1, protoTypes, rbfScale)
           @(X1, X2)kernelPoly(X1, X2, polydeg)};
titles  = {<span class="string">'rbf'</span>, <span class="string">'rbf prototypes'</span>, <span class="string">'poly'</span>};
<span class="keyword">for</span> i=1:numel(kernels)
    preproc = preprocessorCreate(<span class="string">'kernelFn'</span>, kernels{i});
    model = logregFit(X, y, <span class="string">'preproc'</span>, preproc);
    yhat = logregPredict(model, X);
    errorRate = mean(yhat ~= y);
    fprintf(<span class="string">'Error rate using %s features: %2.f%%\n'</span>, titles{i}, 100*errorRate);
    predictFcn = @(Xtest)logregPredict(model, Xtest);
    plotDecisionBoundary(X, y, predictFcn);
    <span class="keyword">if</span> i==2
       hold <span class="string">on</span>;
       plot(protoTypes(:, 1), protoTypes(:, 2), <span class="string">'*k'</span>, <span class="string">'linewidth'</span>, 2, <span class="string">'markersize'</span>, 10)
    <span class="keyword">end</span>
    title(titles{i});
<span class="keyword">end</span>

<span class="comment">% We discuss how to choose the parameters of the kernel functions below.</span>
</pre><pre class="codeoutput">Error rate using rbf features:  0%
Error rate using rbf prototypes features:  0%
Error rate using poly features:  0%
</pre><img vspace="5" hspace="5" src="tutSupervised_06.png" alt=""> <img vspace="5" hspace="5" src="tutSupervised_07.png" alt=""> <img vspace="5" hspace="5" src="tutSupervised_08.png" alt=""> <h2>Overfitting, regularization and MAP estimation<a name="40"></a></h2><p>Using maximum likelihood to train a model  often results in overfitting. This means that the model fits the training set well, but is overly complex and consequently performs poorly on test data. This is easiest to illustrate in the context of polynomial regression in 1d, as shown below (based on <tt>linregPolyVsReg</tt>)</p><pre class="codeinput">[xtrain, ytrain, xtest, ytestNoisefree, ytest] = polyDataMake(<span class="string">'sampling'</span>,<span class="string">'thibaux'</span>);
deg = 14;
Xtrain = xtrain; Xtest = xtest;
pp = preprocessorCreate(<span class="string">'rescaleX'</span>, true, <span class="string">'poly'</span>, deg);
model = linregFit(Xtrain, ytrain, <span class="string">'preproc'</span>, pp);
[ypredTest] = linregPredict(model, Xtest);
figure;
scatter(xtrain, ytrain,<span class="string">'b'</span>,<span class="string">'filled'</span>); hold <span class="string">on</span>;
plot(xtest, ypredTest, <span class="string">'k'</span>, <span class="string">'linewidth'</span>, 3);
</pre><img vspace="5" hspace="5" src="tutSupervised_09.png" alt=""> <p>We see that the fitted function is very 'wiggly', and fits the noise. This is common when we have very little data compared to the number of parameters (in this example, we have N=21 data points, and 15 parameters, since we fit a degree 14 polynomial).</p><p>Using Bayesian inference with an uninformative prior does not help, since the mean of the posterior predictive distribution can be obtained by plugging in the posterior mean parameter, which is equal to the MLE:</p><p><img src="tutSupervised_eq94705.png" alt="$$ E[y|x,D] = E[ E[y|x,w]| D] = E[ x^T  w | D] = x^T E[w|D]$$"></p><p>What we need is an informative prior, that encodes our preference for simpler models. A popular away to achieve this is to use a zero-mean spherical Gaussian prior of the form <img src="tutSupervised_eq54469.png" alt="$p(w) = N(w|0,\alpha^{-1} I)$">, where <img src="tutSupervised_eq87919.png" alt="$\alpha$"> is the precision (strength) of the prior. This says that, a priori, we expect the regression weights to be small, which means we believe the function is simple/ smooth (not "too wiggly"). We can compute the posterior of w with this prior using a variety of different models/ likelihood functions. But a computationally simpler approach is to use MAP estimation (aka regularization), which just computes the posterior mode, which is given by</p><p><img src="tutSupervised_eq93496.png" alt="$$\hat{w} = \arg \max_w \log p(w|D) = \arg \max_w \log p(D|w) + \log p(w)$$"></p><p>In the case of a Gaussian likelihood (linear regression) and Gaussian prior, we get</p><p><img src="tutSupervised_eq66205.png" alt="$$\log p(D|w) + \log p(w) = -\frac{\beta}{2} ||Xw - y||^2 -&#xA; \frac{\alpha}{2} ||w||^2 + \mbox{const}$$"></p><p>where <img src="tutSupervised_eq35555.png" alt="$\beta=1/\sigma^2$"> is the precision of the measurement noise. If we define <img src="tutSupervised_eq65989.png" alt="$\lambda = \alpha/ \beta$"> to be the amount of regularization, we can rewrite this as follows:</p><p><img src="tutSupervised_eq19845.png" alt="$$\hat{w} = \arg \min_w ||Xw - y||^2 + \lambda ||w||^2$$"></p><p>We see that this is a least squares problem with an L2 penalty on the weight vector (this is known as ridge regression). Below we show how to fit this model for several settings of <img src="tutSupervised_eq23351.png" alt="$\lambda$">. We see that increasing <img src="tutSupervised_eq23351.png" alt="$\lambda$"> results in a smoother fit.</p><pre class="codeinput">lambdas = [0.00001, 0.001];
NL = length(lambdas);
<span class="keyword">for</span> k=1:NL
    lambda = lambdas(k);
    model = linregFit(Xtrain, ytrain, <span class="string">'lambda'</span>, lambda, <span class="string">'preproc'</span>, pp, <span class="string">'regtype'</span>, <span class="string">'L2'</span>);
    [ypredTest] = linregPredict(model, Xtest);
    figure;
    scatter(xtrain, ytrain,<span class="string">'b'</span>,<span class="string">'filled'</span>);
    hold <span class="string">on</span>;
    plot(xtest, ypredTest, <span class="string">'k'</span>, <span class="string">'linewidth'</span>, 3);
    title(sprintf(<span class="string">'lambda %5.3f'</span>, log10(lambda)))
<span class="keyword">end</span>
</pre><img vspace="5" hspace="5" src="tutSupervised_10.png" alt=""> <img vspace="5" hspace="5" src="tutSupervised_11.png" alt=""> <p>If <img src="tutSupervised_eq23351.png" alt="$\lambda$"> is too small, the model will overfit (since the function is too wiggly), but if it is too big, the model will underfit (since the function is too smooth). This is illustrated below, where we examine the mean squared error on the training and  test sets as a function of <img src="tutSupervised_eq23351.png" alt="$\lambda$">. This illustrates the characteristic U-shape on the test set.</p><pre class="codeinput">lambdas = logspace(-10,1.2,9);
NL = length(lambdas);
 testMse = zeros(1,NL); trainMse = zeros(1,NL);
<span class="keyword">for</span> k=1:NL
    lambda = lambdas(k);
    [model] = linregFit(Xtrain, ytrain, <span class="string">'lambda'</span>, lambda, <span class="string">'preproc'</span>, pp);
    ypredTest = linregPredict(model, Xtest);
    ypredTrain = linregPredict(model, Xtrain);
    testMse(k) = mean((ypredTest - ytest).^2);
    trainMse(k) = mean((ypredTrain - ytrain).^2);
<span class="keyword">end</span>
figure; hold <span class="string">on</span>
ndx =  log(lambdas); <span class="comment">% 1:length(lambdas);</span>
plot(ndx, trainMse, <span class="string">'bs:'</span>, <span class="string">'linewidth'</span>, 2, <span class="string">'markersize'</span>, 12);
plot(ndx, testMse, <span class="string">'rx-'</span>, <span class="string">'linewidth'</span>, 2, <span class="string">'markersize'</span>, 12);
legend(<span class="string">'train mse'</span>, <span class="string">'test mse'</span>, <span class="string">'location'</span>, <span class="string">'northwest'</span>)
xlabel(<span class="string">'log regularizer'</span>)
</pre><img vspace="5" hspace="5" src="tutSupervised_12.png" alt=""> <p>We can apply L2 regularization to logistic regression and neural networks just as easily.</p><h2>Discriminant analysis<a name="55"></a></h2><p>For generative models such as naive Bayes and discriminant analysis, Gaussian priors (corresponding to L2 regularization) are not appropriate (since the parameters do not live in <img src="tutSupervised_eq36024.png" alt="$R^D$">); however, analogous regularization techniques can be devised (and are recommended).</p><p>For example, when fitting a discriminant analysis model using a shared full covariance, we will encounter numerical problems when N &lt; D. However, we can use a Wishart prior to compute a MAP estimate of <img src="tutSupervised_eq53484.png" alt="$\Sigma$">. This is called regularized discriminant analysis, and can be fit using <tt>discrimAnalysisFit(X, y, 'rda', lambda)</tt>, where <tt>lambda</tt> controls the amount of regularization. See <a href="http://pmtk3.googlecode.com/svn/trunk/docs/demoOutput/Generative_models_for_classification_and_regression/cancerHighDimClassifDemo.html">cancerHighDimClassifDemo</a> for an example.</p><p>Another example is discriminant analysis with a shared diagonal covariance (a special case of naive Bayes). In this case, there are O(D) parameters for the covariance, but O(C D) for the mean. To prevent overfitting, we can shrink the class-conditional means towards the overall mean; this technique is called nearest shrunken centroids. We can fit this model using <tt>discrimAnalysisFit(X, y, 'shrunkenCentroids', lambda)</tt>. We given an example of this below (from <a href="http://pmtk3.googlecode.com/svn/trunk/docs/demoOutput/Generative_models_for_classification_and_regression/shrunkenCentroidsSRBCTdemo.html">shrunkenCentroidsSRBCTdemo</a>), where we apply the method to the SRBCT gene microarray dataset, which has N=144 training examples, D=16063 features and C=4 classes.</p><pre class="codeinput">loadData(<span class="string">'srbct'</span>);

Xtest = Xtest(~isnan(ytest), :);
ytest = ytest(~isnan(ytest));

fitFn = @(X,y,lam)  discrimAnalysisFit(X, y, <span class="string">'shrunkenCentroids'</span>, lam);
predictFn = @(model, X)  discrimAnalysisPredict(model, X);

lambdas = linspace(0, 8, 20);
nTrain = length(ytrain);
nTest = length(ytest);
<span class="keyword">for</span> i=1:length(lambdas)
    model = fitFn(Xtrain, ytrain, lambdas(i));
    yhatTrain = predictFn(model, Xtrain);
    yhatTest = predictFn(model, Xtest);
    errTrain(i) = sum(zeroOneLossFn(yhatTrain, ytrain))/nTrain;
    errTest(i) = sum(zeroOneLossFn(yhatTest, ytest))/nTest;
    numgenes(i) = sum(model.shrunkenCentroids(:) ~= 0);
<span class="keyword">end</span>

figure;
plot(Deltas, errTrain, <span class="string">'gx-'</span>, lambdas, errTest, <span class="string">'bo--'</span>,<span class="keyword">...</span>
  <span class="string">'MarkerSize'</span>, 10, <span class="string">'linewidth'</span>, 2)
legend(<span class="string">'Training'</span>, <span class="string">'Test'</span>, <span class="string">'Location'</span>, <span class="string">'northwest'</span>);
xlabel(<span class="string">'Amount of shrinkage'</span>)
ylabel(<span class="string">'misclassification rate'</span>)
title(<span class="string">'SRBCT data'</span>)
</pre><pre class="codeoutput">Undefined function or variable 'Deltas'.

Error in ==&gt; tutSupervised at 523
plot(Deltas, errTrain, 'gx-', lambdas, errTest, 'bo--',...
</pre><h2>Cross validation<a name="58"></a></h2><p class="footer"><br>
      Published with MATLAB&reg; 7.9<br></p></div><!--
##### SOURCE BEGIN #####
%% Tutorial on supervised learning using pmtk3
% _This page was auto-generated by publishing_
% <http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/tutSupervised.m>.
%
% We provide a description of the main models and algorithms supported by
% pmtk for solving supervised learning problems (i.e., univariate
% conditional density estimation).
%

%% Models
% The following is a list of pmtk models that are designed for 
% supervised learning (in alphabetical order).
% We have classified the models based on whether they can be used for
% classification, regression or both; whether they are generative models of
% the form $p(y,x|\theta)$ or discriminative models of the form $p(y|x,\theta)$;
% and whether they are parametric (so $\theta$ has fixed size) or non-parametric.
% Note that, in the classification setting, we assume $y \in \{1,...C\}$,
% where C is a relatively small number of classes. 
% Structured-output classifiers based on conditional random fields will be
% discussed later.
%
%%
% <html>
% <table border=1>
% <TR ALIGN=left>
% <td> Model </td>
% <td> Description</td>
% <td> Classif/regr</td>
% <td> Gen/Discr</td>
% <td> Param/non</td>
% <tr>
% <td> discrimAnalysis
% <td> Discriminant analysis (linear, quadratic, regularized) 
% <td> Classif
% <td> Gen
% <td> Param
% <tr>
% <td> knn
% <td> k nearest neighbors
% <td> Classif
% <td> Gen
% <td> Nonparam
% <tr>
% <td> linreg
% <td> Linear regression
% <td> Regr
% <td> Discrim
% <td> Param
% <tr>
% <td> logreg
% <td> Logistic regression 
% <td> Classif
% <td> Discrim
% <td> Param
% <tr>
% <td> mlp
% <td> multi-layer perceptron (aka feedforward neural network)
% <td> Both
% <td> Discrim
% <td> Param
% <tr>
% <td> naiveBayes
% <td> Naive Bayes classifier
% <td> Classif
% <td> Gen
% <td> Param
% <tr> 
% <td> svm
% <td> Support vector machine
% <td> Both
% <td> Discrimg
% <td> Nonparam
% </table>
% </html>
%%
% More models may be added in the future.
%
%% Methods
% Below we describe the main 'methods' that can be applied to these models.
% Note that these are just Matlab functions, but we will sometimes call
% them methods since they behave like object-oriented methods.
%
% Note that *not all models support all methods*.
% To find out if a model of type foo supports method bar,
% just type |help fooBar|. If you get an error, then you know
% that foo does not implement bar.
%
%% Creating a model
% To create a model of type 'foo', use one of the following
%%
%  model = fooCreate(...) % manually specify parameters
% model = fooFit(X, y, ...) % Compute ML or MAP estimate of params
% model = fooFitBayes(X, y, ...) % Compute posterior of params
%%
% where
%
% *  '...' refers to optional arguments (see below)
% * X  is an N*D design matrix containing the training data,
%  where N is the number of training cases and D is the number of features.
% * y is an N*1 response vector, which can be real-valued (regression),
%     0/1 or -1/+1 (binary classification), or 1:C (multi-class).
%
% It is straightforward to handle NaNs (missing values) in X when fitting a
% generative model; however this functionality is not yet supported.
% NaNs in y correspond to semi-supervised learning, which
% is not yet supported.
%
% The resulting model is a Matlab structure, rather than an object.
% However, we will sometimes call it an object, since it behaves like one.
%
% In the case of fooCreate and fooFit, the parameters are point estimates.
% In the case of fooFitBayes, the parameters are represented as
% distributions; this is often represented parametrically in terms
% of the hyper-parameters. The details will be explained below
% when we look at specific model classes.
%
%
% As an example, we can fit a linear regression model to the caterpillar
% data set as follows (using |linregBayesCaterpillar|):
%%
clear all
X = loadData('caterpillar'); % from http://www.ceremade.dauphine.fr/~xian/BCS/caterpillar
y = log(X(:,11)); % log number of nests
X = X(:,1:10);
[model] = linregFit(X, y) % MLE
%%
% Or we can use Bayesian methods
[modelB] = linregFitBayes(X, y, 'prior', 'uninf') % uninformative Jeffreys prior
%%
%% Examining the model's parameters
% If you fit by the model by ML/MAP estimation, you can examine the
% parameters by typing |model.params|, where |params| is the name of the
% parameter you are interested in (this varies from model to model,
% as we saw above).
%
% If you fit the model by Bayesian inference, you can examine
% summaries of the posterior using
%%
%  post = fooParamBayes(model)
%
% This computes the marginals of each parameter, and displays
% the posterior mean, standard deviation and 95% credible interval
% as a latex table.
% A simple way to assess if a coefficient is significantly different from 0
% is to see if its 95% CI excludes 0 or not. If it does, we can put
% a little * next to it.
% For example, here is the output for the linear regression model
%%
linregParamBayes(modelB, 'display', true);
%%
% We see that coefficients  0, 1, 2, 4, 5 are "significant"
% by this measure. (Other methods of testing significance, based on Bayes factors, can also be
% used, but are a bit more complicated, and one cannot use uninformative
% priors in such cases.)
%
% It turns out that in this particular example, the Bayesian analysis is
% identical to a classical frequentist analysis (because the posterior
% for linear regression under an uninformative Jeffreys prior is equivalent
% to the sampling distribution of the MLE). To see this, let us use the
% <http://www.mathworks.com/products/statistics/ stats toolbox> to fit the model and perform a frequentist analysis:
%%
[w, wint] = regress(y, [ones(size(X,1),1) X]);
for i=1:length(w)
  fprintf('%5.3f, [%5.3f, %5.3f]\n', w(i), wint(i,1), wint(i,2));
end
fprintf('\n');
%%
% We see that the MLE is the same as the posterior mean,
% and the 95% confidence interval is the same as the 95% credible interval.
% (If you don't have the stats toolbox, you can use |linregFrequentist|
% instead, which does more or less the same thing.)
%
% In general, a Bayesian and frequentist analysis may not give the same
% results. In pmtk, all inference is Bayesian.

%% Using a model for prediction
% In machine learning, we usually care more about prediction than in trying
% to interpret the fitted parameters (especially since many models of
% interest are hard to interpret or even strictly unidentifiable).
%
% Once the model has been created, you can use it to make predictions
%  as follows
%
%%
%  [yhat, py] = fooPredict(model, Xtest) % plugin approximation
%  [yhat, py] = fooPredictBayes(model, Xtest) % posterior predictive
%%
% Here yhat is an Ntest*1 vector of predicted responses of the same type
% as ytrain, where Ntest is the number of rows in Xtest.
% For regression this is the predicted mean, for classification this is the
% predicted mode (most probable class label).
% The meaning of py depends on the model, as follows:
%   
% * For regression, py is an Ntest*1 vector of predicted variances.
% * For binary classification, py is an Ntest*1 vector of the probability of being in class 1.
% * For multi-class, py is an Ntest*C matrix, where py(i,c) = p(y=c|Xtest(i,:),params)
%
% The difference between |predict| and |predictBayes| is as follows.
% |predict| computes $p(y|x,\hat{\theta})$, which "plugs in" a point estimate
% of the parameters, where |predictBayes| computes $\int p(y|x,\theta)
% p(\theta) d\theta$ ; this is called the (posterior) predictive density.
% In practice, the Bayesian approach results in similar (often identical)
% values for yhat, but quite different values for py. In particular, the
% uncertainty is reflected more accurately in the Bayesian approach, as we
% illustrate below.

%% Prediction with linear regression
%
% As an example, consider fitting a linear regression model to some 1d data
% using MLE and Bayesian methods (using |linregPostPredLinearDemo|),
% and then plotting the predictions on a test set (which is just a grid of
% point in the interval [-7,7])
%%
setSeed(1);
[xtrain, ytrain, xtest] =  polyDataMake('sampling', 'sparse', 'deg', 2);

fitMethods = {@(x,y) linregFit(x,y), @(x,y) linregFitBayes(x,y)};
predictMethods = {@(x,y) linregPredict(x,y), @(x,y) linregPredictBayes(x,y)};
names = {'MLE', 'Bayes'};

for i=1:length(fitMethods)
  model = fitMethods{i}(xtrain, ytrain);
  [mu, v] = predictMethods{i}(model, xtest);
  figure; hold on
  plot(xtest, mu,  'k-', 'linewidth', 3, 'displayname', 'prediction');
  plot(xtrain,ytrain,'ro','markersize', 14, 'linewidth', 3, ...
     'displayname', 'training data');
  NN = length(xtest);
  ndx = 1:5:NN; % plot subset of errorbars to reduce clutter
  sigma = sqrt(v);
  legend('location', 'northwest');
  errorbar(xtest(ndx), mu(ndx), sigma(ndx));
  title(names{i});
end
%%
% We see that the main difference is that in the plugin case, the predicted
% variance is constant, whereas in the Bayesian case, the predicted
% variance increases as we move further away from the training data.
%


%% Prediction with logistic regression
% As another example of this, consider fitting a binary logistic regression
% model to some SAT scores, where the response is whether the student
% passed or failed the class. First we compute the MLE and use a plugin
% approximation for prediction, as is standard practice (from |logregSATdemo|)
%
%%
close all; clear all
stat = loadData('sat');  y = stat(:,1); X = stat(:,4);
model = logregFit(X, y);
[yhat, prob] = logregPredict(model, X); %ok
figure;
plot(X, y, 'ko', 'linewidth', 2, 'MarkerSize', 7, 'markerfacecolor', 'k');
hold on
plot(X, prob, 'ro', 'linewidth', 2,'MarkerSize', 10)

%%
% Now we fit the model using Bayesian inference with an noninformative
% Gaussian prior. By default, the fitting procedure uses a Laplace
% approximation to the posterior. To approximate the predictive density, we
% can plugin in the posterior mean:
%%
% $$p(y=1|x,D) = \int \sigma(w^T * x) N(w|\mu,\Sigma) dw 
% \approx \sigma(w^T \mu)$$
%%
% However, this gives essentially the same result as plugging in the MLE.
% To get a measure of confidence in this prediction, we can sample values
% of w from their (approximate) posterior, use each such sample to make a
% prediction, and then compute empirical quantiles of this distribution to
% get a 95% credible interval.
% This is done using |logregPredictBayes| and gives the results shown below
% (from |logregSATdemoBayes|)

%%
model = logregFitBayes(X, y);
[yhat, prob, pCI] = logregPredictBayes(model, X);
figure;
plot(X, y, 'ko', 'linewidth', 2, 'MarkerSize', 7, 'markerfacecolor', 'k');
hold on
plot(X, prob, 'ro', 'linewidth', 2,'MarkerSize', 10)
for i=1:size(X,1)
  line([X(i,1) X(i,1)], [pCI(i,1) pCI(i,3)]);
end

%%
%
% Not all models support Bayesian fitting or prediction (the list will hopefully grow
% over time, as people add new methods).
% However, all models provide some measure of confidence on their outputs.
%

%% Visualizing the decision boundaires
% When comparing classification methods, it is useful to apply them to 2d
% datasets and to plot the regions of space that get mapped to each class;
% these are called decision regions, and the boundaries are called decision
% boundaries. We can do this using the |plotDecisionBoundary(X, y, predFn)|
% function, where predFn(X) takes a test matrix and computes the MAP
% estimate of the labels for each row.
% As an example of this, consider the famous XOR dataset.
% Let us try fitting a logistic regression model to it in the original
% feature space (from |logregXorLinearDemo|)
%%
close all; clear all;
[X, y] = createXORdata();
model = logregFit(X, y);
plotDecisionBoundary(X, y, @(X)logregPredict(model, X));
yhat = logregPredict(model, X);
errorRate = mean(yhat ~= y)

% We see that the method performs at chance level, because the data is not
% linearly separable. We give a simple fix to this problem below, when we
% discuss basis function expansion.

%% Preprocessing, basis function expansion and kernels
% We are free to preprocess the data in any way we choose before fitting the model.
% In pmtk, you can create a preprocessor (pp) 'object', and then pass it to the fitting function;
% the pp will  be applied to the training data before fitting the model, and will be applied again to the test data.
% The advantage of this approach is that the pp is stored inside the model, which makes sense,
% since it is an integral part of the model.
%
% One common form of preprocessing is basis function expansion.
% This replaces the original features with a larger set, thus permitting us to fit nonlinear models.
% A popular approach is to use kernel functions, and to define the new feature vector as follows:
%%
% $$\phi(x) = (K(x,\mu_1), ..., K(x,mu_D))$$
%%
% where the $\mu_j$ are 'prototypes'
% and K(x,x') is a 'kernel function', which in this context just means a function of two arguments.
% A common example is the Gaussian or RBF kernel
%%
% $$K(x,x') = \exp(-\frac{||x-x'||^2}{2\sigma^2})$$
%%
% where $\sigma$ is the 'bandwidth'.
% Another common example is the polynomial kerne
%%
% $$K(x,x') = (1+x^T x')^d$$
%%
% where d is the degree.
% Often we take the prototypes to be the training vectors, but we don't have to.
%
% Below we show an example where we fit the XOR data using kernelized
% logistic regression, with various kernels and prototypes. 
%%
clear all; close all
[X, y] = createXORdata();
rbfScale = 1;
polydeg  = 2;
protoTypes = [1 1; 1 5; 5 1; 5 5];
kernels = {@(X1, X2)kernelRbfSigma(X1, X2, rbfScale)
           @(X1, X2)kernelRbfSigma(X1, protoTypes, rbfScale)
           @(X1, X2)kernelPoly(X1, X2, polydeg)};
titles  = {'rbf', 'rbf prototypes', 'poly'};
for i=1:numel(kernels)
    preproc = preprocessorCreate('kernelFn', kernels{i});
    model = logregFit(X, y, 'preproc', preproc);
    yhat = logregPredict(model, X);
    errorRate = mean(yhat ~= y);
    fprintf('Error rate using %s features: %2.f%%\n', titles{i}, 100*errorRate);
    predictFcn = @(Xtest)logregPredict(model, Xtest);
    plotDecisionBoundary(X, y, predictFcn);
    if i==2
       hold on; 
       plot(protoTypes(:, 1), protoTypes(:, 2), '*k', 'linewidth', 2, 'markersize', 10)
    end
    title(titles{i});
end

% We discuss how to choose the parameters of the kernel functions below.

%% Overfitting, regularization and MAP estimation
% Using maximum likelihood to train a model  often results in overfitting.
% This means that the model fits the training set well, but is overly complex
% and consequently performs poorly on test data. This is easiest to
% illustrate in the context of polynomial regression in 1d, as shown below (based on
% |linregPolyVsReg|)
%%
[xtrain, ytrain, xtest, ytestNoisefree, ytest] = polyDataMake('sampling','thibaux');
deg = 14;
Xtrain = xtrain; Xtest = xtest;
pp = preprocessorCreate('rescaleX', true, 'poly', deg);
model = linregFit(Xtrain, ytrain, 'preproc', pp);
[ypredTest] = linregPredict(model, Xtest);
figure;
scatter(xtrain, ytrain,'b','filled'); hold on;
plot(xtest, ypredTest, 'k', 'linewidth', 3);

%%
% We see that the fitted function is very 'wiggly', and fits the noise.
% This is common when we have very little data compared to the number of parameters
% (in this example, we have 
% N=21 data points, and 15 parameters, since we fit a degree 14 polynomial).
%
% Using Bayesian inference with an uninformative prior does not help, since
% the mean of the posterior predictive distribution can be obtained by
% plugging in the posterior mean parameter, which is equal to the MLE:
%%
% $$ E[y|x,D] = E[ E[y|x,w]| D] = E[ x^T  w | D] = x^T E[w|D]$$
%%
%
% What we need is an informative prior, that encodes our preference for
% simpler models. A popular away to achieve this is to use a zero-mean spherical
% Gaussian prior of the form $p(w) = N(w|0,\alpha^{-1} I)$,
% where $\alpha$ is the precision (strength) of the prior. This says
% that, a priori, we expect the regression weights to be small, which
% means we believe the function is simple/ smooth (not "too wiggly").
% We can compute the posterior of w with this prior using a variety of
% different models/ likelihood functions. But a computationally simpler
% approach is to use MAP estimation (aka regularization), which just computes the posterior
% mode, which is given by
%%
% $$\hat{w} = \arg \max_w \log p(w|D) = \arg \max_w \log p(D|w) + \log p(w)$$
%%
% In the case of a Gaussian likelihood (linear regression) and Gaussian
% prior, we get
%%
% $$\log p(D|w) + \log p(w) = -\frac{\beta}{2} ||Xw - y||^2 - 
%  \frac{\alpha}{2} ||w||^2 + \mbox{const}$$
%%
% where $\beta=1/\sigma^2$ is the precision of the measurement noise.
% If we define $\lambda = \alpha/ \beta$ to be the amount of regularization,
% we can rewrite this as follows:
%%
% $$\hat{w} = \arg \min_w ||Xw - y||^2 + \lambda ||w||^2$$
%%
% We see that this is a least squares problem with an L2 penalty on the
% weight vector (this is known as ridge regression).
% Below we show how to fit this model for several settings of $\lambda$.
% We see that increasing $\lambda$ results in a smoother fit.

%%
lambdas = [0.00001, 0.001];
NL = length(lambdas);
for k=1:NL
    lambda = lambdas(k);
    model = linregFit(Xtrain, ytrain, 'lambda', lambda, 'preproc', pp, 'regtype', 'L2');
    [ypredTest] = linregPredict(model, Xtest);  
    figure; 
    scatter(xtrain, ytrain,'b','filled');
    hold on;
    plot(xtest, ypredTest, 'k', 'linewidth', 3);
    title(sprintf('lambda %5.3f', log10(lambda)))
end

%%
% If $\lambda$ is too small, the model will overfit (since the function
% is too wiggly), but if it is too big, the model will underfit
% (since the function is too smooth). This is illustrated below, where we
% examine the mean squared error on the training and  test sets as a function
% of $\lambda$. This illustrates the characteristic U-shape on the test
% set.
%%
lambdas = logspace(-10,1.2,9);
NL = length(lambdas);
 testMse = zeros(1,NL); trainMse = zeros(1,NL);
for k=1:NL
    lambda = lambdas(k);
    [model] = linregFit(Xtrain, ytrain, 'lambda', lambda, 'preproc', pp);
    ypredTest = linregPredict(model, Xtest);
    ypredTrain = linregPredict(model, Xtrain);
    testMse(k) = mean((ypredTest - ytest).^2); 
    trainMse(k) = mean((ypredTrain - ytrain).^2);
end
figure; hold on
ndx =  log(lambdas); % 1:length(lambdas);
plot(ndx, trainMse, 'bs:', 'linewidth', 2, 'markersize', 12);
plot(ndx, testMse, 'rx-', 'linewidth', 2, 'markersize', 12);
legend('train mse', 'test mse', 'location', 'northwest')
xlabel('log regularizer')
%%
% We can apply L2 regularization to logistic regression and neural
% networks just as easily.
%
%% Discriminant analysis
% For generative models such as naive Bayes and
% discriminant analysis, Gaussian priors (corresponding to L2 regularization)
% are not appropriate (since the
% parameters do not live in $R^D$); however, analogous regularization
% techniques can be devised (and are recommended).
%
% For example, when fitting a discriminant analysis model using
% a shared full covariance, we will encounter numerical problems
% when N < D. However, we can use a Wishart prior to compute a MAP
% estimate of $\Sigma$. This is called regularized discriminant analysis,
% and can be fit using |discrimAnalysisFit(X, y, 'rda', lambda)|,
% where |lambda| controls the amount of regularization.
% See <http://pmtk3.googlecode.com/svn/trunk/docs/demoOutput/Generative_models_for_classification_and_regression/cancerHighDimClassifDemo.html cancerHighDimClassifDemo>
% for an example.
%
% Another example is discriminant analysis with a shared diagonal
% covariance (a special case of naive Bayes). In this case, 
% there are O(D) parameters for the covariance, but O(C D) for the mean. To
% prevent overfitting, we can shrink the class-conditional means towards
% the overall mean; this technique is called nearest shrunken centroids. We
% can fit this model using |discrimAnalysisFit(X, y, 'shrunkenCentroids',
% lambda)|. We given an example of this below (from
% <http://pmtk3.googlecode.com/svn/trunk/docs/demoOutput/Generative_models_for_classification_and_regression/shrunkenCentroidsSRBCTdemo.html shrunkenCentroidsSRBCTdemo>),
% where we apply the method to
% the SRBCT gene microarray dataset, which has N=144 training examples,
% D=16063 features and C=4 classes.
%%
loadData('srbct');

Xtest = Xtest(~isnan(ytest), :);
ytest = ytest(~isnan(ytest));

fitFn = @(X,y,lam)  discrimAnalysisFit(X, y, 'shrunkenCentroids', lam);
predictFn = @(model, X)  discrimAnalysisPredict(model, X);

lambdas = linspace(0, 8, 20);
nTrain = length(ytrain);
nTest = length(ytest);
for i=1:length(lambdas)
    model = fitFn(Xtrain, ytrain, lambdas(i));
    yhatTrain = predictFn(model, Xtrain);
    yhatTest = predictFn(model, Xtest);
    errTrain(i) = sum(zeroOneLossFn(yhatTrain, ytrain))/nTrain;
    errTest(i) = sum(zeroOneLossFn(yhatTest, ytest))/nTest;
    numgenes(i) = sum(model.shrunkenCentroids(:) ~= 0);
end

figure;
plot(Deltas, errTrain, 'gx-', lambdas, errTest, 'boREPLACE_WITH_DASH_DASH',...
  'MarkerSize', 10, 'linewidth', 2)
legend('Training', 'Test', 'Location', 'northwest');
xlabel('Amount of shrinkage')
ylabel('misclassification rate')
title('SRBCT data')
%%
%% Cross validation


##### SOURCE END #####
--></body></html>