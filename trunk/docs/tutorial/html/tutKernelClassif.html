
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML is auto-generated from an M-file.
To make changes, update the M-file and republish this document.
      --><title>tutKernelClassif</title><meta name="generator" content="MATLAB 7.9"><meta name="date" content="2010-09-07"><meta name="m-file" content="tutKernelClassif"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head><body><div class="content"><h2>Contents</h2><div><ul><li><a href="#1">Supervised learning using non-parametric discriminative models in pmtk3</a></li><li><a href="#2">Sparse multinomial logistic regression (SMLR)</a></li><li><a href="#3">Comparison of SVM, RVM, SMLR, RMLR</a></li></ul></div><h2>Supervised learning using non-parametric discriminative models in pmtk3<a name="1"></a></h2><p><i>This page was auto-generated by publishing</i> <a href="http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/tutKernelClassif.m">tutKernelClassif.m</a></p><pre class="codeinput"><span class="comment">%{
</span><span class="comment">%% Kernel functions
</span><span class="comment">%
</span><span class="comment">% One common form of basis function expansion
</span><span class="comment">% is to define a new feature vector $\phi(x)$ by comparing the input
</span><span class="comment">% $x$ to a set of prototypes or examplars $\mu_k$ as follows:
</span><span class="comment">%%
</span><span class="comment">% $$\phi(x) = (K(x,\mu_1), ..., K(x,\mu_D))$$
</span><span class="comment">%%
</span><span class="comment">% Here $K(x,\mu)$ is a 'kernel function',
</span><span class="comment">% which in this context just means a function of two arguments.
</span><span class="comment">% A common example is the Gaussian or RBF kernel
</span><span class="comment">%%
</span><span class="comment">% $$K(x,\mu) = \exp(-\frac{||x-\mu||^2}{2\sigma^2})$$
</span><span class="comment">%%
</span><span class="comment">% where $\sigma$ is the 'bandwidth'.
</span><span class="comment">% This can be created using &lt;http://pmtk3.googlecode.com/svn/trunk/toolbox/SupervisedModels/svm/sub/kernelRbfSigma.m kernelRbfSigma.m&gt; .
</span><span class="comment">% Alternatively, we can write
</span><span class="comment">%%
</span><span class="comment">% $$K(x,\mu) = \exp(-\gamma ||x-\mu||^2})$$
</span><span class="comment">%%
</span><span class="comment">% The quantity $\gamma=1/\sigma^2$ is known as
</span><span class="comment">% the scale or precision. This can be created using &lt;http://pmtk3.googlecode.com/svn/trunk/toolbox/SupervisedModels/svm/sub/kernelRbfGamma.m kernelRbfGamma.m&gt; .
</span><span class="comment">% Most software packages use this latter parameterization.
</span><span class="comment">%
</span><span class="comment">% Another common example is the polynomial kernel
</span><span class="comment">%%
</span><span class="comment">% $$K(x,\mu) = (1+x^T \mu)^d$$
</span><span class="comment">%%
</span><span class="comment">% where d is the degree.
</span><span class="comment">% Often we take the prototypes $\mu_j$ to be the training vectors (rows of $X$), but we don't have to.
</span><span class="comment">%
</span><span class="comment">% The advantages of using kernels include the following
</span><span class="comment">%
</span><span class="comment">% * We can apply standard parametric models (e.g., linear and logistic
</span><span class="comment">% regression) to non-vectorial inputs (e.g., strings, molecular structures, etc.),
</span><span class="comment">% by defining $K(x,\mu)$ to be some
</span><span class="comment">% kind of function for comparing structured inputs.
</span><span class="comment">% * We can increase the flexibility of the model by working in an
</span><span class="comment">% enlarged feature space.
</span><span class="comment">%
</span><span class="comment">% Below we show an example where we fit the XOR data using kernelized
</span><span class="comment">% logistic regression, with various kernels and prototypes
</span><span class="comment">% (from &lt;http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Introduction/logregXorDemo.m logregXorDemo.m&gt; ).
</span><span class="comment">%%
</span><span class="comment">clear all; close all
</span><span class="comment">[X, y] = createXORdata();
</span><span class="comment">rbfScale = 1;
</span><span class="comment">polydeg  = 2;
</span><span class="comment">protoTypes = [1 1; 1 5; 5 1; 5 5];
</span><span class="comment">protoTypesStnd = standardizeCols(protoTypes);
</span><span class="comment">kernels = {@(X1, X2)kernelRbfSigma(X1, protoTypesStnd, rbfScale)
</span><span class="comment">           @(X1, X2)kernelRbfSigma(X1, X2, rbfScale)
</span><span class="comment">           @(X1, X2)kernelPoly(X1, X2, polydeg)};
</span><span class="comment">titles  = {'rbf', 'rbf prototypes', 'poly'};
</span><span class="comment">for i=1:numel(kernels)
</span><span class="comment">    preproc = preprocessorCreate('kernelFn', kernels{i}, 'standardizeX', true, 'addOnes', true);
</span><span class="comment">    model = logregFit(X, y, 'preproc', preproc);
</span><span class="comment">    yhat = logregPredict(model, X);
</span><span class="comment">    errorRate = mean(yhat ~= y);
</span><span class="comment">    fprintf('Error rate using %s features: %2.f%%\n', titles{i}, 100*errorRate);
</span><span class="comment">    predictFcn = @(Xtest)logregPredict(model, Xtest);
</span><span class="comment">    plotDecisionBoundary(X, y, predictFcn);
</span><span class="comment">    if i==2
</span><span class="comment">       hold on;
</span><span class="comment">       plot(protoTypes(:, 1), protoTypes(:, 2), '*k', 'linewidth', 2, 'markersize', 10)
</span><span class="comment">    end
</span><span class="comment">    title(titles{i});
</span><span class="comment">end
</span><span class="comment">%%
</span><span class="comment">% In the first example, we use an RBF kernel with centers at 4
</span><span class="comment">% manually chosen points, shown with black stars.
</span><span class="comment">% In the second and third examples, we use an RBF and polynomial kernel,
</span><span class="comment">% centered at all the training data.
</span><span class="comment">% This is an example of a non-parametric model,
</span><span class="comment">% that stores all the training data.
</span><span class="comment">% We can use sparsity promoting priors  to select a subset of the training
</span><span class="comment">% data, as we illustrate below.
</span><span class="comment">
</span><span class="comment">
</span><span class="comment">%% Using cross validation to choose the kernel parameters
</span><span class="comment">% We can create a grid of models, with different kernel params
</span><span class="comment">% and different strength regularizers, as shown in the example
</span><span class="comment">% below ( from &lt;http://pmtk3.googlecode.com/svn/trunk/demos/otherDemos/supervisedModels/logregKernelCrabsDemo.m logregKernelCrabsDemo.m&gt; ).
</span><span class="comment">% If CV does not pick a point on the edge of the grid,
</span><span class="comment">% we can be faily confident we have searched over
</span><span class="comment">% a reasonable range. For this reason,
</span><span class="comment">% it is helpful to plot the cost surface.
</span><span class="comment">%%
</span><span class="comment">clear all
</span><span class="comment">loadData('crabs');
</span><span class="comment">% Here we cross validate over both lambda and Sigma
</span><span class="comment">lambda     = logspace(-5, 1, 5);
</span><span class="comment">gamma      = logspace(-5, 5, 5);
</span><span class="comment">paramRange = crossProduct(lambda, gamma);
</span><span class="comment">regtypes = {'L1', 'L2'};
</span><span class="comment">for r=1:length(regtypes)
</span><span class="comment">  regtype = regtypes{r};
</span><span class="comment">  fitFn = @(X, y, param)...
</span><span class="comment">    logregFit(X, y, 'lambda', param(1), 'regType', regtype, 'preproc', ...
</span><span class="comment">    preprocessorCreate('kernelFn', @(X1, X2)kernelRbfGamma(X1, X2, param(2))));
</span><span class="comment">  predictFn = @logregPredict;
</span><span class="comment">  lossFn = @(ytest, yhat)mean(yhat ~= ytest);
</span><span class="comment">  nfolds = 5;
</span><span class="comment">  useSErule = true;
</span><span class="comment">  plotCv = true;
</span><span class="comment">  tic;
</span><span class="comment">  [LRmodel, lambdaStar, LRmu, LRse] = ...
</span><span class="comment">    fitCv(paramRange, fitFn, predictFn, lossFn, Xtrain, ytrain, nfolds, ...
</span><span class="comment">    'useSErule', useSErule, 'doPlot', plotCv, 'params1', lambda, 'params2', Sigma);
</span><span class="comment">  time(r) = toc
</span><span class="comment">  yhat = logregPredict(LRmodel, Xtest);
</span><span class="comment">  nerrors(r) = sum(yhat ~= ytest)
</span><span class="comment">end
</span><span class="comment">%%
</span><span class="comment">% We see that L2 regularization (which results in a dense model)
</span><span class="comment">% is both more accurate and faster to train, in this example
</span><span class="comment">% at least.
</span><span class="comment">
</span><span class="comment">%}</span>
</pre><h2>Sparse multinomial logistic regression (SMLR)<a name="2"></a></h2><p>We can select a subset of the training examples by using an L1 regularizer. This is called Sparse multinomial logistic regression (SMLR). See <a href="http://pmtk3.googlecode.com/svn/trunk/toolbox/SupervisedModels/smlr/smlrFit.m">smlrFit.m</a> and <a href="http://pmtk3.googlecode.com/svn/trunk/toolbox/SupervisedModels/smlr/smlrPredict.m">smlrPredict.m</a> .</p><p>One way to implement <a href="http://pmtk3.googlecode.com/svn/trunk/toolbox/SupervisedModels/smlr/smlrFit.m">smlrFit.m</a> is to kernelize the data, and then pick the best lambda on the regularization path. However, a better way is to call <a href="http://matlabtools.googlecode.com/svn/trunk/stats/fitCv.m">fitCv.m</a> , which let us use a different kernel basis for each fold. See</p><h2>Comparison of SVM, RVM, SMLR, RMLR<a name="3"></a></h2><p>Below we show the characteristics of some data sets on which we will apply various classifiers.</p><p>
<TABLE BORDER=3 CELLPADDING=5 WIDTH="100%" >
<TR ALIGN=left>
<TH BGCOLOR=#00CCFF><FONT COLOR=000000></FONT></TH>
<TH BGCOLOR=#00CCFF><FONT COLOR=000000>nClasses</FONT></TH>
<TH BGCOLOR=#00CCFF><FONT COLOR=000000>nFeatures</FONT></TH>
<TH BGCOLOR=#00CCFF><FONT COLOR=000000>nCases</FONT></TH>
</TR>
<tr>
<td BGCOLOR=#00CCFF><FONT COLOR=000000>soy</FONT>
<td> 3
<td> 35
<td> 307
<tr>
<td BGCOLOR=#00CCFF><FONT COLOR=000000>fglass</FONT>
<td> 6
<td> 9
<td> 214
<tr>
<td BGCOLOR=#00CCFF><FONT COLOR=000000>colon</FONT>
<td> 2
<td> 2000
<td> 62
<tr>
<td BGCOLOR=#00CCFF><FONT COLOR=000000>amlAll</FONT>
<td> 2
<td> 7129
<td> 72
</table>
</p><p>In <a href="http://pmtk3.googlecode.com/svn/trunk/demos/otherDemos/supervisedModels/linearKernelDemo.m">linearKernelDemo.m</a> , we compare various kernelized classifiers (using linear kernels) on the above data. Below we show the median misclassification rates on the different data sets, averaged over 3 random splits. For each split, we use 70% of the data for training and 30% for testing. Cross validation on the training set is then used internally, if necessary, to tune the regularization parameter.</p><p>
<TABLE BORDER=3 CELLPADDING=5 WIDTH="100%" >
<TR><TH COLSPAN=7 ALIGN=center> test error rate (median over 3 trials) </font></TH></TR>
<TR ALIGN=left>
<TH BGCOLOR=#00CCFF><FONT COLOR=000000></FONT></TH>
<TH BGCOLOR=#00CCFF><FONT COLOR=000000>SVM</FONT></TH>
<TH BGCOLOR=#00CCFF><FONT COLOR=000000>RVM</FONT></TH>
<TH BGCOLOR=#00CCFF><FONT COLOR=000000>SMLR</FONT></TH>
<TH BGCOLOR=#00CCFF><FONT COLOR=000000>RMLR</FONT></TH>
<TH BGCOLOR=#00CCFF><FONT COLOR=000000>logregL2</FONT></TH>
<TH BGCOLOR=#00CCFF><FONT COLOR=000000>logregL1</FONT></TH>
</TR>
<tr>
<td BGCOLOR=#00CCFF><FONT COLOR=000000>soy</FONT>
<td> 0.108
<td> 0.108
<td> 0.118
<td> 0.129
<td> 0.710
<td> 0.108
<tr>
<td BGCOLOR=#00CCFF><FONT COLOR=000000>fglass</FONT>
<td> 0.477
<td> 0.554
<td> 0.400
<td> 0.431
<td> 0.708
<td> 0.492
<tr>
<td BGCOLOR=#00CCFF><FONT COLOR=000000>colon</FONT>
<td> 0.211
<td> 0.211
<td> 0.158
<td> 0.211
<td> 0.316
<td> 0.211
<tr>
<td BGCOLOR=#00CCFF><FONT COLOR=000000>amlAll</FONT>
<td> 0.455
<td> 0.227
<td> 0.136
<td> 0.182
<td> 0.364
<td> 0.182
</table>
</p><p>There is considerable variance in performance across the 3 different trials (as you can see for yourself by running <a href="http://pmtk3.googlecode.com/svn/trunk/demos/otherDemos/supervisedModels/linearKernelDemo.m">linearKernelDemo.m</a>  ), so it is difficult to draw reliable conclusions. In terms of median performace, all methods are similar, with the exception of logregL2, which is somewhat worse than the others.</p><p>The error rates for all methods on the forensic glass data are quite high (chance would be 5/6=0.83). This is because we used a linear kernel, but we only have 9 features. Using a nonlinear kernel helps performance a lot on such low-dim datasets. However, on high-dim datasets, linear kernels are usually sufficient. Indeed, we can just as well work in the original feature space and not use kernels at all for the other 3 datasets.</p><p>The error rate for SVM on AML/ALL is very high (chance is 0.5). Other papers have reported much better results for SVMs on this data (with linear kernels). This is possibly due to the way we do cross validation. However, it may be that other authors have chosen particularly favorable train/test splits. We find that, across the 3 folds, the number of errors were 11, 10 and 4 (out of 22).</p><p>It is also interesting to look at the amount of time required to train each model (prediction is very fast with all of these methods). Below we show median time in seconds. We are comparing apples with oranges here, however, since the svm code is in C (libsvm), the rvm code is an optimized greedy algoritnm in Matlab (SparseBayes), the SMLR and RMLR is unoptimized Matlab code, and the L2 and L1 code is Fortran (glmnet).</p><p>
<TABLE BORDER=3 CELLPADDING=5 WIDTH="100%" >
<TR><TH COLSPAN=7 ALIGN=center> training time in seconds (median over 3 trials) </font></TH></TR>
<TR ALIGN=left>
<TH BGCOLOR=#00CCFF><FONT COLOR=000000></FONT></TH>
<TH BGCOLOR=#00CCFF><FONT COLOR=000000>SVM</FONT></TH>
<TH BGCOLOR=#00CCFF><FONT COLOR=000000>RVM</FONT></TH>
<TH BGCOLOR=#00CCFF><FONT COLOR=000000>SMLR</FONT></TH>
<TH BGCOLOR=#00CCFF><FONT COLOR=000000>RMLR</FONT></TH>
<TH BGCOLOR=#00CCFF><FONT COLOR=000000>logregL2</FONT></TH>
<TH BGCOLOR=#00CCFF><FONT COLOR=000000>logregL1</FONT></TH>
</TR>
<tr>
<td BGCOLOR=#00CCFF><FONT COLOR=000000>soy</FONT>
<td> 0.566
<td> 0.549
<td> 43.770
<td> 24.193
<td> 0.024
<td> 0.720
<tr>
<td BGCOLOR=#00CCFF><FONT COLOR=000000>fglass</FONT>
<td> 0.586
<td> 0.146
<td> 67.552
<td> 30.204
<td> 0.043
<td> 0.684
<tr>
<td BGCOLOR=#00CCFF><FONT COLOR=000000>colon</FONT>
<td> 1.251
<td> 0.028
<td> 2.434
<td> 2.618
<td> 0.021
<td> 0.418
<tr>
<td BGCOLOR=#00CCFF><FONT COLOR=000000>amlAll</FONT>
<td> 3.486
<td> 0.017
<td> 2.337
<td> 2.569
<td> 0.097
<td> 1.674
</table>
</p><p>Reproducing</p><p class="footer"><br>
      Published with MATLAB&reg; 7.9<br></p></div><!--
##### SOURCE BEGIN #####
%% Supervised learning using non-parametric discriminative models in pmtk3
% _This page was auto-generated by publishing_
% <http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/tutKernelClassif.m tutKernelClassif.m>
%
%{
%% Kernel functions
%
% One common form of basis function expansion
% is to define a new feature vector $\phi(x)$ by comparing the input
% $x$ to a set of prototypes or examplars $\mu_k$ as follows:
%%
% $$\phi(x) = (K(x,\mu_1), ..., K(x,\mu_D))$$
%%
% Here $K(x,\mu)$ is a 'kernel function',
% which in this context just means a function of two arguments.
% A common example is the Gaussian or RBF kernel
%%
% $$K(x,\mu) = \exp(-\frac{||x-\mu||^2}{2\sigma^2})$$
%%
% where $\sigma$ is the 'bandwidth'.
% This can be created using <http://pmtk3.googlecode.com/svn/trunk/toolbox/SupervisedModels/svm/sub/kernelRbfSigma.m kernelRbfSigma.m> .
% Alternatively, we can write
%%
% $$K(x,\mu) = \exp(-\gamma ||x-\mu||^2})$$
%%
% The quantity $\gamma=1/\sigma^2$ is known as
% the scale or precision. This can be created using <http://pmtk3.googlecode.com/svn/trunk/toolbox/SupervisedModels/svm/sub/kernelRbfGamma.m kernelRbfGamma.m> .
% Most software packages use this latter parameterization.
%
% Another common example is the polynomial kernel
%%
% $$K(x,\mu) = (1+x^T \mu)^d$$
%%
% where d is the degree.
% Often we take the prototypes $\mu_j$ to be the training vectors (rows of $X$), but we don't have to.
%
% The advantages of using kernels include the following
%
% * We can apply standard parametric models (e.g., linear and logistic
% regression) to non-vectorial inputs (e.g., strings, molecular structures, etc.),
% by defining $K(x,\mu)$ to be some
% kind of function for comparing structured inputs.
% * We can increase the flexibility of the model by working in an
% enlarged feature space.
%
% Below we show an example where we fit the XOR data using kernelized
% logistic regression, with various kernels and prototypes
% (from <http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Introduction/logregXorDemo.m logregXorDemo.m> ).
%%
clear all; close all
[X, y] = createXORdata();
rbfScale = 1;
polydeg  = 2;
protoTypes = [1 1; 1 5; 5 1; 5 5];
protoTypesStnd = standardizeCols(protoTypes);
kernels = {@(X1, X2)kernelRbfSigma(X1, protoTypesStnd, rbfScale)
           @(X1, X2)kernelRbfSigma(X1, X2, rbfScale)
           @(X1, X2)kernelPoly(X1, X2, polydeg)};
titles  = {'rbf', 'rbf prototypes', 'poly'};
for i=1:numel(kernels)
    preproc = preprocessorCreate('kernelFn', kernels{i}, 'standardizeX', true, 'addOnes', true);
    model = logregFit(X, y, 'preproc', preproc);
    yhat = logregPredict(model, X);
    errorRate = mean(yhat ~= y);
    fprintf('Error rate using %s features: %2.f%%\n', titles{i}, 100*errorRate);
    predictFcn = @(Xtest)logregPredict(model, Xtest);
    plotDecisionBoundary(X, y, predictFcn);
    if i==2
       hold on; 
       plot(protoTypes(:, 1), protoTypes(:, 2), '*k', 'linewidth', 2, 'markersize', 10)
    end
    title(titles{i});
end
%%
% In the first example, we use an RBF kernel with centers at 4
% manually chosen points, shown with black stars.
% In the second and third examples, we use an RBF and polynomial kernel,
% centered at all the training data.
% This is an example of a non-parametric model,
% that stores all the training data.
% We can use sparsity promoting priors  to select a subset of the training
% data, as we illustrate below.


%% Using cross validation to choose the kernel parameters
% We can create a grid of models, with different kernel params
% and different strength regularizers, as shown in the example
% below ( from <http://pmtk3.googlecode.com/svn/trunk/demos/otherDemos/supervisedModels/logregKernelCrabsDemo.m logregKernelCrabsDemo.m> ).
% If CV does not pick a point on the edge of the grid,
% we can be faily confident we have searched over
% a reasonable range. For this reason,
% it is helpful to plot the cost surface.
%%
clear all
loadData('crabs');
% Here we cross validate over both lambda and Sigma
lambda     = logspace(-5, 1, 5);  
gamma      = logspace(-5, 5, 5);
paramRange = crossProduct(lambda, gamma); 
regtypes = {'L1', 'L2'};
for r=1:length(regtypes)
  regtype = regtypes{r};
  fitFn = @(X, y, param)...
    logregFit(X, y, 'lambda', param(1), 'regType', regtype, 'preproc', ...
    preprocessorCreate('kernelFn', @(X1, X2)kernelRbfGamma(X1, X2, param(2))));
  predictFn = @logregPredict;
  lossFn = @(ytest, yhat)mean(yhat ~= ytest);
  nfolds = 5;
  useSErule = true;
  plotCv = true;
  tic;
  [LRmodel, lambdaStar, LRmu, LRse] = ...
    fitCv(paramRange, fitFn, predictFn, lossFn, Xtrain, ytrain, nfolds, ...
    'useSErule', useSErule, 'doPlot', plotCv, 'params1', lambda, 'params2', Sigma);
  time(r) = toc
  yhat = logregPredict(LRmodel, Xtest);
  nerrors(r) = sum(yhat ~= ytest)
end
%%
% We see that L2 regularization (which results in a dense model)
% is both more accurate and faster to train, in this example
% at least.

%}
%% Sparse multinomial logistic regression (SMLR)
% We can select a subset of the training examples
% by using an L1 regularizer.
% This is called Sparse multinomial logistic regression (SMLR).
% See <http://pmtk3.googlecode.com/svn/trunk/toolbox/SupervisedModels/smlr/smlrFit.m smlrFit.m> and <http://pmtk3.googlecode.com/svn/trunk/toolbox/SupervisedModels/smlr/smlrPredict.m smlrPredict.m> .
%
% One way to implement <http://pmtk3.googlecode.com/svn/trunk/toolbox/SupervisedModels/smlr/smlrFit.m smlrFit.m> is to
% kernelize the data,
% and then pick the best lambda on the regularization path.
% However, a better way is to call <http://matlabtools.googlecode.com/svn/trunk/stats/fitCv.m fitCv.m> , which
% let us use a different kernel basis for each fold.
% See 

%% Comparison of SVM, RVM, SMLR, RMLR
% Below we show the characteristics of some data sets
% on which we will apply various classifiers.
%
%%
% <html>
% <TABLE BORDER=3 CELLPADDING=5 WIDTH="100%" >
% <TR ALIGN=left>
% <TH BGCOLOR=#00CCFF><FONT COLOR=000000></FONT></TH>
% <TH BGCOLOR=#00CCFF><FONT COLOR=000000>nClasses</FONT></TH>
% <TH BGCOLOR=#00CCFF><FONT COLOR=000000>nFeatures</FONT></TH>
% <TH BGCOLOR=#00CCFF><FONT COLOR=000000>nCases</FONT></TH>
% </TR>
% <tr>
% <td BGCOLOR=#00CCFF><FONT COLOR=000000>soy</FONT>
% <td> 3
% <td> 35
% <td> 307
% <tr>
% <td BGCOLOR=#00CCFF><FONT COLOR=000000>fglass</FONT>
% <td> 6
% <td> 9
% <td> 214
% <tr>
% <td BGCOLOR=#00CCFF><FONT COLOR=000000>colon</FONT>
% <td> 2
% <td> 2000
% <td> 62
% <tr>
% <td BGCOLOR=#00CCFF><FONT COLOR=000000>amlAll</FONT>
% <td> 2
% <td> 7129
% <td> 72
% </table>
% </html>
%%
%
% In <http://pmtk3.googlecode.com/svn/trunk/demos/otherDemos/supervisedModels/linearKernelDemo.m linearKernelDemo.m> , we compare various kernelized
% classifiers (using linear kernels) on the above data.
% Below we show the median misclassification rates on the different data sets,
% averaged over 3 random splits.
% For each split, we use 70% of the data for training and 30% for testing.
% Cross validation on the training set is then used internally,
% if necessary, to tune the regularization parameter.
%%
% <html>
% <TABLE BORDER=3 CELLPADDING=5 WIDTH="100%" >
% <TR><TH COLSPAN=7 ALIGN=center> test error rate (median over 3 trials) </font></TH></TR>
% <TR ALIGN=left>
% <TH BGCOLOR=#00CCFF><FONT COLOR=000000></FONT></TH>
% <TH BGCOLOR=#00CCFF><FONT COLOR=000000>SVM</FONT></TH>
% <TH BGCOLOR=#00CCFF><FONT COLOR=000000>RVM</FONT></TH>
% <TH BGCOLOR=#00CCFF><FONT COLOR=000000>SMLR</FONT></TH>
% <TH BGCOLOR=#00CCFF><FONT COLOR=000000>RMLR</FONT></TH>
% <TH BGCOLOR=#00CCFF><FONT COLOR=000000>logregL2</FONT></TH>
% <TH BGCOLOR=#00CCFF><FONT COLOR=000000>logregL1</FONT></TH>
% </TR>
% <tr>
% <td BGCOLOR=#00CCFF><FONT COLOR=000000>soy</FONT>
% <td> 0.108
% <td> 0.108
% <td> 0.118
% <td> 0.129
% <td> 0.710
% <td> 0.108
% <tr>
% <td BGCOLOR=#00CCFF><FONT COLOR=000000>fglass</FONT>
% <td> 0.477
% <td> 0.554
% <td> 0.400
% <td> 0.431
% <td> 0.708
% <td> 0.492
% <tr>
% <td BGCOLOR=#00CCFF><FONT COLOR=000000>colon</FONT>
% <td> 0.211
% <td> 0.211
% <td> 0.158
% <td> 0.211
% <td> 0.316
% <td> 0.211
% <tr>
% <td BGCOLOR=#00CCFF><FONT COLOR=000000>amlAll</FONT>
% <td> 0.455
% <td> 0.227
% <td> 0.136
% <td> 0.182
% <td> 0.364
% <td> 0.182
% </table>
% </html>
%%
% There is considerable variance in performance
% across the 3 different trials (as you can see
% for yourself by running <http://pmtk3.googlecode.com/svn/trunk/demos/otherDemos/supervisedModels/linearKernelDemo.m linearKernelDemo.m>  ),
% so it is difficult to draw reliable conclusions.
% In terms of median performace, all methods are similar,
% with the exception of logregL2,
% which is somewhat worse than the others.
%
% The error rates for all methods on the forensic glass
% data are quite high (chance would be 5/6=0.83).
% This is because we used a linear kernel,
% but we only have 9 features. Using a nonlinear kernel
% helps performance a lot on such low-dim datasets.
% However, on high-dim datasets, linear kernels
% are usually sufficient. Indeed, we can just as well
% work in the original feature space and not use kernels at all
% for the other 3 datasets.
%
% The error rate for SVM on AML/ALL is very high (chance is 0.5).
% Other papers have reported much better results
% for SVMs on this data (with linear kernels). This is possibly
% due to the way we do cross validation. However, it may be
% that other authors have chosen particularly favorable train/test splits.
% We find that, across the 3 folds, the number of errors were 11, 10 and 4
% (out of 22).
%
% It is also interesting to look at the amount of time required
% to train each model (prediction is very fast with all of these methods).
% Below we show median time in seconds.
% We are comparing apples with oranges here, however,
% since the svm code is in C (libsvm), the rvm code is an optimized
% greedy algoritnm in Matlab (SparseBayes), the SMLR and RMLR is unoptimized
% Matlab code, and the L2 and L1 code is Fortran (glmnet).
%%
% <html>
% <TABLE BORDER=3 CELLPADDING=5 WIDTH="100%" >
% <TR><TH COLSPAN=7 ALIGN=center> training time in seconds (median over 3 trials) </font></TH></TR>
% <TR ALIGN=left>
% <TH BGCOLOR=#00CCFF><FONT COLOR=000000></FONT></TH>
% <TH BGCOLOR=#00CCFF><FONT COLOR=000000>SVM</FONT></TH>
% <TH BGCOLOR=#00CCFF><FONT COLOR=000000>RVM</FONT></TH>
% <TH BGCOLOR=#00CCFF><FONT COLOR=000000>SMLR</FONT></TH>
% <TH BGCOLOR=#00CCFF><FONT COLOR=000000>RMLR</FONT></TH>
% <TH BGCOLOR=#00CCFF><FONT COLOR=000000>logregL2</FONT></TH>
% <TH BGCOLOR=#00CCFF><FONT COLOR=000000>logregL1</FONT></TH>
% </TR>
% <tr>
% <td BGCOLOR=#00CCFF><FONT COLOR=000000>soy</FONT>
% <td> 0.566
% <td> 0.549
% <td> 43.770
% <td> 24.193
% <td> 0.024
% <td> 0.720
% <tr>
% <td BGCOLOR=#00CCFF><FONT COLOR=000000>fglass</FONT>
% <td> 0.586
% <td> 0.146
% <td> 67.552
% <td> 30.204
% <td> 0.043
% <td> 0.684
% <tr>
% <td BGCOLOR=#00CCFF><FONT COLOR=000000>colon</FONT>
% <td> 1.251
% <td> 0.028
% <td> 2.434
% <td> 2.618
% <td> 0.021
% <td> 0.418
% <tr>
% <td BGCOLOR=#00CCFF><FONT COLOR=000000>amlAll</FONT>
% <td> 3.486
% <td> 0.017
% <td> 2.337
% <td> 2.569
% <td> 0.097
% <td> 1.674
% </table>
% </html>
%%
% Reproducing

##### SOURCE END #####
--></body></html>