
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML is auto-generated from an M-file.
To make changes, update the M-file and republish this document.
      --><title>Supervised learning using non-parametric discriminative models in pmtk3</title><meta name="generator" content="MATLAB 7.9"><meta name="date" content="2010-09-06"><meta name="m-file" content="tutKernelClassif"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head><body><div class="content"><h1>Supervised learning using non-parametric discriminative models in pmtk3</h1><!--introduction--><p><i>This page was auto-generated by publishing</i> <a href="http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/tutKernelClassif.m">tutKernelClassif.m</a></p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Kernel functions</a></li><li><a href="#10">Using cross validation to choose the kernel parameters</a></li></ul></div><h2>Kernel functions<a name="1"></a></h2><p>One common form of basis function expansion is to define a new feature vector <img src="tutKernelClassif_eq31990.png" alt="$\phi(x)$"> by comparing the input <img src="tutKernelClassif_eq43551.png" alt="$x$"> to a set of prototypes or examplars <img src="tutKernelClassif_eq62509.png" alt="$\mu_k$"> as follows:</p><p><img src="tutKernelClassif_eq09998.png" alt="$$\phi(x) = (K(x,\mu_1), ..., K(x,\mu_D))$$"></p><p>Here <img src="tutKernelClassif_eq94202.png" alt="$K(x,\mu)$"> is a 'kernel function', which in this context just means a function of two arguments. A common example is the Gaussian or RBF kernel</p><p><img src="tutKernelClassif_eq89797.png" alt="$$K(x,\mu) = \exp(-\frac{||x-\mu||^2}{2\sigma^2})$$"></p><p>where <img src="tutKernelClassif_eq24873.png" alt="$\sigma$"> is the 'bandwidth'. (The quantity <img src="tutKernelClassif_eq21526.png" alt="$1/\sigma$"> is known as the scale or precision.) Another common example is the polynomial kernel</p><p><img src="tutKernelClassif_eq61193.png" alt="$$K(x,\mu) = (1+x^T \mu)^d$$"></p><p>where d is the degree. Often we take the prototypes <img src="tutKernelClassif_eq14632.png" alt="$\mu_j$"> to be the training vectors (rows of <img src="tutKernelClassif_eq03598.png" alt="$X$">), but we don't have to.</p><p>The advantages of using kernels include the following</p><div><ul><li>We can apply standard parametric models (e.g., linear and logistic regression) to non-vectorial inputs (e.g., strings, molecular structures, etc.), by defining <img src="tutKernelClassif_eq94202.png" alt="$K(x,\mu)$"> to be some kind of function for comparing structured inputs.</li><li>We can increase the flexibility of the model by working in an enlarged feature space.</li></ul></div><p>Below we show an example where we fit the XOR data using kernelized logistic regression, with various kernels and prototypes (from <a href="http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Introduction/logregXorDemo.m">logregXorDemo.m</a> ).</p><pre class="codeinput">clear <span class="string">all</span>; close <span class="string">all</span>
[X, y] = createXORdata();
rbfScale = 1;
polydeg  = 2;
protoTypes = [1 1; 1 5; 5 1; 5 5];
protoTypesStnd = standardizeCols(protoTypes);
kernels = {@(X1, X2)kernelRbfSigma(X1, protoTypesStnd, rbfScale)
           @(X1, X2)kernelRbfSigma(X1, X2, rbfScale)
           @(X1, X2)kernelPoly(X1, X2, polydeg)};
titles  = {<span class="string">'rbf'</span>, <span class="string">'rbf prototypes'</span>, <span class="string">'poly'</span>};
<span class="keyword">for</span> i=1:numel(kernels)
    preproc = preprocessorCreate(<span class="string">'kernelFn'</span>, kernels{i}, <span class="string">'standardizeX'</span>, true, <span class="string">'addOnes'</span>, true);
    model = logregFit(X, y, <span class="string">'preproc'</span>, preproc);
    yhat = logregPredict(model, X);
    errorRate = mean(yhat ~= y);
    fprintf(<span class="string">'Error rate using %s features: %2.f%%\n'</span>, titles{i}, 100*errorRate);
    predictFcn = @(Xtest)logregPredict(model, Xtest);
    plotDecisionBoundary(X, y, predictFcn);
    <span class="keyword">if</span> i==2
       hold <span class="string">on</span>;
       plot(protoTypes(:, 1), protoTypes(:, 2), <span class="string">'*k'</span>, <span class="string">'linewidth'</span>, 2, <span class="string">'markersize'</span>, 10)
    <span class="keyword">end</span>
    title(titles{i});
<span class="keyword">end</span>
</pre><pre class="codeoutput">Error rate using rbf features:  0%
Error rate using rbf prototypes features:  0%
Error rate using poly features:  0%
</pre><img vspace="5" hspace="5" src="tutKernelClassif_01.png" alt=""> <img vspace="5" hspace="5" src="tutKernelClassif_02.png" alt=""> <img vspace="5" hspace="5" src="tutKernelClassif_03.png" alt=""> <p>In the first example, we use an RBF kernel with centers at 4 manually chosen points, shown with black stars. In the second and third examples, we use an RBF and polynomial kernel, centered at all the training data. We can use L1 regularization to select a subset of the training data, as we illustrate below.</p><h2>Using cross validation to choose the kernel parameters<a name="10"></a></h2><p>We can create a grid of models, with different kernel params and different strength regularizers, as shown in the example below ( from <a href="http://pmtk3.googlecode.com/svn/trunk/demos/otherDemos/supervisedModels/logregKernelCrabsDemo.m">logregKernelCrabsDemo.m</a> ). If CV does not pick a point on the edge of the grid, we can be faily confident we have searched over a reasonable range. For this reason, it is helpful to plot the cost surface.</p><pre class="codeinput">clear <span class="string">all</span>
loadData(<span class="string">'crabs'</span>);
<span class="comment">% Here we cross validate over both lambda and Sigma</span>
lambda     = logspace(-5, -2, 10); <span class="comment">%logspace(-7, -4, 20);</span>
Sigma      = 1:2:8; <span class="comment">% 8:0.5:10;</span>
paramRange = crossProduct(lambda, Sigma);
regtypes = {<span class="string">'L1'</span>, <span class="string">'L2'</span>};
<span class="keyword">for</span> r=1:length(regtypes)
  regtype = regtypes{r};
  fitFn = @(X, y, param)<span class="keyword">...</span>
    logregFit(X, y, <span class="string">'lambda'</span>, param(1), <span class="string">'regType'</span>, regtype, <span class="string">'preproc'</span>, <span class="keyword">...</span>
    preprocessorCreate(<span class="string">'kernelFn'</span>, @(X1, X2)kernelRbfSigma(X1, X2, param(2))));
  predictFn = @logregPredict;
  lossFn = @(ytest, yhat)mean(yhat ~= ytest);
  nfolds = 5;
  useSErule = true;
  plotCv = true;
  tic;
  [LRmodel, lambdaStar, LRmu, LRse] = <span class="keyword">...</span>
    fitCv(paramRange, fitFn, predictFn, lossFn, Xtrain, ytrain, nfolds, <span class="keyword">...</span>
    <span class="string">'useSErule'</span>, useSErule, <span class="string">'doPlot'</span>, plotCv, <span class="string">'params1'</span>, lambda, <span class="string">'params2'</span>, Sigma);
  time(r) = toc
  yhat = logregPredict(LRmodel, Xtest);
  nerrors(r) = sum(yhat ~= ytest)
<span class="keyword">end</span>
</pre><pre class="codeoutput">time =
   15.3646
nerrors =
    61
time =
   15.3646    3.5041
nerrors =
    61    59
</pre><img vspace="5" hspace="5" src="tutKernelClassif_04.png" alt=""> <img vspace="5" hspace="5" src="tutKernelClassif_05.png" alt=""> <p>We see that L2 regularization (which results in a dense model) is both more accurate and faster to train, in this example at least.</p><p class="footer"><br>
      Published with MATLAB&reg; 7.9<br></p></div><!--
##### SOURCE BEGIN #####
%% Supervised learning using non-parametric discriminative models in pmtk3
% _This page was auto-generated by publishing_
% <http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/tutKernelClassif.m tutKernelClassif.m>
%
%% Kernel functions
%
% One common form of basis function expansion
% is to define a new feature vector $\phi(x)$ by comparing the input
% $x$ to a set of prototypes or examplars $\mu_k$ as follows:
%%
% $$\phi(x) = (K(x,\mu_1), ..., K(x,\mu_D))$$
%%
% Here $K(x,\mu)$ is a 'kernel function',
% which in this context just means a function of two arguments.
% A common example is the Gaussian or RBF kernel
%%
% $$K(x,\mu) = \exp(-\frac{||x-\mu||^2}{2\sigma^2})$$
%%
% where $\sigma$ is the 'bandwidth'. (The quantity $1/\sigma$ is known as
% the scale or precision.)
% Another common example is the polynomial kernel
%%
% $$K(x,\mu) = (1+x^T \mu)^d$$
%%
% where d is the degree.
% Often we take the prototypes $\mu_j$ to be the training vectors (rows of $X$), but we don't have to.
%
% The advantages of using kernels include the following
%
% * We can apply standard parametric models (e.g., linear and logistic
% regression) to non-vectorial inputs (e.g., strings, molecular structures, etc.),
% by defining $K(x,\mu)$ to be some
% kind of function for comparing structured inputs.
% * We can increase the flexibility of the model by working in an
% enlarged feature space.
%
% Below we show an example where we fit the XOR data using kernelized
% logistic regression, with various kernels and prototypes
% (from <http://pmtk3.googlecode.com/svn/trunk/demos/bookDemos/Introduction/logregXorDemo.m logregXorDemo.m> ).
%%
clear all; close all
[X, y] = createXORdata();
rbfScale = 1;
polydeg  = 2;
protoTypes = [1 1; 1 5; 5 1; 5 5];
protoTypesStnd = standardizeCols(protoTypes);
kernels = {@(X1, X2)kernelRbfSigma(X1, protoTypesStnd, rbfScale)
           @(X1, X2)kernelRbfSigma(X1, X2, rbfScale)
           @(X1, X2)kernelPoly(X1, X2, polydeg)};
titles  = {'rbf', 'rbf prototypes', 'poly'};
for i=1:numel(kernels)
    preproc = preprocessorCreate('kernelFn', kernels{i}, 'standardizeX', true, 'addOnes', true);
    model = logregFit(X, y, 'preproc', preproc);
    yhat = logregPredict(model, X);
    errorRate = mean(yhat ~= y);
    fprintf('Error rate using %s features: %2.f%%\n', titles{i}, 100*errorRate);
    predictFcn = @(Xtest)logregPredict(model, Xtest);
    plotDecisionBoundary(X, y, predictFcn);
    if i==2
       hold on; 
       plot(protoTypes(:, 1), protoTypes(:, 2), '*k', 'linewidth', 2, 'markersize', 10)
    end
    title(titles{i});
end
%%
% In the first example, we use an RBF kernel with centers at 4
% manually chosen points, shown with black stars.
% In the second and third examples, we use an RBF and polynomial kernel,
% centered at all the training data.
% We can use L1 regularization to select a subset of the training
% data, as we illustrate below.

%% Using cross validation to choose the kernel parameters
% We can create a grid of models, with different kernel params
% and different strength regularizers, as shown in the example
% below ( from <http://pmtk3.googlecode.com/svn/trunk/demos/otherDemos/supervisedModels/logregKernelCrabsDemo.m logregKernelCrabsDemo.m> ).
% If CV does not pick a point on the edge of the grid,
% we can be faily confident we have searched over
% a reasonable range. For this reason,
% it is helpful to plot the cost surface.
%%
clear all
loadData('crabs');
% Here we cross validate over both lambda and Sigma
lambda     = logspace(-5, -2, 10); %logspace(-7, -4, 20); 
Sigma      = 1:2:8; % 8:0.5:10;
paramRange = crossProduct(lambda, Sigma); 
regtypes = {'L1', 'L2'};
for r=1:length(regtypes)
  regtype = regtypes{r};
  fitFn = @(X, y, param)...
    logregFit(X, y, 'lambda', param(1), 'regType', regtype, 'preproc', ...
    preprocessorCreate('kernelFn', @(X1, X2)kernelRbfSigma(X1, X2, param(2))));
  predictFn = @logregPredict;
  lossFn = @(ytest, yhat)mean(yhat ~= ytest);
  nfolds = 5;
  useSErule = true;
  plotCv = true;
  tic;
  [LRmodel, lambdaStar, LRmu, LRse] = ...
    fitCv(paramRange, fitFn, predictFn, lossFn, Xtrain, ytrain, nfolds, ...
    'useSErule', useSErule, 'doPlot', plotCv, 'params1', lambda, 'params2', Sigma);
  time(r) = toc
  yhat = logregPredict(LRmodel, Xtest);
  nerrors(r) = sum(yhat ~= ytest)
end
%%
% We see that L2 regularization (which results in a dense model)
% is both more accurate and faster to train, in this example
% at least.







##### SOURCE END #####
--></body></html>