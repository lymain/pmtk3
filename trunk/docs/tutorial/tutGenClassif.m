%% Supervised learning using generative models in pmtk3
% _This page was auto-generated by publishing_
% <http://pmtk3.googlecode.com/svn/trunk/docs/tutorial/tutGenClassif.m>.
%
% Generative models for classification/ regression are joint models of the
% outputs and inputs
% of the form $p(y,x|\theta)$.
% We consider various examples below.

%% Naive Bayes classifier
% We now a simple kind of generative classifier
% called naive Bayes, which is a model of the form
%%
% $$p(y,x|\theta) = p(y|\pi) \prod_{j=1}^D p(x_j|y,\theta)$$
%%
% We can fit and predict with this model using
% naiveBayesFit.m and naiveBayesPredict.m
% For simplicity, the current implementation
% assumes  all the features are binary,
% so $p(x_j|y=c,\theta) = Ber(x_j|\theta_{jc})$.
% It fits by MAP estimation
% with a vague Dirichlet prior (add-one-smoothing).
%
% Below is an example (from naiveBayesBowDemo.m )
% which fits a model to some bag of words data,
% and then classifies a test set.
%%
loadData('XwindowsDocData'); % 900x600, 2 classes Xwindows vs MSwindows
Xtrain = xtrain; Xtest = xtest;
model = naiveBayesFit(Xtrain, ytrain);
ypred_train = naiveBayesPredict(model, Xtrain);
err_train = mean(zeroOneLossFn(ytrain, ypred_train));
ypred_test = naiveBayesPredict(model, Xtest);
err_test = mean(zeroOneLossFn(ytest, ypred_test));
fprintf('misclassification rates  on train = %5.2f pc, on test = %5.2f pc\n', ...
    err_train*100, err_test*100);
%%
% See also naiveBayesMnistDemo.m for application of NBC
% to classify binarized MNIST digits.
%
% It is simple to modify NBC to handle missing data in X
% at training and test time; this is left as an exercise
% to the reader.

%% Discriminant analysis
% Discriminant analysis is a generative classifier where
% the class conditional density  is a multivariate Gaussian:
%%
% $$p(y=c,x|\theta) = \mbox{discrete}(y|\pi) N(x|\mu_c,\Sigma_c)$$
%%
% PMTK supports the following variants of this model:
%%
% <html>
% <table border=1>
% <TR ALIGN=left>
% <td> Type
% <td> Description
% <tr>
% <td> QDA
% <td> $\Sigma_c$ is different for each class.
% Induces quadratic decision boundaries.
% <tr>
% <td> LDA (linear)
% <td>  $\Sigma_c=\Sigma$ is the same (tied) for each class.
% Induces linear decision boundaries.
% <tr>
% <td> diag
% <td> $\Sigma_c$ is diagonal, so the features are conditionally
% independent; this is an example of a naive Bayes classifier.
% Induces linear decision boundaries.
% <tr>
% <td> RDA
% <td> Regularized LDA; uses MAP estimation for $\Sigma$.
% <tr>
% <td> shrunkenCentroids
% <td> Diagonal LDA with L1 shrinkage on offsets (see below)
% </table>
% </html>
%%
% We give more details below.

%% QDA/ LDA/ NBC
% Below we give an example (from discrimAnalaysisFisherIrisDemo.m)
% of how to fit a QDA/LDA/ diagDA model.
% We apply it to a subset of the Fisher Iris dataset.
%%
loadData('fisheriris')
X = meas(51:end, 1:2);  % for illustrations use 2 species, 2 features
labels = species(51:end);
[y, support] = canonizeLabels(labels);
types = {'quadratic', 'linear', 'diag'};
for tt=1:length(types)
  model = discrimAnalysisFit(X, y, types{tt});
  h = plotDecisionBoundary(X, y, @(Xtest)discrimAnalysisPredict(model, Xtest));
  title(sprintf('Discrim. analysis of type %s', types{tt}));
  if ~isOctave
    legend(h, support, 'Location', 'NorthWest');
    set(gca, 'Xtick', 5:8, 'Ytick', 2:0.5:4);
  end
  xlabel('X_1'); ylabel('X_2');
end
%%

%% Regularized discriminant analysis
% When fitting a discriminant analysis model 
% we will encounter numerical problems
% when estimating $\Sigma$ when N < D, even if we use
% a tied  covariance matrix (i.e., one shared across classes, a method
% known as linear discriminant analysis).
% A simple solution is to use a Wishart prior to compute a MAP
% estimate of $\Sigma$. This is called regularized discriminant analysis,
% and can be fit using |discrimAnalysisFit(X, y, 'rda', lambda)|,
% where |lambda| controls the amount of regularization.
% See cancerHighDimClassifDemo.m
% for an example, which reproduces
% table 18.1 from "Elements of statistical learning" 2nd edn p656.
% (We don't run this demo here since it is a bit slow,
% since the data has 16,000 features.)
%
%% Nearest shrunken centroid
% Consider a naive Bayes model in which the diagonal covariance
% is tied. This has O(D) parameters for the covariance, but O(C D) for the mean.
% To prevent overfitting, we can shrink the class-conditional means towards
% the overall mean; this technique is called nearest shrunken centroids. We
% can fit this model using |discrimAnalysisFit(X, y, 'shrunkenCentroids',
% lambda)|. We given an example of this below (from
%  shrunkenCentroidsSRBCTdemo.m),
% where we apply the method to
% the SRBCT gene microarray dataset, which has N=144 training examples,
% D=16063 features and C=4 classes.
% This roughly reproduces figure 18.4 from "Elements of statistical
% learning" 2nd ed.
%%
close all; clear all;
loadData('srbct');

Xtest = Xtest(~isnan(ytest), :);
ytest = ytest(~isnan(ytest));

fitFn = @(X,y,lam)  discrimAnalysisFit(X, y, 'shrunkenCentroids', 'lambda',lam);
predictFn = @(model, X)  discrimAnalysisPredict(model, X);

lambdas = linspace(0, 8, 20);
nTrain = length(ytrain);
nTest = length(ytest);
for i=1:length(lambdas)
    model = fitFn(Xtrain, ytrain, lambdas(i));
    yhatTrain = predictFn(model, Xtrain);
    yhatTest = predictFn(model, Xtest);
    errTrain(i) = sum(zeroOneLossFn(yhatTrain, ytrain))/nTrain;
    errTest(i) = sum(zeroOneLossFn(yhatTest, ytest))/nTest;
    numgenes(i) = sum(model.shrunkenCentroids(:) ~= 0);
end

figure;
plot(lambdas, errTrain, 'gx-', lambdas, errTest, 'bo--',...
  'MarkerSize', 10, 'linewidth', 2)
legend('Training', 'Test', 'Location', 'northwest');
xlabel('Amount of shrinkage')
ylabel('misclassification rate')
title('SRBCT data')

%% K-nearest neighbor classifier
% One can view a KNN classifier as a generative
% classifier where the class conditional density is a non-parametric
% kernel density estimator.
% Below we give an example where we apply
% a 1-NN classifier to a subset of the MNIST digit set
% (from mnistKNNdemo.m: see mnist1NNdemo.m for special-purpose
% code that can handle the full dataset).
%%
loadData('mnistAll');
trainndx = 1:10000;
testndx =  1:1000;
ntrain = length(trainndx);
ntest = length(testndx);
Xtrain = double(reshape(mnist.train_images(:,:,trainndx),28*28,ntrain)');
Xtest  = double(reshape(mnist.test_images(:,:,testndx),28*28,ntest)');
ytrain = (mnist.train_labels(trainndx));
ytest  = (mnist.test_labels(testndx));
clear mnist trainndx testndx; % save space

m = knnFit(Xtrain, ytrain, 1);
ypred = knnPredict(m, Xtest);
errorRate = mean(ypred ~= ytest);
fprintf('Error Rate: %.2f%%\n',100*errorRate);
%%
