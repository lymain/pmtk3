
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML is auto-generated from an M-file.
To make changes, update the M-file and republish this document.
      --><title>cancerHighDimClassifDemo</title><meta name="generator" content="MATLAB 7.10"><meta name="date" content="2010-05-01"><meta name="m-file" content="cancerHighDimClassifDemo"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head><body><div class="content"><h2>Contents</h2><div><ul><li><a href="#2">Load data</a></li><li><a href="#3">Run methods</a></li><li><a href="#4">Print results</a></li></ul></div><pre class="codeinput"><span class="comment">% Try to reproduce table 18.1 from "Elements of statistical learning" 2nd edn p656</span>
<span class="comment">%PMTKauthor Hannes Bretschneider</span>
<span class="comment">%PMTKslow</span>
</pre><h2>Load data<a name="2"></a></h2><pre class="codeinput">load(<span class="string">'14cancer.mat'</span>) <span class="comment">% modified data so X is N*D as usual</span>
ytrain = colvec(ytrain);
ytest = colvec(ytest);
<span class="comment">% on p654, they say "the data from each patient (row) is standardized</span>
<span class="comment">% to have mean 0 and variance 1"</span>
xtrain_std = standardizeCols(Xtrain')';
xtest_std = standardizeCols(Xtest')';
[N, D] = size(xtrain_std);
clear <span class="string">Xtest</span> <span class="string">Xtrain</span>

<span class="comment">% we can either explicitly use the same folds as Hastie</span>
<span class="comment">% or we can choose our own (which lets us control computation time)</span>
<span class="comment">% The dataset is so small that some classes might not be present</span>
<span class="comment">% in any given training fold. The Hastie folds have been carefully</span>
<span class="comment">% chosen to avoid this (although it is really the algorithm's job</span>
<span class="comment">% to handle this eg by using priors).</span>
<span class="keyword">if</span> 1
  folds = importdata(<span class="string">'cvfolds.txt'</span>);
  folds = folds.data(:,2:size(folds.data,2));
  Nfolds = [];
<span class="keyword">else</span>
  Nfolds = 3;
  folds = [];
<span class="keyword">end</span>
</pre><h2>Run methods<a name="3"></a></h2><p>L1 is very slow, L2 is somewhat slow</p><pre class="codeinput"><span class="comment">%methods = {'nsc', 'nb', 'rda', 'knn', 'l2logreg', 'svm', 'l1logreg'};</span>
<span class="comment">%methods = {'nsc', 'nb', 'rda', 'knn', 'l2logreg', 'svm'};</span>
methods = {<span class="string">'nsc'</span>, <span class="string">'nb'</span>, <span class="string">'rda'</span>, <span class="string">'knn'</span>, <span class="string">'svm'</span>};
<span class="comment">% warning - l1logreg can take upwards of 6 hours to run.</span>
M = length(methods);

<span class="keyword">for</span> m=1:M
  method = methods{m};
  <span class="keyword">switch</span> method
    <span class="keyword">case</span> <span class="string">'nsc'</span>
      name{m} = <span class="string">'Nearest shrunken centroids'</span>;
      params = linspace(0, 10, 10);
      fitFn = @naiveBayesGaussFitShrunkenCentroids;
      predictFn = @naiveBayesGaussPredict;
      noGenesFn = @(model)sum(model.relevant);
    <span class="keyword">case</span> <span class="string">'nb'</span>
      name{m} = <span class="string">'Naive bayes'</span>;
      params = 1;
      fitFn = @(X, y, param)naiveBayesGaussFit(X, y);
      predictFn = @naiveBayesGaussPredict;
      noGenesFn = @(model)D;
    <span class="keyword">case</span> <span class="string">'rda'</span>
      name{m} = <span class="string">'Regularized discriminant analysis'</span>;
      params = linspace(0, 2, 10);
      <span class="comment">% we don;t have to do multiple SVDs, since we</span>
      <span class="comment">% are just changing the weighting term</span>
      [U S V] = svd(xtrain_std, <span class="string">'econ'</span>);
      R = U*S;
      fitFn = @(X, y, gamma)RDAfit(X, y, gamma, <span class="string">'R'</span>, R, <span class="string">'V'</span>, V);
      predictFn = @RDApredict;
      noGenesFn = @(model)D;
    <span class="keyword">case</span> <span class="string">'knn'</span>
      name{m} = <span class="string">'k-nearest neighbors'</span>;
      params = 1:3;
      fitFn = @knnFit;
      predictFn = @knnPredict;
      noGenesFn = @(model)D;
    <span class="keyword">case</span> <span class="string">'l2logreg'</span>
      name{m} = <span class="string">'l2logreg'</span>;
      params = linspace(0, 10, 10);
      fitFn = @(X, y, lambda)logregFitL2Dual(X, y, lambda);
      predictFn = @logregPredict;
      noGenesFn = @(m) D;
    <span class="keyword">case</span> <span class="string">'l1logreg'</span>
      name{m} = <span class="string">'l1-penalized logistic regression'</span>;
      params = linspace(1, 10, 10);
      fitFn = @(X, y, param)logregFit(X, y, <span class="string">'lambda'</span>, param,<span class="keyword">...</span>
        <span class="string">'regType'</span>, <span class="string">'L1'</span>, <span class="string">'fitMethod'</span>, <span class="string">'minFunc'</span>);
      predictFn = @logregPredict;
      noGenesFn = @(model) sum(sum(model.w,1)~=0);
    <span class="keyword">case</span> <span class="string">'svm'</span>
      params =  logspace(-1,1,5);
      name{m} = <span class="string">'SVM'</span>;
      fitFn = @(X, y, param) svmFit(X, y, <span class="string">'kernel'</span>, <span class="string">'linear'</span>, <span class="string">'C'</span>, param);
      predictFn = @svmPredict;
      noGenesFn = @(model) D; <span class="comment">%sum(sum(model.w,1)~=0);</span>
  <span class="keyword">end</span>

  useSErule=0; doPlot=0; plotArgs= [];
  [model{m}, bestParam(m)] = fitCv(params,<span class="keyword">...</span>
    fitFn, predictFn, @zeroOneLossFn, xtrain_std, ytrain, <span class="keyword">...</span>
    Nfolds, useSErule, doPlot, plotArgs,  folds);
  <span class="comment">%ndx = find(err==min(err), 1);</span>
  <span class="comment">%lossCv(m)= err(ndx); seCv(m) = se(ndx);</span>
  yhat = predictFn(model{m}, xtest_std);
  lossTest(m) = sum(zeroOneLossFn(yhat, ytest));
  noGenes(m) = noGenesFn(model{m});
<span class="keyword">end</span>
</pre><h2>Print results<a name="4"></a></h2><pre class="codeinput">latextable([lossTest' noGenes' bestParam'],<span class="keyword">...</span>
     <span class="string">'Horiz'</span>, {<span class="string">'Test errors'</span>, <span class="string">'Genes used'</span>, <span class="string">'Best Param'</span>},<span class="keyword">...</span>
     <span class="string">'Vert'</span>, name, <span class="string">'name'</span>, <span class="string">''</span>)

<span class="keyword">for</span> m=1:M
  fprintf(<span class="string">'method %s, test errors %d, ngenes %d, best param %5.3f\n'</span>, <span class="keyword">...</span>
    name{m}, lossTest(m), noGenes(m), bestParam(m));
<span class="keyword">end</span>
</pre><pre class="codeoutput">\begin{tabular}{cccc}
\hline
&amp; Test errors &amp; Genes used &amp; Best Param \\
Nearest shrunken centroids &amp; 17.0 &amp; 3187.0 &amp; 3.3 \\
Naive bayes &amp; 21.0 &amp; 16063.0 &amp; 1.0 \\
Regularized discriminant analysis &amp; 13.0 &amp; 16063.0 &amp; 0.0 \\
k-nearest neighbors &amp; 24.0 &amp; 16063.0 &amp; 1.0 \\
SVM &amp; 12.0 &amp; 16063.0 &amp; 0.1 \\
\hline
\end{tabular}
method Nearest shrunken centroids, test errors 17, ngenes 3187, best param 3.333
method Naive bayes, test errors 21, ngenes 16063, best param 1.000
method Regularized discriminant analysis, test errors 13, ngenes 16063, best param 0.000
method k-nearest neighbors, test errors 24, ngenes 16063, best param 1.000
method SVM, test errors 12, ngenes 16063, best param 0.100
</pre><p class="footer"><br>
      Published with MATLAB&reg; 7.10<br></p></div><!--
##### SOURCE BEGIN #####
% Try to reproduce table 18.1 from "Elements of statistical learning" 2nd edn p656
%PMTKauthor Hannes Bretschneider
%PMTKslow

%% Load data

load('14cancer.mat') % modified data so X is N*D as usual
ytrain = colvec(ytrain);
ytest = colvec(ytest); 
% on p654, they say "the data from each patient (row) is standardized
% to have mean 0 and variance 1"
xtrain_std = standardizeCols(Xtrain')';
xtest_std = standardizeCols(Xtest')';
[N, D] = size(xtrain_std);
clear Xtest Xtrain 

% we can either explicitly use the same folds as Hastie
% or we can choose our own (which lets us control computation time)
% The dataset is so small that some classes might not be present
% in any given training fold. The Hastie folds have been carefully
% chosen to avoid this (although it is really the algorithm's job
% to handle this eg by using priors).
if 1
  folds = importdata('cvfolds.txt');
  folds = folds.data(:,2:size(folds.data,2));
  Nfolds = [];
else
  Nfolds = 3;
  folds = [];
end



%% Run methods
% L1 is very slow, L2 is somewhat slow

%methods = {'nsc', 'nb', 'rda', 'knn', 'l2logreg', 'svm', 'l1logreg'}; 
%methods = {'nsc', 'nb', 'rda', 'knn', 'l2logreg', 'svm'}; 
methods = {'nsc', 'nb', 'rda', 'knn', 'svm'};
% warning - l1logreg can take upwards of 6 hours to run. 
M = length(methods);

for m=1:M
  method = methods{m};
  switch method
    case 'nsc'
      name{m} = 'Nearest shrunken centroids';
      params = linspace(0, 10, 10);
      fitFn = @naiveBayesGaussFitShrunkenCentroids;
      predictFn = @naiveBayesGaussPredict;
      noGenesFn = @(model)sum(model.relevant);
    case 'nb'
      name{m} = 'Naive bayes';
      params = 1;
      fitFn = @(X, y, param)naiveBayesGaussFit(X, y);
      predictFn = @naiveBayesGaussPredict;
      noGenesFn = @(model)D;
    case 'rda'
      name{m} = 'Regularized discriminant analysis';
      params = linspace(0, 2, 10);
      % we don;t have to do multiple SVDs, since we
      % are just changing the weighting term
      [U S V] = svd(xtrain_std, 'econ');
      R = U*S;
      fitFn = @(X, y, gamma)RDAfit(X, y, gamma, 'R', R, 'V', V);
      predictFn = @RDApredict;
      noGenesFn = @(model)D;
    case 'knn'
      name{m} = 'k-nearest neighbors';
      params = 1:3;
      fitFn = @knnFit;
      predictFn = @knnPredict;
      noGenesFn = @(model)D;
    case 'l2logreg'
      name{m} = 'l2logreg';
      params = linspace(0, 10, 10);
      fitFn = @(X, y, lambda)logregFitL2Dual(X, y, lambda);
      predictFn = @logregPredict;
      noGenesFn = @(m) D;
    case 'l1logreg'
      name{m} = 'l1-penalized logistic regression';
      params = linspace(1, 10, 10);
      fitFn = @(X, y, param)logregFit(X, y, 'lambda', param,...
        'regType', 'L1', 'fitMethod', 'minFunc');
      predictFn = @logregPredict;
      noGenesFn = @(model) sum(sum(model.w,1)~=0);
    case 'svm'
      params =  logspace(-1,1,5);
      name{m} = 'SVM';
      fitFn = @(X, y, param) svmFit(X, y, 'kernel', 'linear', 'C', param); 
      predictFn = @svmPredict;
      noGenesFn = @(model) D; %sum(sum(model.w,1)~=0);
  end
  
  useSErule=0; doPlot=0; plotArgs= [];
  [model{m}, bestParam(m)] = fitCv(params,...
    fitFn, predictFn, @zeroOneLossFn, xtrain_std, ytrain, ...
    Nfolds, useSErule, doPlot, plotArgs,  folds);
  %ndx = find(err==min(err), 1);
  %lossCv(m)= err(ndx); seCv(m) = se(ndx);
  yhat = predictFn(model{m}, xtest_std);
  lossTest(m) = sum(zeroOneLossFn(yhat, ytest));
  noGenes(m) = noGenesFn(model{m});
end

%% Print results
latextable([lossTest' noGenes' bestParam'],...
     'Horiz', {'Test errors', 'Genes used', 'Best Param'},...
     'Vert', name, 'name', '')

for m=1:M
  fprintf('method %s, test errors %d, ngenes %d, best param %5.3f\n', ...
    name{m}, lossTest(m), noGenes(m), bestParam(m));
end


##### SOURCE END #####
--></body></html>