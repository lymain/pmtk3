#summary Tutorial on unsupervised learning using pmtk3

= Basics =

We can create 
 an unconditional density model of type 'foo' as follows

{{{
  model = fooCreate(...)
}}}

The optional arguments '...' depends on the form of the model,
and are explained below.

Currently, 'foo' can be one of the following (in alphabetical order)

 * crf2 (Conditional random field with discrete hidde nodes, pairwise potentials)
 * hmm (hidden Markov model, any kind of observation)
 * markov (discrete time, discrete state Markov chain)
 * mrf2 (Markov random field with discrete nodes, pairwise potentials)

Below we describe the methods that each model supports.
Note thatnot all models support all methods.
Furthermore, this list is currently a specification of what the future
interface will look like, rather than a description of the current interface
(which can be inferred from the auto-generated documentation).
To determine if a given model supports a given method,
just type 'help fooMethod' and if Matlab says that 'fooMethod' is undefined,
then it is not supported.

The basic operations on an unconditional density model  are as follows 

{{{
model = fooFit(model, X, ...) % Compute ML or MAP estimate of parameters
 
 L = fooLogprob(model, X) % L(i) = log p(X(i,:))
 
E = fooEnergy(model, X) % E(i) = - log p(X(i,:) ) - log Z

X = fooSample(model, N, ...) % X(i,:) ~ p(.), i=1:N

}}}

For some models,  fooEnergy  is the same as -fooLogprob, but for those
with intractable normalization constants Z (e.g., MRFs), fooEnergy is more efficient.

X is an N*D matrix, as usual.
It may have NaN's in it, which means the corresponding entry is
missing (hidden). Whether NaN's  are supported depends on the
particular model and the particular fitting/ inference method you
choose; this is controlled by the optional arguments '...' passed to
create and/or fit. These optional arguments  vary
from case to case (see below for details).

All of the above methods  are vectorized (conceptually, at least), in the
sense that they work on a set of datacases in batch model.


For joint probability models (with more than one variable), it is possible to condition on one set of
variables and infer the values of others. We therefore  define the following additional
methods (datacase is like a single row of X, and may contain NaN's for 'unclamped' variables):


{{{
   Xhat = fooImpute(model, X)  % replace values of X which are NaN with predicted value

  model = fooCondition(model, datacase) % new model represents p(hidden | datacase)
      Only supported by a few models.

  X = fooSampleConditional(model, N, datacase, ...) % X(i,:) ~ p(. |  datacase)
      Conceptually the same as fooSample(fooCondition(model, datacase), N)
      The conditioned-on nodes are clamped to their observed values.
      If N=1, this is a bit like fooImpute.
 }}} 

For graphical models (with no explicit latent variables),
we define the following additional methods (we assume hidden nodes are discrete):

{{{
xhat = fooEstNodes(model, datacase) % xhat(1,:) = argmax_x p(x|datacase) (MAP decoding)

xel = fooQuery(model, datacase, queryNodes) % bel = p(Q|datacase)

nodeBel = fooInferNodes(model, datacase) % nodeBel(n,k) = p(X(n)=k|datacase)
        We set nodeBel(n,k)=0 if k > nstates(n)

[nodeBel, edgeBel] = fooInferNodesAndEdges(model, datacase) % for pairwise models
       returns nodelBel(n,:) and edgeBel(:,:,e) 

[nodeBel, famBel] = fooInferNodesAndFamilies(model, datacase) % for DGMs
       returns nodelBel{f}(:) and familyBel{f}(:,:,...:)
}}}

For models with latent variables, we define the following
additional methods:

{{{
zhat = fooEstLatentNode(model, X) % Z(i,:) = argmax_z p(z|X(i,:))
       for single LV (eg. mixture model or PCA)

pz = fooInferLatentNode(model, X) % pz(i,k) = p(Z=k|X(i,:))
       for single discrete LV (eg mixture model)

[muZ, SigmaZ] = fooInferLatentNode(model, X) % [muZ(i,:), SigmaZ(:,:,i)] = p(Z|X(i,:))
         for single Gaussian LV (eg PCA)

nodeBel = fooInferLatentNodes(model, datacase) % nodeBel(k,n) = p(Zn=k | datacase)
      for multiple discrete LVs (eg HMM)

[mu, Sigma] = fooInferLatentNodes(model, datacase) % [mu(:,n), Sigma(:,:,n)] = p(Zn | datacase)
      for multiple Gaussian LVs (eg LDS)

[X,Z] = fooSample(model, N)  % [X(i,:),Z(i,:)] ~ p(.)

Z = fooSampleLatent(model, N, datacase) % Z(i,:) ~ p(Z | datacase)
}}}